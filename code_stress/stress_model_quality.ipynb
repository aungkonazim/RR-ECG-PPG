{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  14 out of  36 | elapsed:    8.7s remaining:   13.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132, 11)\n",
      "(1258, 11)\n",
      "0.05 (1258, 11) (1258,) 1258 289 1258 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  36 out of  36 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 288 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1889 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3826 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 5364 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7200 out of 7200 | elapsed:   18.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7240143369175627 0.411889486328604 bias\n",
      "[5 array([[902,  67],\n",
      "       [ 87, 202]]) 0.7240143369175627\n",
      " 0.7509293680297398 0.698961937716263]\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler,QuantileTransformer\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "# from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "from copy import deepcopy\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            if np.abs(precision[i]-recall[i])<.1:\n",
    "                f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "                if f > f1:\n",
    "                    f1 = f\n",
    "                    bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=10,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=1)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    delta = 0.1\n",
    "    paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[False]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=1)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "    print(data)\n",
    "    while len(feature_names)>1:\n",
    "        results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "        results = np.array(results)\n",
    "        print(results,results.shape)\n",
    "        ind_min = np.argmax(results[:,0])\n",
    "        min_f1 = results[ind_min,0]\n",
    "        min_index = np.int64(results[ind_min,1])\n",
    "        name_feature = feature_names[min_index]\n",
    "        data.append([name_feature,min_f1])\n",
    "        X = np.delete(X,min_index,axis=1)\n",
    "        feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "        print(data)\n",
    "    print(data)\n",
    "    return data    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_f1(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "#             'svc__min_samples_leaf': [4],\n",
    "#             'svc__max_features': [.7,1],\n",
    "#             'svc__n_estimators': [100,200,300],\n",
    "#             'svc__criterion':['gini','entropy'],\n",
    "             'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[False]\n",
    "            }\n",
    "#     paramGrid = {\n",
    "#             'svc__min_samples_leaf': [4],\n",
    "#             'svc__max_features': [.7,1],\n",
    "#             'svc__n_estimators': [100,200,300],\n",
    "#             'svc__criterion':['gini','entropy'],\n",
    "# #              'svc__kernel': ['rbf'],\n",
    "# #              'svc__C': [1],\n",
    "# # #              'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "# #              'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "# #              'svc__probability':[False]\n",
    "#             }\n",
    "    my_score = make_scorer(f1Bias_scorer_CV,needs_proba=True)\n",
    "    clf = Pipeline([('svc',SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=2)\n",
    "    grid_search.fit(X[:,:],y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    print(f1,bias,'bias')\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>=bias] = 1\n",
    "    y_pred[y_pred<bias] = 0\n",
    "    y_pred = np.int64(y_pred)\n",
    "    clf.fit(X,y)\n",
    "    return confusion_matrix(y,y_pred),f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred),clf,y,y_pred\n",
    "    \n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    NNs = RRints\n",
    "    tss = tmStamps\n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "#     a = a[:,0]\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_ms(ecg_rr):\n",
    "    mean_col = []\n",
    "    std_col = []\n",
    "    i = 0\n",
    "    while i < len(ecg_rr):\n",
    "        start_ts = ecg_rr[i,0]\n",
    "        j = i\n",
    "        while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "            j+=1\n",
    "        mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "        std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "        i=j\n",
    "    m = np.percentile(mean_col,70)\n",
    "    s = np.percentile(std_col,30)\n",
    "    return m,s\n",
    "\n",
    "\n",
    "def remove_3sd(rr_ppg_int):\n",
    "    ts_array = np.arange(rr_ppg_int[0,0],rr_ppg_int[-1,0],1000)\n",
    "    rr_interval = np.zeros((0,3))\n",
    "    for t in ts_array:\n",
    "        index = np.where((rr_ppg_int[:,0]>=t-4000)&(rr_ppg_int[:,0]<=t+4000))[0]\n",
    "        if len(index) < 1:\n",
    "            continue\n",
    "        if np.sum(rr_ppg_int[index,2])==0:\n",
    "            continue\n",
    "        mw,sw = weighted_avg_and_std(rr_ppg_int[index,1],rr_ppg_int[index,2])\n",
    "        rr_interval = np.concatenate((rr_interval,np.array([t,mw,np.mean(rr_ppg_int[index,2])]).reshape(-1,3)))\n",
    "    if len(rr_interval)<30:\n",
    "        return []\n",
    "    rr_ppg_int = rr_interval\n",
    "    return rr_ppg_int\n",
    "\n",
    "no_of_feature = 11\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data,qual=0):\n",
    "    if len(user_col)==0:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "#     try:\n",
    "    participant = user_col[0]\n",
    "    feature_matrix = []\n",
    "    feature_matrix_quality = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    quality_col = []\n",
    "#     if qual>=.4:\n",
    "#         heart_rate_final[heart_rate_final[:,2]>=.6,2] = 1\n",
    "#     heart_rate_final = heart_rate_final[heart_rate_final[:,2]>=qual]\n",
    "    \n",
    "    if len(heart_rate_final)<80:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "#     heart_rate_final = heart_rate_final[heart_rate_final[:,3]>0.001174897554939529]\n",
    "    heart_rate_final = heart_rate_final[heart_rate_final[:,2]>=qual]\n",
    "#     heart_rate_final = remove_3sd(heart_rate_final)\n",
    "    if len(heart_rate_final)<80:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "#     print(heart_rate_final.shape)\n",
    "    ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "#     m,s = get_ms(heart_rate_final)\n",
    "#     heart_rate_final[:,1] = (heart_rate_final[:,1]-m)/s\n",
    "    for t in ts_array:\n",
    "        index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        heart_rate_window = heart_rate_final[index]\n",
    "#         if np.median(heart_rate_window[:,2])<.1:\n",
    "#             continue\n",
    "        label = get_label(label_data,t-30000,t+30000)\n",
    "#         try:\n",
    "# #             r,tt = np.mean(heart_rate_window[:,1]),np.std(heart_rate_window[:,1])\n",
    "#             r,tt = weighted_avg_and_std(heart_rate_window[:,1],heart_rate_window[:,2])\n",
    "#         except:\n",
    "#             continue\n",
    "#         index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "#         heart_rate_window = heart_rate_window[index]\n",
    "        if len(heart_rate_window)<30:\n",
    "            continue\n",
    "        feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "#         try:\n",
    "        feature_freq = frequencyDomain(heart_rate_window[:,1]/1000,heart_rate_window[:,0]/1000)\n",
    "#         except:\n",
    "#             continue\n",
    "#             print(feature_freq,feature_freq.values())\n",
    "        feature = list(feature)+list(feature_freq.values())\n",
    "        feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "        feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "        feature_matrix_quality.append(np.array(feature_quality).reshape(-1,4))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(label)\n",
    "        quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "    return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,4)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,X_qual = X[index,:],groups[index],y[index],X_qual[index]\n",
    "    ind = []\n",
    "    for grp in np.unique(groups):\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1 and np.sum(y[tmp])/len(y[tmp])>=.1 and len(y[tmp])>20:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],X_qual[ind]\n",
    "\n",
    "def get_XY(window,qual1=0):\n",
    "    final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.v1.p','rb'))\n",
    "    duration = window\n",
    "#     final_output = [combine_data_sobc(*(a+[qual1])) for a in final_data]\n",
    "    final_output = Parallel(n_jobs=30,verbose=1)(delayed(combine_data_sobc)(*(a[:-1]+[qual1])) for i,a in enumerate(final_data) if i%2==0)\n",
    "    X = np.zeros((0,no_of_feature))\n",
    "    X_qual = []\n",
    "    y = []\n",
    "    groups = []\n",
    "#     qual = []\n",
    "    for m in final_output:\n",
    "        feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "        if len(feature_matrix)<10:\n",
    "            continue\n",
    "        quals1 = np.array([1]*feature_matrix.shape[0])\n",
    "#         quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "        ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "        rr_70th = np.percentile(ss,20)\n",
    "        rr_95th = np.percentile(ss,95)\n",
    "        index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "            feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "        tmp = StandardScaler().fit_transform(np.nan_to_num(feature_matrix))\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "#         print(X.shape)\n",
    "        X_qual.extend(list(quals1))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "#         qual.extend(quality_col)\n",
    "#     X[X<-4] = -4\n",
    "#     X[X>4] = 4\n",
    "    print(X.shape)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    X_qual = np.array(X_qual)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    groups = groups[~np.isnan(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "#     y = y[~np.isinf(X).any(axis=1)]\n",
    "#     qual = np.array(qual)\n",
    "#     qual = qual[~np.isinf(X).any(axis=1)]\n",
    "#     groups = groups[~np.isinf(X).any(axis=1)]\n",
    "#     X = X[~np.isinf(X).any(axis=1)]\n",
    "#     X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "    X,y,groups,X_qual = get_only_stress_no_stress(X,groups,y,X_qual)\n",
    "    print(X.shape)\n",
    "#     pickle.dump([X,y,groups,X_qual],open('lab_data.p','wb'))\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    print(qual1,X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(np.unique(groups)))\n",
    "#     data  = get_results_backward_elimination(X[:,:],y,groups)\n",
    "#     print(data)\n",
    "    m,f,p,r,clf,y,y_pred = get_f1(X[:,index],y,groups)    \n",
    "    print(np.array([duration,m,f,p,r]))\n",
    "    return np.array([qual1,duration,m,f,p,r,len(np.unique(groups)),X.shape[0]]),clf\n",
    "#     return [qual,data]\n",
    "# for window in np.arange(2,16,1):\n",
    "# results = Parallel(n_jobs=30,verbose=4)(delayed(get_XY)(window) for window in np.arange(2,16,1)[:1])\n",
    "\n",
    "# x_act,y_act,x_like,y_like = pickle.load(open('../code_stress/thresholds.p','rb'))\n",
    "# thresholds = np.zeros((len(x_like),3))\n",
    "# thresholds[:,0] = x_like\n",
    "# thresholds[:,1] = y_like\n",
    "# likelihoods = np.array(list(zip(x_act,y_act)))\n",
    "# for i in range(thresholds.shape[0]):\n",
    "#     yield_ = thresholds[i,1]\n",
    "#     index = np.abs(likelihoods[:,1]-yield_).argmin()\n",
    "#     activity_threshold = likelihoods[index,0]\n",
    "#     thresholds[i,2] = activity_threshold\n",
    "results = [get_XY(window=5,qual1=q) for q in [.05]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([thresholds[:,:],results],open('likelihood_threshold_lab_f12.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results,open('../data/stress_without_quality_without_normalization_new.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results[0][1],open('../models/stress_ppg_final_no_quality_0_qual.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../data/backward_elimination_with_quality_and_without_normalization_for_.2_threshold.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "conf = np.array([[2305,  299],\n",
    "         [ 248,  568]])\n",
    "plt.rcParams.update({\"font.size\":45})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(conf,fmt='g',annot=True,cbar=False)\n",
    "plt.xticks([0.5,1.5],['Not Stress','Stress'])\n",
    "plt.yticks([0.5,1.5],['Not \\nStress','Stress'],rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))\n",
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want,not_1ant\n",
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))\n",
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])\n",
    "final = []\n",
    "for i in range(11):\n",
    "    final.append([want[i][0],want[i][1],'Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "for i in range(11):\n",
    "    final.append([not_want[i][0],not_want[i][1],'Original Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "df = pd.DataFrame(final,columns=['Quality Threshold','F1 Score','Type of Features'])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.lineplot(x='Quality Threshold',y='F1 Score',hue='Type of Features',data=df, dashes=True)\n",
    "plt.plot(want[:,0],want[:,1],'--',marker='o',linewidth=3,markersize=20,label='Using Quality Weighted Features')\n",
    "plt.plot(not_want[:,0],not_want[:,1],'--',marker='s',linewidth=3,markersize=20,label='Using Normal Features')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Minimum Signal Quality')\n",
    "plt.legend()\n",
    "# plt.xticks(want[:,0],want[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final,columns=['Quality Threshold','F1 Score','Type of Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.lineplot(x='Quality Threshold',y='F1 Score',hue='Type of Features',data=df, dashes=True)\n",
    "plt.plot(want[:,0],want[:,1],'--',marker='o',linewidth=3,markersize=20,label='Using Quality Weighted Features')\n",
    "plt.plot(not_want[:,0],not_want[:,1],'--',marker='s',linewidth=3,markersize=20,label='Using Normal Features')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Minimum Signal Quality')\n",
    "plt.legend()\n",
    "# plt.xticks(want[:,0],want[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][1]\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size':25})\n",
    "feature_importances = np.array([['All features', 0.7104795737122557], \n",
    "                     ['Mean', 0.7176781002638521], \n",
    "                     ['Lf-Hf', 0.7166813768755516], \n",
    "                     ['Median', 0.7165775401069518], \n",
    "                     ['Vlf', 0.7184986595174263], \n",
    "                     ['Hf', 0.7225691347011596], \n",
    "                     ['20th', 0.717720391807658], \n",
    "                     ['Var', 0.7109170305676856], \n",
    "                     ['IQR', 0.6998254799301921], \n",
    "                     ['Lf', 0.6993865030674846], \n",
    "                     ['80th', 0.6678832116788321],\n",
    "                    ['Heart Rate',.65]])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(feature_importances[:6,0],[np.float(a) for a in feature_importances[:6,1]],'>--',c='g',linewidth=8,markersize=25)\n",
    "plt.plot(feature_importances[5:,0],[np.float(a) for a in feature_importances[5:,1]],'>--',c='r',linewidth=8,markersize=25)\n",
    "plt.ylabel('Leave one subject F1 score \\n Stress Classification')\n",
    "plt.yticks([.65,.67,.69,.71,.73],[0,.67,.69,.71,.73])\n",
    "# ax = plt.gca()\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# plt.title('Most Important Feature = Heart Rate')\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(r'Stepwise Feature Elimination  $\\Longrightarrow$')\n",
    "plt.savefig('../feature_elimination.pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "feature_importances[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open('../data/rice/stress_ppg_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_day_5/'\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "[['all', 0.7104795737122557], ['mean', 0.7176781002638521], ['lf-hf', 0.7166813768755516], ['median', 0.7165775401069518], ['vlf', 0.7184986595174263], ['hf', 0.7225691347011596], ['20th', 0.717720391807658], ['var', 0.7109170305676856], ['iqr', 0.6998254799301921], ['lf', 0.6993865030674846], ['80th', 0.6678832116788321]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,groups,qual = [],[],[]\n",
    "for f in os.listdir(directory1):\n",
    "    if f[-1]!='p':\n",
    "        continue\n",
    "    data = pickle.load(open(directory1+f,'rb'))[0]\n",
    "    print(data.shape,data[['final_feature_matrix','quality_mag']].dropna().shape)\n",
    "    ind = np.arange(data.shape[0])\n",
    "    ind = np.random.choice(ind,25)\n",
    "    X.append(np.array(list(data['final_feature_matrix']))[ind])\n",
    "    groups.extend([f]*ind.shape[0])\n",
    "    qual.extend(list(np.array(data['quality_mag'])[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([np.concatenate(X),np.array(groups),np.array(qual)],open('field_data.p','wb'))\n",
    "np.concatenate(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "\n",
    "plt.rcParams.update({'font.size':20})\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def get_f1(X,y,X_test,y_test):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "        'svc__class_weight':[{-1:delta,1:1-delta} for delta in np.arange(0,1,.1)],\n",
    "        'svc__bootstrap': [True, False],\n",
    "         'svc__max_features': [1],\n",
    "         'svc__n_estimators': [100,200,300,400,500]\n",
    "    }\n",
    "    clf = Pipeline([('sts',StandardScaler()),('svc',RandomForestClassifier())])\n",
    "    gkf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X, y)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.fit(X,y)\n",
    "    probs = clf.predict_proba(X_test)[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y_test, ret_bias=True)\n",
    "#     print(f1,bias,'bias')\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>=bias] = 1\n",
    "    y_pred[y_pred<bias] = -1\n",
    "    y_pred = np.int64(y_pred)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(16,8))\n",
    "    tmp = np.int64(confusion_matrix(y_test,y_pred))\n",
    "    sns.heatmap(tmp,annot=True,fmt='g',annot_kws={\"fontsize\":25})\n",
    "    plt.xticks([.5,1.5],['LAB','FIELD'])\n",
    "    plt.yticks([0.5,1.5],['LAB','FIELD'])\n",
    "    plt.show()\n",
    "#     print(accuracy_score(y_test,y_pred),np.mean(y),np.mean(y_test))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    return y_test,y_pred,confusion_matrix(y_test,y_pred),accuracy_score(y_test,y_pred),precision_score(y_test,y_pred),recall_score(y_test,y_pred),f1_score(y_test,y_pred)\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    \n",
    "    delta = 0.1\n",
    "    paramGrid = {'rf__kernel': ['rbf'],\n",
    "                 'rf__C': [100,10,200],\n",
    "                 'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .25)],\n",
    "                 'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3]],\n",
    "                 'rf__probability':[True]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "    print(data)\n",
    "    while len(feature_names)>1:\n",
    "        results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "        results = np.array(results)\n",
    "        print(results,results.shape)\n",
    "        ind_min = np.argmax(results[:,0])\n",
    "        min_f1 = results[ind_min,0]\n",
    "        min_index = np.int64(results[ind_min,1])\n",
    "        name_feature = feature_names[min_index]\n",
    "        data.append([name_feature,min_f1])\n",
    "        X = np.delete(X,min_index,axis=1)\n",
    "        feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "        print(data)\n",
    "    return data\n",
    "\n",
    "def partition_participants_lab(X,y,groups,qual,stop = 25):\n",
    "    lpgo = LeavePGroupsOut(n_groups=6)\n",
    "    lpgo.get_n_splits(X, y, groups)\n",
    "    count = 0\n",
    "    for train_index, test_index in lpgo.split(X, y, groups):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        qual_train, qual_test = qual[train_index], qual[test_index]\n",
    "        stress_train,stress_test = y[train_index],y[test_index]\n",
    "        groups_train,groups_test = groups[train_index],groups[test_index]\n",
    "        count+=1\n",
    "        if count==stop:\n",
    "            break\n",
    "#     print(X_train.shape,X_test.shape)\n",
    "    return [X_train,qual_train,groups_train,stress_train],[X_test,qual_test,groups_test,stress_test]\n",
    "\n",
    "def partition_participants_field(X,groups,qual,stop = 25):\n",
    "    lpgo = LeavePGroupsOut(n_groups=25)\n",
    "    lpgo.get_n_splits(X, qual, groups)\n",
    "    count = 0\n",
    "    for train_index, test_index in lpgo.split(X, qual, groups):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        qual_train, qual_test = qual[train_index], qual[test_index]\n",
    "        groups_train,groups_test = groups[train_index],groups[test_index]\n",
    "        count+=1\n",
    "        if count==stop:\n",
    "            break\n",
    "    \n",
    "    return [X_train,qual_train,groups_train],[X_test,qual_test,groups_test]\n",
    "\n",
    "def get_train_data(train_lab,train_field):\n",
    "    X_train,qual_train,groups_train,stress_train = train_lab\n",
    "    X_train_field,qual_train_field,groups_train_field = train_field\n",
    "    y_train = np.array([-1]*X_train.shape[0]+[1]*X_train_field.shape[0])\n",
    "    X_train = np.concatenate([X_train,X_train_field])\n",
    "    qual = np.array(list(qual_train) + list(qual_train_field))\n",
    "    groups_train = np.array(list(groups_train) + list(groups_train_field))\n",
    "#     print(X_train.shape,y_train.shape,qual.shape,np.unique(y_train))\n",
    "    return X_train,y_train,qual,groups_train\n",
    "\n",
    "def get_results(stop):\n",
    "    X_lab,y_lab,groups_lab,qual_lab = pickle.load(open('lab_data.p','rb'))\n",
    "    X_field,groups_field,qual_field = pickle.load(open('field_data.p','rb'))\n",
    "    train_lab,test_lab = partition_participants_lab(X_lab,y_lab,groups_lab,qual_lab,stop=stop)\n",
    "    train_field,test_field = partition_participants_field(X_field,groups_field,qual_field,stop=stop)\n",
    "    X_train,y_train,qual_train,groups_train = get_train_data(train_lab,train_field)\n",
    "    X_test,y_test,qual_test,groups_test = get_train_data(test_lab,test_field)\n",
    "    y_test,y_pred,conf,acc,precision,recall,f1 = get_f1(np.nan_to_num(X_train),y_train,np.nan_to_num(X_test),y_test)\n",
    "    return y_test,y_pred,qual_test,conf,acc,precision,recall,f1\n",
    "output = [get_results(i) for i in range(2,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(output,open('results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('results.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all,y_pred_all,qual_test_all = [],[],[]\n",
    "conf = np.zeros((2,2))\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in results:\n",
    "    y_test_all.extend(list(a[0]))\n",
    "    y_pred_all.extend(list(a[1]))\n",
    "    qual_test_all.extend(list(a[2]))\n",
    "    conf+=a[3]\n",
    "    scores.append(np.array(a[4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are  ~25 participants in lab and ~93 participants in field. \n",
    "Since each lab participant has around 100 instances (rows), we randomly sample 25 instances from every participant in field to balance the data.\n",
    "\n",
    "Now we obtain results by partitioning participants in train and test fold. (6 lab & 25 field participants in test fold). \n",
    "\n",
    "In training, we perform 10 fold stratified cross validation to maximize the AUC score. The random forest model found is then applied to the test fold. 98 such runs are made shuffling the participant-ids to create the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,8))\n",
    "tmp = np.int64(confusion_matrix(y_test_all,y_pred_all))//98\n",
    "sns.heatmap(tmp,annot=True,fmt='g',annot_kws={\"fontsize\":35})\n",
    "plt.xticks([.5,1.5],['LAB','FIELD'])\n",
    "plt.yticks([0.5,1.5],['LAB','FIELD'])\n",
    "plt.title('Average Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.boxplot(scores)\n",
    "plt.xticks([1,2,3,4],['accuracy','precision','recall','f1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.scatter(scores[:,1],scores[:,2],c=scores[:,3])\n",
    "plt.xlabel('PRECISION')\n",
    "plt.ylabel('RECALL')\n",
    "plt.title('PRECISION vs. RECALL (F1 in colorbar)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all,y_pred_all,qual_test_all = np.array(y_test_all),np.array(y_pred_all),np.array(qual_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "qscores,x = [],[]\n",
    "for i in np.arange(0,100,10):\n",
    "    index = np.where((qual_test_all>=i/100)&(qual_test_all<(i+10)/100))[0]\n",
    "    y = y_test_all[index]\n",
    "    y_pred = y_pred_all[index]\n",
    "    qscores.append(f1_score(y,y_pred))\n",
    "    x.append(str(np.mean(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.bar(x,np.array(qscores))\n",
    "plt.xlabel('Minute Level Quality')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models.cifar as models\n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 and 100 Training')\n",
    "# Datasets\n",
    "parser.add_argument('-d', '--dataset', default='cifar10', type=str)\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--train-batch', default=128, type=int, metavar='N',\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--test-batch', default=100, type=int, metavar='N',\n",
    "                    help='test batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--drop', '--dropout', default=0, type=float,\n",
    "                    metavar='Dropout', help='Dropout ratio')\n",
    "parser.add_argument('--schedule', type=int, nargs='+', default=[150, 225],\n",
    "                        help='Decrease learning rate at these epochs.')\n",
    "parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "# Checkpoints\n",
    "parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH',\n",
    "                    help='path to save checkpoint (default: checkpoint)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "# Architecture\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                        ' | '.join(model_names) +\n",
    "                        ' (default: resnet20)')\n",
    "parser.add_argument('--depth', type=int, default=20, help='Model depth.')\n",
    "parser.add_argument('--widen-factor', type=int, default=10, help='Widen factor. 10')\n",
    "parser.add_argument('--growthRate', type=int, default=12, help='Growth rate for DenseNet.')\n",
    "parser.add_argument('--compressionRate', type=int, default=2, help='Compression Rate (theta) for DenseNet.')\n",
    "# Miscs\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "\n",
    "# Random Erasing\n",
    "parser.add_argument('--p', default=0, type=float, help='Random Erasing probability')\n",
    "parser.add_argument('--sh', default=0.4, type=float, help='max erasing area')\n",
    "parser.add_argument('--r1', default=0.3, type=float, help='aspect of erasing area')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "window = 5\n",
    "final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
