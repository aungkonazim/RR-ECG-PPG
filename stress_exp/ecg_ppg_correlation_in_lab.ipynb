{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "window=5\n",
    "final_data1 = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.v1.p','rb'))\n",
    "final_data2 = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "for i,a in enumerate(final_data2):\n",
    "    a.append(final_data1[i][-1])\n",
    "    final_data.append(a)\n",
    "pickle.dump(final_data,open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.with.ecg.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i,a in enumerate(final_data):\n",
    "    if i%2!=0:\n",
    "        continue\n",
    "    ppg_data = a[-3]\n",
    "    total+= (ppg_data[-1,0]-ppg_data[0,0])/(1000*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.94038539672852"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mins = []\n",
    "for a in final_data:\n",
    "    if len(a[-1])==0:\n",
    "        continue\n",
    "    mins.append(np.min(a[-1][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = np.min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for i,a in enumerate(final_data):\n",
    "    if i%2!=0 or len(a[-1])==0:\n",
    "        continue\n",
    "    ecg_rr = a[-1]\n",
    "    sum_+=ecg_rr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.341388888888886"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  14 out of  36 | elapsed:    8.1s remaining:   12.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2701, 11)\n",
      "(2701, 3) 2701 0.2\n",
      "(1452, 11) (1452,) 28 (1452, 3) (1452,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  36 out of  36 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 28 folds for each of 144 candidates, totalling 4032 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1889 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3185 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4032 out of 4032 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7170370370370371 0.375473876664173 bias\n",
      "[[1019  102]\n",
      " [  89  242]] 0.2 0.7170370370370371 0.7034883720930233 0.7311178247734139\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler,QuantileTransformer\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "# from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "from copy import deepcopy\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=10,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=1)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    delta = 0.1\n",
    "    paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[False]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=1)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "    print(data)\n",
    "    while len(feature_names)>1:\n",
    "        results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "        results = np.array(results)\n",
    "        print(results,results.shape)\n",
    "        ind_min = np.argmax(results[:,0])\n",
    "        min_f1 = results[ind_min,0]\n",
    "        min_index = np.int64(results[ind_min,1])\n",
    "        name_feature = feature_names[min_index]\n",
    "        data.append([name_feature,min_f1])\n",
    "        X = np.delete(X,min_index,axis=1)\n",
    "        feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "        print(data)\n",
    "    print(data)\n",
    "    return data    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_f1(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "             'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[False]\n",
    "            }\n",
    "    my_score = make_scorer(f1Bias_scorer_CV,needs_proba=True)\n",
    "    clf = Pipeline([('svc',SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=2)\n",
    "    grid_search.fit(X[:,:],y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    print(f1,bias,'bias')\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>=bias] = 1\n",
    "    y_pred[y_pred<bias] = 0\n",
    "    y_pred = np.int64(y_pred)\n",
    "    clf.fit(X,y)\n",
    "    return confusion_matrix(y,y_pred),f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred),clf,y,y_pred\n",
    "    \n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.mean(a),np.percentile(a,50),np.min(a)]\n",
    "    return np.array(feature)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    NNs = RRints\n",
    "    tss = tmStamps\n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "#     a = a[:,0]\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "\n",
    "no_of_feature = 11\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data,ecg_rr,qual=0,indicator='ppg'):\n",
    "    if len(user_col)==0:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "    if indicator not in ['ppg']:\n",
    "        try:\n",
    "            heart_rate_final = np.zeros((ecg_rr.shape[0],3))\n",
    "            heart_rate_final[:,0] = ecg_rr[:,0]\n",
    "            heart_rate_final[:,1] = ecg_rr[:,1]\n",
    "            heart_rate_final[:,2] = 1\n",
    "        except:\n",
    "            return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "    participant = user_col[0]\n",
    "    feature_matrix = []\n",
    "    feature_matrix_quality = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    quality_col = []\n",
    "    if len(heart_rate_final)<80:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "    if len(heart_rate_final)<80:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "    ts_array = np.arange(min_value,heart_rate_final[-1,0],60000)\n",
    "    ts_array = [t for t in ts_array if t>=heart_rate_final[0,0]]\n",
    "    for t in ts_array:\n",
    "        index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        heart_rate_window = heart_rate_final[index]\n",
    "        label = get_label(label_data,t-30000,t+30000)\n",
    "        if len(heart_rate_window)<30:\n",
    "            continue\n",
    "        feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "        feature_freq = frequencyDomain(heart_rate_window[:,1]/1000,heart_rate_window[:,0]/1000)\n",
    "        feature = list(feature)+list(feature_freq.values())\n",
    "        feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "        if feature_quality[0]<qual:\n",
    "            continue\n",
    "        feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "        feature_matrix_quality.append(np.array(feature_quality).reshape(-1,3))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(label)\n",
    "        quality_col.append(t)\n",
    "    return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,3)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,X_qual,time_matching):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,X_qual,time_matching = X[index,:],groups[index],y[index],X_qual[index],time_matching[index]\n",
    "    ind = []\n",
    "    for grp in np.unique(groups):\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1 and np.sum(y[tmp])/len(y[tmp])>=.1 and len(y[tmp])>20:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],X_qual[ind],time_matching[ind]\n",
    "\n",
    "def get_XY(final_data,window,qual1=0,indicator='ppg'):\n",
    "    \n",
    "    duration = window\n",
    "    final_output = Parallel(n_jobs=30,verbose=1)(delayed(combine_data_sobc)(*(a+[qual1,indicator])) for i,a in enumerate(final_data) if i%2==0)\n",
    "    X = np.zeros((0,no_of_feature))\n",
    "    X_qual = []\n",
    "    y = []\n",
    "    groups = []\n",
    "    time_matching = []\n",
    "    for m in final_output:\n",
    "        feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "        if len(feature_matrix)<10:\n",
    "            continue\n",
    "        quals1 = np.array([1]*feature_matrix.shape[0])\n",
    "        ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "        rr_70th = np.percentile(ss,20)\n",
    "        rr_95th = np.percentile(ss,95)\n",
    "        index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "            feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "        tmp = StandardScaler().fit_transform(np.nan_to_num(feature_matrix))\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "        X_qual.extend(quals.reshape(1,-2,3))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "        time_matching.extend(list(quality_col))\n",
    "    print(X.shape)\n",
    "    time_matching = np.array(time_matching)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    X_qual = np.concatenate(X_qual)\n",
    "    print(X_qual.shape,len(time_matching),qual1)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    time_matching = time_matching[~np.isnan(X).any(axis=1)]\n",
    "    groups = groups[~np.isnan(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    X,y,groups,X_qual,time_matching = get_only_stress_no_stress(X,groups,y,X_qual,time_matching)\n",
    "    print(X.shape,y.shape,len(np.unique(groups)),X_qual.shape,time_matching.shape)\n",
    "    return X,y,groups,X_qual,time_matching\n",
    "\n",
    "no_of_feature = 11\n",
    "# feature_index = np.array([0,1,3,4,5,6,8])\n",
    "for q in [.2]:\n",
    "#     X,y,groups,X_qual,X_time = get_XY(final_data,window=5,qual1=q,indicator='ecg')\n",
    "#     m,f,p,r,clf,y,y_pred = get_f1(X,y,groups)\n",
    "#     pickle.dump([X,y,groups,X_qual,X_time,clf],open('ecg_data.p','wb'))\n",
    "    X,y,groups,X_qual,X_time = get_XY(final_data,window=5,qual1=q,indicator='ppg')\n",
    "    m,f,p,r,clf,y,y_pred = get_f1(X,y,groups)\n",
    "    print(m,q,f,p,r)\n",
    "#     pickle.dump([X,y,groups,X_qual,X_time,clf],open('ppg_data.p','wb'))\n",
    "    \n",
    "#     print(q,m,f,p,r)\n",
    "#     results.append([m,f,p,r,clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "[.62,.62,.62,.61,.64,.61,.64,.66,.70,.71]\n",
    "[.65,.65,.65,.65,.67,.64,.67,.68,.72,.73]\n",
    "[.64,.65,.65,.65,.66,.70,.73,.75,.79,.81]\n",
    "[.697,.697,.70,.699,.72,.73,.77,.77,.79,.81]\n",
    "[0,.05,.1,.15,.2,.25,.3,.35,.4,.45]\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "import pandas as pd\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "def get_dataframe(filename,type_of_stress='stress_ppg_weighted'):\n",
    "    X,y,groups,X_qual,X_time,clf = pickle.load(open(filename,'rb'))\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    data = pd.DataFrame({'time':X_time,'user':groups,'stress_likelihood':probs,'groundtruth':y,'quality_features':list(X_qual)})\n",
    "    data['type'] = type_of_stress\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ppg_not_weighted = get_dataframe('ppg_data_not_weighted.p',type_of_stress='stress_ppg_not_weighted')\n",
    "data_ppg = get_dataframe('ppg_data.p',type_of_stress='stress_ppg_weighted')\n",
    "data_ecg = get_dataframe('ecg_data.p',type_of_stress='stress_ecg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ppg = pd.concat([data_ppg,data_ppg_not_weighted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_col = []\n",
    "for user in data_ppg.user.unique():\n",
    "    tmp_ppg = data_ppg[data_ppg.user==user]\n",
    "    tmp_ecg = data_ecg[data_ecg.user==user]\n",
    "    tmp_ecg['stress_likelihood_ecg'] = tmp_ecg['stress_likelihood']\n",
    "    tmp_ppg = pd.merge(tmp_ppg, tmp_ecg[['stress_likelihood_ecg','time']], how='inner',on=['time'])\n",
    "    data_col.append(tmp_ppg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat(data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1Bias_scorer_CV(data_all['groundtruth'],data_all['stress_likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1310,), (2620, 7), (2620, 7), (1481, 6), (3552, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.time.unique().shape,data_all.shape,data_all.dropna().shape,data_ecg.shape,data_ppg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all.dropna()[-200:]\n",
    "# 7096+2956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7796307510615962, 3.872171784974957e-268)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "df_weighted = [df for i,df in data_all.groupby('type',as_index=False)][1]\n",
    "pearsonr(df_weighted['stress_likelihood'],df_weighted['stress_likelihood_ecg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7316869545323773, 5.883671307762137e-220)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "df_weighted = [df for i,df in data_all.groupby('type',as_index=False)][0]\n",
    "pearsonr(df_weighted['stress_likelihood'],df_weighted['stress_likelihood_ecg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['min'] = data_all['quality_features'].apply(lambda a:a[2])\n",
    "data_all['median'] = data_all['quality_features'].apply(lambda a:a[1])\n",
    "data_all['mean'] = data_all['quality_features'].apply(lambda a:a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_feature_names = ['mean']\n",
    "quality_range = np.arange(0,.85,.05)\n",
    "def get_corr(df):\n",
    "    all_data = []\n",
    "    type_of_stress,user = df['type'].values[0],df['user'].values[0]\n",
    "    for i,qname in enumerate(quality_feature_names):\n",
    "        for q in quality_range:\n",
    "            tmp_df = df[df[qname]>=q]\n",
    "            if tmp_df.shape[0]<5:\n",
    "                continue\n",
    "            corr = pearsonr(tmp_df['stress_likelihood_ecg'],tmp_df['stress_likelihood'])[0]\n",
    "            all_data.append([type_of_stress,user,qname,q,corr])\n",
    "    return pd.DataFrame(all_data,columns=['type','user','qualityname','qualityvalue','corr'])\n",
    "data_corr = data_all.groupby(['user','type'],as_index=False).apply(get_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>qualitytype</th>\n",
       "      <th>qualityvalue</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.0</td>\n",
       "      <td>0.78$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.0</td>\n",
       "      <td>0.85$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.05</td>\n",
       "      <td>0.78$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.05</td>\n",
       "      <td>0.85$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.1</td>\n",
       "      <td>0.78$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.1</td>\n",
       "      <td>0.85$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.15000000000000002</td>\n",
       "      <td>0.78$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.15000000000000002</td>\n",
       "      <td>0.83$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.2</td>\n",
       "      <td>0.79$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.2</td>\n",
       "      <td>0.85$\\pm$0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.25</td>\n",
       "      <td>0.79$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.25</td>\n",
       "      <td>0.86$\\pm$0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.30000000000000004</td>\n",
       "      <td>0.82$\\pm$0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.30000000000000004</td>\n",
       "      <td>0.86$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.35000000000000003</td>\n",
       "      <td>0.83$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.35000000000000003</td>\n",
       "      <td>0.89$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.4</td>\n",
       "      <td>0.84$\\pm$0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.4</td>\n",
       "      <td>0.92$\\pm$0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.45</td>\n",
       "      <td>0.87$\\pm$0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.45</td>\n",
       "      <td>0.90$\\pm$0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.5</td>\n",
       "      <td>0.82$\\pm$0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.5</td>\n",
       "      <td>0.84$\\pm$0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.55</td>\n",
       "      <td>0.81$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.55</td>\n",
       "      <td>0.84$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.6000000000000001</td>\n",
       "      <td>0.84$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.6000000000000001</td>\n",
       "      <td>0.81$\\pm$0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.65</td>\n",
       "      <td>0.90$\\pm$0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.65</td>\n",
       "      <td>0.87$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.7000000000000001</td>\n",
       "      <td>0.91$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.7000000000000001</td>\n",
       "      <td>0.91$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.75</td>\n",
       "      <td>0.75$\\pm$0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.75</td>\n",
       "      <td>0.77$\\pm$0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.8</td>\n",
       "      <td>0.68$\\pm$0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_weighted</td>\n",
       "      <td>mean</td>\n",
       "      <td>$\\ge$0.8</td>\n",
       "      <td>0.70$\\pm$0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         type qualitytype              qualityvalue  \\\n",
       "0  0  stress_ppg_not_weighted        mean                  $\\ge$0.0   \n",
       "1  0      stress_ppg_weighted        mean                  $\\ge$0.0   \n",
       "2  0  stress_ppg_not_weighted        mean                 $\\ge$0.05   \n",
       "3  0      stress_ppg_weighted        mean                 $\\ge$0.05   \n",
       "4  0  stress_ppg_not_weighted        mean                  $\\ge$0.1   \n",
       "5  0      stress_ppg_weighted        mean                  $\\ge$0.1   \n",
       "6  0  stress_ppg_not_weighted        mean  $\\ge$0.15000000000000002   \n",
       "7  0      stress_ppg_weighted        mean  $\\ge$0.15000000000000002   \n",
       "8  0  stress_ppg_not_weighted        mean                  $\\ge$0.2   \n",
       "9  0      stress_ppg_weighted        mean                  $\\ge$0.2   \n",
       "10 0  stress_ppg_not_weighted        mean                 $\\ge$0.25   \n",
       "11 0      stress_ppg_weighted        mean                 $\\ge$0.25   \n",
       "12 0  stress_ppg_not_weighted        mean  $\\ge$0.30000000000000004   \n",
       "13 0      stress_ppg_weighted        mean  $\\ge$0.30000000000000004   \n",
       "14 0  stress_ppg_not_weighted        mean  $\\ge$0.35000000000000003   \n",
       "15 0      stress_ppg_weighted        mean  $\\ge$0.35000000000000003   \n",
       "16 0  stress_ppg_not_weighted        mean                  $\\ge$0.4   \n",
       "17 0      stress_ppg_weighted        mean                  $\\ge$0.4   \n",
       "18 0  stress_ppg_not_weighted        mean                 $\\ge$0.45   \n",
       "19 0      stress_ppg_weighted        mean                 $\\ge$0.45   \n",
       "20 0  stress_ppg_not_weighted        mean                  $\\ge$0.5   \n",
       "21 0      stress_ppg_weighted        mean                  $\\ge$0.5   \n",
       "22 0  stress_ppg_not_weighted        mean                 $\\ge$0.55   \n",
       "23 0      stress_ppg_weighted        mean                 $\\ge$0.55   \n",
       "24 0  stress_ppg_not_weighted        mean   $\\ge$0.6000000000000001   \n",
       "25 0      stress_ppg_weighted        mean   $\\ge$0.6000000000000001   \n",
       "26 0  stress_ppg_not_weighted        mean                 $\\ge$0.65   \n",
       "27 0      stress_ppg_weighted        mean                 $\\ge$0.65   \n",
       "28 0  stress_ppg_not_weighted        mean   $\\ge$0.7000000000000001   \n",
       "29 0      stress_ppg_weighted        mean   $\\ge$0.7000000000000001   \n",
       "30 0  stress_ppg_not_weighted        mean                 $\\ge$0.75   \n",
       "31 0      stress_ppg_weighted        mean                 $\\ge$0.75   \n",
       "32 0  stress_ppg_not_weighted        mean                  $\\ge$0.8   \n",
       "33 0      stress_ppg_weighted        mean                  $\\ge$0.8   \n",
       "\n",
       "               corr  \n",
       "0  0  0.78$\\pm$0.19  \n",
       "1  0  0.85$\\pm$0.17  \n",
       "2  0  0.78$\\pm$0.19  \n",
       "3  0  0.85$\\pm$0.17  \n",
       "4  0  0.78$\\pm$0.19  \n",
       "5  0  0.85$\\pm$0.17  \n",
       "6  0  0.78$\\pm$0.19  \n",
       "7  0  0.83$\\pm$0.17  \n",
       "8  0  0.79$\\pm$0.18  \n",
       "9  0  0.85$\\pm$0.16  \n",
       "10 0  0.79$\\pm$0.19  \n",
       "11 0  0.86$\\pm$0.15  \n",
       "12 0  0.82$\\pm$0.20  \n",
       "13 0  0.86$\\pm$0.18  \n",
       "14 0  0.83$\\pm$0.19  \n",
       "15 0  0.89$\\pm$0.17  \n",
       "16 0  0.84$\\pm$0.10  \n",
       "17 0  0.92$\\pm$0.09  \n",
       "18 0  0.87$\\pm$0.14  \n",
       "19 0  0.90$\\pm$0.15  \n",
       "20 0  0.82$\\pm$0.16  \n",
       "21 0  0.84$\\pm$0.16  \n",
       "22 0  0.81$\\pm$0.19  \n",
       "23 0  0.84$\\pm$0.18  \n",
       "24 0  0.84$\\pm$0.18  \n",
       "25 0  0.81$\\pm$0.16  \n",
       "26 0  0.90$\\pm$0.17  \n",
       "27 0  0.87$\\pm$0.18  \n",
       "28 0  0.91$\\pm$0.18  \n",
       "29 0  0.91$\\pm$0.19  \n",
       "30 0  0.75$\\pm$0.21  \n",
       "31 0  0.77$\\pm$0.19  \n",
       "32 0  0.68$\\pm$0.18  \n",
       "33 0  0.70$\\pm$0.19  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = ['corr']\n",
    "\n",
    "def get_mean_std(df):\n",
    "    t = df['qualityname'].values[0]\n",
    "    v = '$\\ge$'+str(df['qualityvalue'].values[0])\n",
    "    tt = df['type'].values[0]\n",
    "    means = []\n",
    "    for f in feature_names:\n",
    "        m = np.median(df[f])\n",
    "        s = np.std(df[f])/1.7\n",
    "        value = \"{:.2f}\".format(m)+\"$\\pm$\"+\"{:.2f}\".format(s)\n",
    "        means.append(value)\n",
    "    return pd.DataFrame([[tt,t,v]+means],columns=['type','qualitytype','qualityvalue']+feature_names)\n",
    "# final_corr.groupby(['Quality Type','Quality Value'],as_index=False).apply(get_mean_std).reset_index(drop=True).to_csv('field_features.csv')\n",
    "\n",
    "data_corr.groupby(['qualityvalue','type','qualityname'],as_index=False).apply(get_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>qualityname</th>\n",
       "      <th>qualityvalue</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>063b1819-aaa3-4368-8430-6ce1a665c1d3</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.581240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>063b1819-aaa3-4368-8430-6ce1a665c1d3</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.581240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>063b1819-aaa3-4368-8430-6ce1a665c1d3</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.581240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>063b1819-aaa3-4368-8430-6ce1a665c1d3</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.571741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress_ppg_not_weighted</td>\n",
       "      <td>063b1819-aaa3-4368-8430-6ce1a665c1d3</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.571741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        type                                  user  \\\n",
       "0 0  stress_ppg_not_weighted  063b1819-aaa3-4368-8430-6ce1a665c1d3   \n",
       "  1  stress_ppg_not_weighted  063b1819-aaa3-4368-8430-6ce1a665c1d3   \n",
       "  2  stress_ppg_not_weighted  063b1819-aaa3-4368-8430-6ce1a665c1d3   \n",
       "  3  stress_ppg_not_weighted  063b1819-aaa3-4368-8430-6ce1a665c1d3   \n",
       "  4  stress_ppg_not_weighted  063b1819-aaa3-4368-8430-6ce1a665c1d3   \n",
       "\n",
       "    qualityname  qualityvalue      corr  \n",
       "0 0        mean          0.00  0.581240  \n",
       "  1        mean          0.05  0.581240  \n",
       "  2        mean          0.10  0.581240  \n",
       "  3        mean          0.15  0.571741  \n",
       "  4        mean          0.20  0.571741  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
