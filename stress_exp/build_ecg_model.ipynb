{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot reshape array of size 1547 into shape (11)\n",
      "File b'/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance///eb672766-4202-4f8d-8cf3-8bf15425d5b0/20180406/ecg_final_data.csv' does not exist\n",
      "File b'/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance///b191d2d6-a307-4a50-9661-0d9db3d8562c/20180228/ecg_final_data.csv' does not exist\n",
      "cannot reshape array of size 966 into shape (11)\n",
      "cannot reshape array of size 1575 into shape (11)\n",
      "cannot reshape array of size 840 into shape (11)\n",
      "cannot reshape array of size 1575 into shape (11)\n",
      "cannot reshape array of size 1575 into shape (11)\n",
      "cannot reshape array of size 1435 into shape (11)\n",
      "cannot reshape array of size 1498 into shape (11)\n",
      "cannot reshape array of size 1792 into shape (11)\n",
      "File b'/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance///50af9985-2197-4e1e-bee3-556816f446a1/20180501/ecg_final_data.csv' does not exist\n",
      "cannot reshape array of size 1085 into shape (11)\n",
      "cannot reshape array of size 1484 into shape (11)\n",
      "cannot reshape array of size 1449 into shape (11)\n",
      "cannot reshape array of size 567 into shape (11)\n",
      "cannot reshape array of size 1176 into shape (11)\n",
      "cannot reshape array of size 1141 into shape (11)\n",
      "cannot reshape array of size 1260 into shape (11)\n",
      "cannot reshape array of size 889 into shape (11)\n",
      "File b'/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance///4c086d48-1d68-4409-8411-55eab3166087/20180627/ecg_final_data.csv' does not exist\n",
      "(133, 11)\n",
      "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7 and the array at index 1 has size 11\n",
      "(63, 11)\n",
      "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7 and the array at index 1 has size 11\n",
      "cannot reshape array of size 1687 into shape (11)\n",
      "File b'/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance///a5d58ebf-379b-4f31-b13e-0e54ed535a0e/20180629/ecg_final_data.csv' does not exist\n",
      "cannot reshape array of size 952 into shape (11)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8f21ce62b906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;31m#             ecg_rr[:,1] = RobustScaler(quantile_range=(20,80)).fit_transform(ecg_rr[:,1:]).reshape(len(ecg_rr),)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mwindow_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mts_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ecg_windowss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecg_rr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_data_sobc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mts_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f21ce62b906>\u001b[0m in \u001b[0;36mcombine_data_sobc\u001b[0;34m(window_col, ts_col, label_data, participant)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;31m#             continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;31m#         feature = ecg_feature_computation(item[:,0]/1000,item[:,1]/1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rr_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;31m#         f = get_time_domain_features(item[:,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;31m#         feature.extend(list(f.values()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f21ce62b906>\u001b[0m in \u001b[0;36mget_rr_features\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_rr_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3701\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3702\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# undo any decay that the ufunc performed (see gh-13105)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3703\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_quantile_is_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3704\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     return _quantile_unchecked(\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_is_valid\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m   3839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3840\u001b[0m         \u001b[0;31m# faster than any()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3841\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3842\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3843\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from ecg import ecg_feature_computation\n",
    "import matplotlib.pyplot as plt\n",
    "from hrvanalysis import get_time_domain_features,get_geometrical_features,get_csi_cvi_features,get_poincare_plot_features\n",
    "from hrvanalysis import get_frequency_domain_features\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Quality(Enum):\n",
    "    ACCEPTABLE = 1\n",
    "    UNACCEPTABLE = 0\n",
    "\n",
    "def outlier_computation(valid_rr_interval_time: list,\n",
    "                        valid_rr_interval_sample: list,\n",
    "                        criterion_beat_difference: float):\n",
    "    \"\"\"\n",
    "    This function implements the rr interval outlier calculation through comparison with the criterion\n",
    "    beat difference and consecutive differences with the previous and next sample\n",
    "\n",
    "    :param valid_rr_interval_time: A python array of rr interval time\n",
    "    :param valid_rr_interval_sample: A python array of rr interval samples\n",
    "    :param criterion_beat_difference: A threshold calculated from the RR interval data passed\n",
    "\n",
    "    yields: The quality of each data point in the RR interval array\n",
    "    \"\"\"\n",
    "    standard_rr_interval_sample = valid_rr_interval_sample[0]\n",
    "    previous_rr_interval_quality = Quality.ACCEPTABLE\n",
    "\n",
    "    for i in range(1, len(valid_rr_interval_sample) - 1):\n",
    "\n",
    "        rr_interval_diff_with_last_good = abs(standard_rr_interval_sample - valid_rr_interval_sample[i])\n",
    "        rr_interval_diff_with_prev_sample = abs(valid_rr_interval_sample[i - 1] - valid_rr_interval_sample[i])\n",
    "        rr_interval_diff_with_next_sample = abs(valid_rr_interval_sample[i] - valid_rr_interval_sample[i + 1])\n",
    "\n",
    "        if previous_rr_interval_quality == Quality.UNACCEPTABLE and rr_interval_diff_with_last_good < criterion_beat_difference:\n",
    "            yield (valid_rr_interval_time[i], Quality.ACCEPTABLE)\n",
    "            previous_rr_interval_quality = Quality.ACCEPTABLE\n",
    "            standard_rr_interval_sample = valid_rr_interval_sample[i]\n",
    "\n",
    "        elif previous_rr_interval_quality == Quality.UNACCEPTABLE and rr_interval_diff_with_last_good > criterion_beat_difference >= rr_interval_diff_with_prev_sample and rr_interval_diff_with_next_sample <= criterion_beat_difference:\n",
    "            yield (valid_rr_interval_time[i], Quality.ACCEPTABLE)\n",
    "            previous_rr_interval_quality = Quality.ACCEPTABLE\n",
    "            standard_rr_interval_sample = valid_rr_interval_sample[i]\n",
    "\n",
    "        elif previous_rr_interval_quality == Quality.UNACCEPTABLE and rr_interval_diff_with_last_good > criterion_beat_difference and (\n",
    "                        rr_interval_diff_with_prev_sample > criterion_beat_difference or rr_interval_diff_with_next_sample > criterion_beat_difference):\n",
    "            yield (valid_rr_interval_time[i], Quality.UNACCEPTABLE)\n",
    "            previous_rr_interval_quality = Quality.UNACCEPTABLE\n",
    "\n",
    "        elif previous_rr_interval_quality == Quality.ACCEPTABLE and rr_interval_diff_with_prev_sample <= criterion_beat_difference:\n",
    "            yield (valid_rr_interval_time[i], Quality.ACCEPTABLE)\n",
    "            previous_rr_interval_quality = Quality.ACCEPTABLE\n",
    "            standard_rr_interval_sample = valid_rr_interval_sample[i]\n",
    "\n",
    "        elif previous_rr_interval_quality == Quality.ACCEPTABLE and rr_interval_diff_with_prev_sample > criterion_beat_difference:\n",
    "            yield (valid_rr_interval_time[i], Quality.UNACCEPTABLE)\n",
    "            previous_rr_interval_quality = Quality.UNACCEPTABLE\n",
    "\n",
    "        else:\n",
    "            yield (valid_rr_interval_time[i], Quality.UNACCEPTABLE)\n",
    "\n",
    "\n",
    "def compute_outlier_ecg(ecg_ts,ecg_rr):\n",
    "    \"\"\"\n",
    "    Reference - Berntson, Gary G., et al. \"An approach to artifact identification: Application to heart period data.\"\n",
    "    Psychophysiology 27.5 (1990): 586-598.\n",
    "\n",
    "    :param ecg_rr: RR interval datastream\n",
    "\n",
    "    :return: An annotated datastream specifying when the ECG RR interval datastream is acceptable\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    valid_rr_interval_sample = [i for i in ecg_rr if i > .3 and i < 2]\n",
    "    valid_rr_interval_time = [ecg_ts[i] for i in range(len(ecg_ts)) if ecg_rr[i] > .3 and ecg_rr[i] < 2]\n",
    "    valid_rr_interval_difference = abs(np.diff(valid_rr_interval_sample))\n",
    "\n",
    "    # Maximum Expected Difference(MED)= 3.32* Quartile Deviation\n",
    "    maximum_expected_difference = 4.5 * 0.5 * iqr(valid_rr_interval_difference)\n",
    "\n",
    "    # Shortest Expected Beat(SEB) = Median Beat – 2.9 * Quartile Deviation\n",
    "    # Minimal Artifact Difference(MAD) = SEB/ 3\n",
    "    maximum_artifact_difference = (np.median(valid_rr_interval_sample) - 2.9 * .5 * iqr(\n",
    "        valid_rr_interval_difference)) / 3\n",
    "\n",
    "    # Midway between MED and MAD is considered\n",
    "    criterion_beat_difference = (maximum_expected_difference + maximum_artifact_difference) / 2\n",
    "    if criterion_beat_difference < .2:\n",
    "        criterion_beat_difference = .2\n",
    "\n",
    "    ecg_rr_quality_array = [(valid_rr_interval_time[0], Quality.ACCEPTABLE)]\n",
    "\n",
    "    for data in outlier_computation(valid_rr_interval_time, valid_rr_interval_sample, criterion_beat_difference):\n",
    "        ecg_rr_quality_array.append(data)\n",
    "    ecg_rr_quality_array.append((valid_rr_interval_time[-1], Quality.ACCEPTABLE))\n",
    "    return ecg_rr_quality_array\n",
    "\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Static name for methods params\n",
    "MALIK_RULE = \"malik\"\n",
    "KARLSSON_RULE = \"karlsson\"\n",
    "KAMATH_RULE = \"kamath\"\n",
    "ACAR_RULE = \"acar\"\n",
    "CUSTOM_RULE = \"custom\"\n",
    "\n",
    "def remove_ectopic_beats(rr_intervals: List[float], method: str = \"malik\",\n",
    "                         custom_removing_rule: float = 0.2, verbose: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    RR-intervals differing by more than the removing_rule from the one proceeding it are removed.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    rr_intervals : list\n",
    "        list of RR-intervals\n",
    "    method : str\n",
    "        method to use to clean outlier. malik, kamath, karlsson, acar or custom.\n",
    "    custom_removing_rule : int\n",
    "        Percentage criteria of difference with previous RR-interval at which we consider\n",
    "        that it is abnormal. If method is set to Karlsson, it is the percentage of difference\n",
    "        between the absolute mean of previous and next RR-interval at which  to consider the beat\n",
    "        as abnormal.\n",
    "    verbose : bool\n",
    "        Print information about ectopic beats.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    nn_intervals : list\n",
    "        list of NN Interval\n",
    "    outlier_count : int\n",
    "        Count of outlier detected in RR-interval list\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [5] Kamath M.V., Fallen E.L.: Correction of the Heart Rate Variability Signal for Ectopics \\\n",
    "    and Miss- ing Beats, In: Malik M., Camm A.J.\n",
    "\n",
    "    .. [6] Geometric Methods for Heart Rate Variability Assessment - Malik M et al\n",
    "    \"\"\"\n",
    "    if method not in [MALIK_RULE, KAMATH_RULE, KARLSSON_RULE, ACAR_RULE, CUSTOM_RULE]:\n",
    "        raise ValueError(\"Not a valid method. Please choose between malik, kamath, karlsson, acar.\\\n",
    "         You can also choose your own removing critera with custom_rule parameter.\")\n",
    "\n",
    "    if method == KARLSSON_RULE:\n",
    "        nn_intervals, outlier_count = _remove_outlier_karlsson(rr_intervals=rr_intervals,\n",
    "                                                               removing_rule=custom_removing_rule)\n",
    "\n",
    "    elif method == ACAR_RULE:\n",
    "        nn_intervals, outlier_count = _remove_outlier_acar(rr_intervals=rr_intervals)\n",
    "\n",
    "    else:\n",
    "        # set first element in list\n",
    "        outlier_count = 0\n",
    "        previous_outlier = False\n",
    "        nn_intervals = [rr_intervals[0]]\n",
    "        for i, rr_interval in enumerate(rr_intervals[:-1]):\n",
    "\n",
    "            if previous_outlier:\n",
    "                nn_intervals.append(rr_intervals[i + 1])\n",
    "                previous_outlier = False\n",
    "                continue\n",
    "\n",
    "            if is_outlier(rr_interval, rr_intervals[i + 1], method=method,\n",
    "                          custom_rule=custom_removing_rule):\n",
    "                nn_intervals.append(rr_intervals[i + 1])\n",
    "            else:\n",
    "                nn_intervals.append(np.nan)\n",
    "                outlier_count += 1\n",
    "                previous_outlier = True\n",
    "\n",
    "    if verbose :\n",
    "        print(\"{} ectopic beat(s) have been deleted with {} rule.\".format(outlier_count, method))\n",
    "\n",
    "    return nn_intervals\n",
    "\n",
    "\n",
    "def is_outlier(rr_interval: int, next_rr_interval: float, method: str = \"malik\",\n",
    "               custom_rule: float = 0.2) -> bool:\n",
    "    \"\"\"\n",
    "    Test if the rr_interval is an outlier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rr_interval : int\n",
    "        RrInterval\n",
    "    next_rr_interval : int\n",
    "        consecutive RrInterval\n",
    "    method : str\n",
    "        method to use to clean outlier. malik, kamath, karlsson, acar or custom\n",
    "    custom_rule : int\n",
    "        percentage criteria of difference with previous RR-interval at which we consider\n",
    "        that it is abnormal\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    outlier : bool\n",
    "        True if RrInterval is valid, False if not\n",
    "    \"\"\"\n",
    "    if method == MALIK_RULE:\n",
    "        outlier = abs(rr_interval - next_rr_interval) <= 0.2 * rr_interval\n",
    "    elif method == KAMATH_RULE:\n",
    "        outlier = 0 <= (next_rr_interval - rr_interval) <= 0.325 * rr_interval or 0 <= \\\n",
    "                  (rr_interval - next_rr_interval) <= 0.245 * rr_interval\n",
    "    else:\n",
    "        outlier = abs(rr_interval - next_rr_interval) <= custom_rule * rr_interval\n",
    "    return outlier\n",
    "\n",
    "\n",
    "def _remove_outlier_karlsson(rr_intervals: List[float], removing_rule: float = 0.2) -> Tuple[list, int]:\n",
    "    \"\"\"\n",
    "    RR-intervals differing by more than the 20 % of the mean of previous and next RR-interval\n",
    "    are removed.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    rr_intervals : list\n",
    "        list of RR-intervals\n",
    "    removing_rule : float\n",
    "        Percentage of difference between the absolute mean of previous and next RR-interval at which \\\n",
    "    to consider the beat as abnormal.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    nn_intervals : list\n",
    "        list of NN Interval\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [7]  Automatic filtering of outliers in RR-intervals before analysis of heart rate \\\n",
    "    variability in Holter recordings: a comparison with carefully edited data - Marcus Karlsson, \\\n",
    "    Rolf Hörnsten, Annika Rydberg and Urban Wiklund\n",
    "    \"\"\"\n",
    "    # set first element in list\n",
    "    nn_intervals = [rr_intervals[0]]\n",
    "    outlier_count = 0\n",
    "\n",
    "    for i in range(len(rr_intervals)):\n",
    "        # Condition to go out of loop at limits of list\n",
    "        if i == len(rr_intervals)-2:\n",
    "            nn_intervals.append(rr_intervals[i + 1])\n",
    "            break\n",
    "        mean_prev_next_rri = (rr_intervals[i] + rr_intervals[i+2]) / 2\n",
    "        if abs(mean_prev_next_rri - rr_intervals[i+1]) < removing_rule * mean_prev_next_rri:\n",
    "            nn_intervals.append(rr_intervals[i+1])\n",
    "        else:\n",
    "            nn_intervals.append(np.nan)\n",
    "            outlier_count += 1\n",
    "    return nn_intervals, outlier_count\n",
    "\n",
    "\n",
    "def _remove_outlier_acar(rr_intervals: List[float], custom_rule=0.2) -> Tuple[list, int]:\n",
    "    \"\"\"\n",
    "    RR-intervals differing by more than the 20 % of the mean of last 9 RrIntervals\n",
    "    are removed.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    rr_intervals : list\n",
    "        list of RR-intervals\n",
    "    custom_rule : int\n",
    "        percentage criteria of difference with mean of  9 previous RR-intervals at\n",
    "        which we consider that RR-interval is abnormal. By default, set to 20 %\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    nn_intervals : list\n",
    "        list of NN Interval\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [8] Automatic ectopic beat elimination in short-term heart rate variability measurements \\\n",
    "    Acar B., Irina S., Hemingway H., Malik M.\n",
    "    \"\"\"\n",
    "    nn_intervals = []\n",
    "    outlier_count = 0\n",
    "    for i, rr_interval in enumerate(rr_intervals):\n",
    "        if i < 9:\n",
    "            nn_intervals.append(rr_interval)\n",
    "            continue\n",
    "        acar_rule_elt = np.nanmean(nn_intervals[-9:])\n",
    "        if abs(acar_rule_elt - rr_interval) < custom_rule * acar_rule_elt:\n",
    "            nn_intervals.append(rr_interval)\n",
    "        else:\n",
    "            nn_intervals.append(np.nan)\n",
    "            outlier_count += 1\n",
    "    return nn_intervals, outlier_count\n",
    "\n",
    "\n",
    "\n",
    "def lomb(time_stamps:List,\n",
    "         samples:List,\n",
    "         low_frequency: float,\n",
    "         high_frequency: float):\n",
    "    \"\"\"\n",
    "   : Lomb–Scargle periodogram implementation\n",
    "    :param data: List[DataPoint]\n",
    "    :param high_frequency: float\n",
    "    :param low_frequency: float\n",
    "    :return lomb-scargle pgram and frequency values\n",
    "    \"\"\"\n",
    "\n",
    "    frequency_range = np.linspace(low_frequency, high_frequency, len(time_stamps))\n",
    "    result = signal.lombscargle(time_stamps, samples, frequency_range)\n",
    "    return result, frequency_range\n",
    "\n",
    "\n",
    "def heart_rate_power(power: np.ndarray,\n",
    "                     frequency: np.ndarray,\n",
    "                     low_rate: float,\n",
    "                     high_rate: float):\n",
    "    \"\"\"\n",
    "    Compute Heart Rate Power for specific frequency range\n",
    "    :param power: np.ndarray\n",
    "    :param frequency: np.ndarray\n",
    "    :param high_rate: float\n",
    "    :param low_rate: float\n",
    "    :return: sum of power for the frequency range\n",
    "    \"\"\"\n",
    "    result_power = float(0.0)\n",
    "    for i, value in enumerate(power):\n",
    "        if low_rate <= frequency[i] <= high_rate:\n",
    "            result_power += value\n",
    "    return result_power\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ecg_feature_computation(timestamp:list,\n",
    "                            value:list,\n",
    "                            low_frequency: float = 0.01,\n",
    "                            high_frequency: float = 0.7,\n",
    "                            low_rate_vlf: float = 0.0009,\n",
    "                            high_rate_vlf: float = 0.04,\n",
    "                            low_rate_hf: float = 0.15,\n",
    "                            high_rate_hf: float = 0.4,\n",
    "                            low_rate_lf: float = 0.04,\n",
    "                            high_rate_lf: float = 0.15):\n",
    "    \"\"\"\n",
    "    ECG Feature Implementation. The frequency ranges for High, Low and Very low heart rate variability values are\n",
    "    derived from the following paper:\n",
    "    'Heart rate variability: standards of measurement, physiological interpretation and clinical use'\n",
    "    :param high_rate_lf: float\n",
    "    :param low_rate_lf: float\n",
    "    :param high_rate_hf: float\n",
    "    :param low_rate_hf: float\n",
    "    :param high_rate_vlf: float\n",
    "    :param low_rate_vlf: float\n",
    "    :param high_frequency: float\n",
    "    :param low_frequency: float\n",
    "    :param datastream: DataStream\n",
    "    :param window_size: float\n",
    "    :param window_offset: float\n",
    "    :return: ECG Feature DataStreams\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # perform windowing of datastream\n",
    "\n",
    "\n",
    "    # initialize each ecg feature array\n",
    "\n",
    "    rr_variance_data = []\n",
    "    rr_mean_data = []\n",
    "    rr_median_data = []\n",
    "    rr_80percentile_data = []\n",
    "    rr_20percentile_data = []\n",
    "    rr_quartile_deviation_data = []\n",
    "    rr_HF_data = []\n",
    "    rr_LF_data = []\n",
    "    rr_VLF_data = []\n",
    "    rr_LF_HF_data = []\n",
    "    rr_heart_rate_data = []\n",
    "\n",
    "    # iterate over each window and calculate features\n",
    "\n",
    "\n",
    "    reference_data = value\n",
    "\n",
    "    rr_variance_data.append(np.var(reference_data))\n",
    "\n",
    "    power, frequency = lomb(time_stamps=timestamp,samples=value,low_frequency=low_frequency, high_frequency=high_frequency)\n",
    "\n",
    "    rr_VLF_data.append(heart_rate_power(power, frequency, low_rate_vlf, high_rate_vlf))\n",
    "\n",
    "    rr_HF_data.append(heart_rate_power(power, frequency, low_rate_hf, high_rate_hf))\n",
    "\n",
    "    rr_LF_data.append(heart_rate_power(power,frequency,low_rate_lf,high_rate_lf))\n",
    "\n",
    "    if heart_rate_power(power, frequency, low_rate_hf, high_rate_hf) != 0:\n",
    "        lf_hf = float(heart_rate_power(power, frequency, low_rate_lf, high_rate_lf) / heart_rate_power(power,\n",
    "                                                                                                       frequency,\n",
    "                                                                                                       low_rate_hf,\n",
    "                                                                                                       high_rate_hf))\n",
    "        rr_LF_HF_data.append(lf_hf)\n",
    "    else:\n",
    "        rr_LF_HF_data.append(0)\n",
    "\n",
    "    rr_mean_data.append(np.mean(reference_data))\n",
    "    rr_median_data.append(np.median(reference_data))\n",
    "    rr_quartile_deviation_data.append((0.5*(np.percentile(reference_data, 75) - np.percentile(reference_data,25))))\n",
    "    rr_heart_rate_data.append(60/np.median(reference_data))\n",
    "\n",
    "    return [rr_variance_data[0],rr_quartile_deviation_data[0],\\\n",
    "           rr_VLF_data[0], rr_LF_data[0],rr_HF_data[0], rr_LF_HF_data[0],\\\n",
    "           rr_mean_data[0], rr_median_data[0],\\\n",
    "           np.percentile(value,80),np.percentile(value,20),\\\n",
    "           rr_heart_rate_data[0]]\n",
    "\n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "def get_ecg_windowss(rr_interval):\n",
    "    window_col,ts_col = [],[]\n",
    "    ts_array = np.arange(rr_interval[0,0],rr_interval[-1,0],30000)\n",
    "    for t in ts_array:\n",
    "        index = np.where((rr_interval[:,0]>=t)&(rr_interval[:,0]<=t+60000))[0]\n",
    "        if len(index)<40:\n",
    "            continue\n",
    "        rr_temp = rr_interval[index,:]\n",
    "#         if len(np.where((rr_temp[:,1]<-3)|(rr_temp[:,1]>3))[0])/len(index) > .1:\n",
    "#             continue\n",
    "        window_col.append(rr_temp)\n",
    "        ts_col.append(t)\n",
    "    return window_col,ts_col\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def combine_data_sobc(window_col,ts_col,label_data,participant):\n",
    "    feature_matrix = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    for i,item in enumerate(window_col):\n",
    "#         if get_label(label_data,ts_col[i],ts_col[i]+50000) not in [1,0]:\n",
    "#             continue\n",
    "#         feature = ecg_feature_computation(item[:,0]/1000,item[:,1]/1000)\n",
    "        feature = get_rr_features(item[:,1])\n",
    "#         f = get_time_domain_features(item[:,1])\n",
    "#         feature.extend(list(f.values()))\n",
    "#         f = get_csi_cvi_features(item[:,1])\n",
    "#         feature.extend(list(f.values()))\n",
    "#         f = get_poincare_plot_features(item[:,1])\n",
    "#         feature.extend(list(f.values()))\n",
    "        feature_matrix.append(np.array(feature).reshape(-1,7))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(get_label(label_data,ts_col[i],ts_col[i]+50000))\n",
    "    return np.array(feature_matrix).reshape(-1,11),user_col,label_col\n",
    "def get_2_sec_ts(rr_ppg_int):\n",
    "    m = np.mean(rr_ppg_int[:,1])\n",
    "    s = np.std(rr_ppg_int[:,1])\n",
    "#     rr_ppg_int = rr_ppg_int[np.where((rr_ppg_int[:,1]>=m-3*s)&(rr_ppg_int[:,1]<=m+3*s))[0],:]\n",
    "    ts_array = np.arange(rr_ppg_int[0,0],rr_ppg_int[-1,0],1000)\n",
    "    rr_interval = np.zeros((0,2))\n",
    "    for t in ts_array:\n",
    "        index = np.where((rr_ppg_int[:,0]>=t-2000)&(rr_ppg_int[:,0]<=t+2000))[0]\n",
    "        if len(index) < 1:\n",
    "            continue\n",
    "        rr_interval = np.concatenate((rr_interval,np.array([t,np.mean(rr_ppg_int[index,1])]).reshape(-1,2)))\n",
    "    return rr_interval\n",
    "from scipy.stats.mstats_basic import winsorize\n",
    "\n",
    "path = '/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance//'\n",
    "participants = os.listdir(path)\n",
    "from ecgdetectors import Detectors\n",
    "detectors = Detectors(100)\n",
    "X,y,groups = np.zeros((0,7)),[],[]\n",
    "for participant in participants:\n",
    "    file_list = os.listdir(path+'/'+participant)\n",
    "    for file in file_list:\n",
    "        final_path = path+'/'+participant+'/'+file+'/'\n",
    "        try:\n",
    "            if 'ecg_rr_pan_tomkins1.csv' not in os.listdir(final_path):\n",
    "                ecg_final_data = pd.read_csv(final_path+'ecg_final_data.csv',header=None,sep=',').values\n",
    "                rpeaks = detectors.pan_tompkins_detector(ecg_final_data[:,2])\n",
    "                rpeaks = np.array(rpeaks)\n",
    "                rpeak_ts = ecg_final_data[rpeaks,0]\n",
    "                ecg_rr_ts = rpeak_ts[1:]\n",
    "                ecg_rr_val = np.diff(rpeak_ts)\n",
    "                index = np.where((ecg_rr_val>=300)&(ecg_rr_val<=2000))[0]\n",
    "                ecg_rr_ts = ecg_rr_ts[index]\n",
    "                ecg_rr_val = ecg_rr_val[index]\n",
    "#                 rr = remove_ectopic_beats(list(ecg_rr_val),'acar')\n",
    "#                 ecg_rr_val = ecg_rr_val[~np.isnan(rr)]\n",
    "#                 ecg_rr_ts = ecg_rr_ts[~np.isnan(rr)]\n",
    "                outlier = compute_outlier_ecg(ecg_rr_ts/1000,ecg_rr_val/1000)\n",
    "                index = []\n",
    "                for ind,tup in enumerate(outlier):\n",
    "                    if tup[1]==Quality.ACCEPTABLE:\n",
    "                        index.append(ind)\n",
    "                index = np.array(index)\n",
    "                ecg_rr_ts = ecg_rr_ts[index]\n",
    "                ecg_rr_val = ecg_rr_val[index]\n",
    "                ecg_rr = np.zeros((len(ecg_rr_ts),2))\n",
    "                ecg_rr[:,0] = ecg_rr_ts\n",
    "                ecg_rr[:,1] = ecg_rr_val\n",
    "                df = pd.DataFrame(ecg_rr, columns= ['time', 'rr'])\n",
    "                df.to_csv(final_path+'ecg_rr_pan_tomkins1.csv',index=False,header=False)\n",
    "            else:\n",
    "                ecg_rr = pd.read_csv(final_path+'ecg_rr_pan_tomkins1.csv',header=None,sep=',').values\n",
    "            if ecg_rr.shape[0]<500:\n",
    "                continue\n",
    "            winsor_limit = .05\n",
    "            ecg_rr[:,1] = winsorize(ecg_rr[:,1],limits=[winsor_limit,winsor_limit])\n",
    "#             plt.plot(ecg_rr[:,1])\n",
    "#             plt.show()\n",
    "            label_data = pd.read_csv(final_path+'label_data.csv',header=None,sep=',').values\n",
    "#             print(label_data.shape,participant)\n",
    "#             mean_col = []\n",
    "#             std_col = []\n",
    "#             i = 0\n",
    "#             while i < len(ecg_rr):\n",
    "#                 start_ts = ecg_rr[i,0]\n",
    "#                 j = i\n",
    "#                 while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "#                     j+=1\n",
    "#                 mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "#                 std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "#                 i=j\n",
    "#             m = np.percentile(mean_col,70)\n",
    "#             s = np.percentile(std_col,30)\n",
    "\n",
    "#             ecg_rr = get_2_sec_ts(ecg_rr)\n",
    "#             index = np.where(ecg_rr[:,0]<ecg_rr[0,0]+10*60*1000)[0]\n",
    "           \n",
    "            m = np.mean(ecg_rr[:,1])\n",
    "            s = np.std(ecg_rr[:,1])\n",
    "            if s==0:\n",
    "                continue\n",
    "#             ecg_rr[:,1] = (ecg_rr[:,1]-m)/s \n",
    "            \n",
    "#             ecg_rr[np.where((ecg_rr[:,1]>=3))[0],1] = 3\n",
    "#             ecg_rr[np.where((ecg_rr[:,1]<=-3))[0],1] = -3\n",
    "#             ecg_rr[:,1] = RobustScaler(quantile_range=(20,80)).fit_transform(ecg_rr[:,1:]).reshape(len(ecg_rr),)\n",
    "            window_col,ts_col = get_ecg_windowss(ecg_rr)\n",
    "            feature_matrix,user_col,label_col = combine_data_sobc(window_col,ts_col,label_data,participant)\n",
    "            temp = np.array(label_col)\n",
    "#             if len(temp[temp==1])/len(temp[temp==0]) < .5:\n",
    "#                 continue\n",
    "#             if len(temp[temp==1])<5 or len(temp[temp==1])<10:\n",
    "#                 continue\n",
    "            labels = np.array(label_col)\n",
    "#             if len(labels[labels==1])<5:\n",
    "#                 continue\n",
    "            X = np.concatenate((X,feature_matrix))\n",
    "            y.extend(label_col)\n",
    "            groups.extend(user_col)\n",
    "            print(X.shape,len(np.unique(groups)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            e\n",
    "\n",
    "# winsor_limit = .02\n",
    "# for k in range(X.shape[1]):\n",
    "#     X[:,k] = winsorize(X[:,k],limits=[winsor_limit,winsor_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5313, 11) 31\n",
      "(3162, 11) 31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "print(X.shape,len(np.unique(groups)))\n",
    "for user in np.unique(groups):\n",
    "    index = np.where(groups==user)[0]\n",
    "    X[index,:] = StandardScaler().fit_transform(X[index,:])\n",
    "X[X>4] = 4\n",
    "X[X<-4] = -4\n",
    "index = np.where(y<2)[0]\n",
    "X,y,groups = X[index],y[index],groups[index]\n",
    "print(X.shape,len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 31 folds for each of 16 candidates, totalling 496 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 496 out of 496 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.852):\n",
      "{'rf__C': 11, 'rf__class_weight': {0: 0.3, 1: 0.7}, 'rf__gamma': 0.03125, 'rf__kernel': 'rbf', 'rf__probability': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {'rf__kernel': ['rbf'],\n",
    "             'rf__C': [11],\n",
    "             'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .5)],\n",
    "             'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3]],\n",
    "             'rf__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('rf', SVC())])\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='accuracy',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2044  286]\n",
      " [ 181  651]]               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.88      0.90      2330\n",
      "         1.0       0.69      0.78      0.74       832\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3162\n",
      "   macro avg       0.81      0.83      0.82      3162\n",
      "weighted avg       0.86      0.85      0.85      3162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups))\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('rf', SVC(C=11, cache_size=200, class_weight={0: 0.3, 1: 0.7}, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.03125, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_model_feature_standardization_sobc1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mats = np.array([[794 ,220],[170 ,497]])\n",
    "sns.set(font_scale=1.5)\n",
    "df_cm = pd.DataFrame(mats, index = [i for i in ['Not Stress','Stress']],\n",
    "                  columns = [i for i in ['Not Stress','Stress']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True,fmt='g',annot_kws={\"fontsize\":28})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "temp = np.array([1.8071820221852526,\n",
    "                 0.12102575000797881,\n",
    "                 1.1841492817462929,\n",
    "                 0.9287861011371492,\n",
    "                 1.9107735231269358,\n",
    "                 -0.9425789234303202,\n",
    "                 1.0006703240454398,\n",
    "                 1.3585399583852928,\n",
    "                 1.3651308639025757,\n",
    "                 1.0414474739221673,\n",
    "                 -1.339951252451994]).reshape(-1,11)\n",
    "print(clf.decision_function(temp),clf.probA_,clf.probB_)\n",
    "print(clf.predict(temp),clf.predict_proba(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X,columns=['var', \n",
    "                             'iqr', \n",
    "                             'vlf',  \n",
    "                             'lf',\n",
    "                             'hf',\n",
    "                             'lfhf',\n",
    "                             'mean',\n",
    "                             'median',\n",
    "                             '80th',\n",
    "                             '20th',\n",
    "                             'heartrate'])\n",
    "df['stress_probability'] = list(clf.predict_proba(X)[:,1])\n",
    "df['label'] = list(y)\n",
    "print(confusion_matrix(y,df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stress_all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = cross_val_predict(clf,X,y,cv=list(gkf.split(X,y,groups=groups)),method='decision_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.2\n",
    "# paramGrid = ParameterGrid({'C': np.logspace(-3,3,20),\n",
    "#               'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)]})\n",
    "paramGrid = {\n",
    "             'C': np.logspace(.1,2,10),\n",
    "             'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "}\n",
    "clf = LogisticRegression()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1',verbose=5)\n",
    "grid_search.fit(decisions.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = CalibratedClassifierCV(clf,cv =gkf.split(X,y,groups=groups), method='isotonic')\n",
    "# clf.fit(X,y)\n",
    "# # list(zip(clf.steps[0][1].mean_,clf.steps[0][1].scale_))\n",
    "# plt.figure(figsize=(16,8))\n",
    "\n",
    "# plt.plot(y)\n",
    "# plt.plot(clf.predict_proba(X)[:,1])\n",
    "# plt.show()\n",
    "# print(f1_score(y,clf.predict(X)))\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "# clf.set_params(rf__probability=True)\n",
    "clf.fit(X,y)\n",
    "# clf.score(X)\n",
    "# print(accuracy_score(y,clf.predict(X1)))\n",
    "# pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_model.p','wb'))\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "score, bias = f1Bias_scorer_CV(clf.predict_proba(X)[:,1], y, True)\n",
    "print(score,bias,confusion_matrix(y,clf.predict(X)))\n",
    "plt.plot(y[:])\n",
    "plt.plot(clf.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(X,columns=['var','iqr','vlf','lf','hf','lfhf','mean','median','80th','20th','heartrate'])\n",
    "df['stress'] = y\n",
    "# df1 = pickle.load(open('./stress_code/feature.p','rb'))\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# sns.set(style=\"ticks\", color_codes=True)\n",
    "# g = sns.pairplot(df)\n",
    "df['median'].hist(bins=20)\n",
    "# plt.title('Original-Heart rate')\n",
    "# plt.savefig('./stress_code/2.png',dps=1e6)\n",
    "\n",
    "iris_X = df[df.columns.difference([\"stress\"])]\n",
    "iris_y = df[\"stress\"]\n",
    "iris_X.to_csv('stress_feature_data.csv')\n",
    "iris_y.to_csv('stress_label_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn2pmml.decoration import ContinuousDomain\n",
    "\n",
    "column_preprocessor = DataFrameMapper([\n",
    "    (['var','iqr','vlf','lf','hf','lfhf','mean','median','80th','20th','heartrate'], [ContinuousDomain(), StandardScaler()])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "# classifier = LogisticRegression(C=0.018329807108324356, class_weight={0: 0.5, 1: 0.5},\n",
    "#           dual=False, fit_intercept=True, intercept_scaling=1,\n",
    "#           max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
    "#           random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
    "#           warm_start=False)\n",
    "classifier = LogisticRegression(C=0.001,\n",
    "          class_weight={0: 0.6000000000000001, 1: 0.3999999999999999},\n",
    "          dual=False, fit_intercept=True, intercept_scaling=1,\n",
    "          max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
    "          random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
    "          warm_start=False)\n",
    "pipeline = PMMLPipeline([\n",
    "    (\"columns\", column_preprocessor),\n",
    "    (\"classifier\", classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y,pipeline.predict(iris_X)),confusion_matrix(y,pipeline.predict(iris_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.verify(iris_X.sample(n = 15)),pipeline.predict_proba(iris_X)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"pipeline.pkl.z\", compress = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y,clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([clf,X,y],open('clf_and_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = preprocessing.StandardScaler().fit_transform(X)\n",
    "import pickle\n",
    "clf.fit(X1,y)\n",
    "print(accuracy_score(y,clf.predict(X1)))\n",
    "pickle.dump(clf,open('ecg_model.p','wb'))\n",
    "pickle.dump(X1,open('X.p','wb'))\n",
    "pickle.dump(y,open('y.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': np.logspace(0,2,2),\n",
    "              'gamma': np.logspace(-9,9,10),\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X1,y,groups=groups)),\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=1, iid=False)\n",
    "grid_search.fit(X1,y)\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X1, y, gkf.split(X1,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "def plot_confusion_matrix(cm, classes=['Not Stress','Stress'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('result.png')\n",
    "    plt.show()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "predicted = cross_val_predict(clf, X, y, cv=gkf.split(X,y,groups=groups),n_jobs=24)\n",
    "plot_confusion_matrix(confusion_matrix(y,predicted))\n",
    "print(f1_score(y,predicted),precision_score(y,predicted),recall_score(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "svc = predictor\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=gkf.split(X,y,groups=groups),\n",
    "              scoring='f1',n_jobs=24)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
