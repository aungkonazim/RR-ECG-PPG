{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.3s remaining:   21.7s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.2s remaining:    6.3s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.8s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   5 | elapsed:    2.7s remaining:    4.0s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of   5 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    3.7s remaining:    8.2s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    4.1s remaining:    2.6s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   7 | elapsed:    1.7s remaining:    4.2s\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   7 | elapsed:    1.9s remaining:    1.4s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of   7 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    2.0s remaining:    4.6s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    3.2s remaining:    2.0s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  16 | elapsed:    3.1s remaining:   21.6s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  16 | elapsed:    3.9s remaining:    5.0s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  16 | elapsed:    4.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=25)]: Done  16 out of  16 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of   6 | elapsed:    3.1s remaining:    3.1s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of   6 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of   6 | elapsed:    2.9s remaining:    2.9s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of   6 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  10 | elapsed:    3.0s remaining:    7.1s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  10 | elapsed:    3.2s remaining:    2.1s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    2.0s remaining:    4.5s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    2.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   7 | elapsed:    1.5s remaining:    3.9s\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   7 | elapsed:    2.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of   7 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  10 | elapsed:    1.8s remaining:    4.2s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  10 | elapsed:    2.1s remaining:    1.4s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  10 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    3.2s remaining:    5.6s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    4.2s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    1.8s remaining:   11.7s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    3.4s remaining:    5.1s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.0s remaining:    2.0s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.5s remaining:   23.0s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.4s remaining:    6.6s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.7s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   8 | elapsed:    1.9s remaining:    1.9s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of   8 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of   6 | elapsed:    2.5s remaining:    2.5s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of   6 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    2.3s remaining:    1.7s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    3.7s remaining:    6.4s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    4.0s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    2.6s remaining:    5.9s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    3.5s remaining:    2.2s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    3.7s remaining:    6.7s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    4.4s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   9 | elapsed:    3.3s remaining:   11.5s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of   9 | elapsed:    4.2s remaining:    3.4s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of   9 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    2.3s remaining:   15.1s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    2.7s remaining:    4.1s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    3.2s remaining:    1.6s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of   6 | elapsed:    2.4s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of   6 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    2.3s remaining:    4.0s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    2.6s remaining:    1.5s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    3.0s remaining:    6.7s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    3.6s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   9 | elapsed:    1.8s remaining:    6.4s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of   9 | elapsed:    2.8s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of   9 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    2.8s remaining:   18.4s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    3.9s remaining:    5.9s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.6s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    1.8s remaining:    3.2s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    2.3s remaining:    1.3s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    3.3s remaining:    6.0s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    4.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    2.6s remaining:   17.2s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    5.0s remaining:    2.5s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    3.0s remaining:    8.9s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.7s remaining:    2.6s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    2.6s remaining:    4.6s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    3.0s remaining:    1.7s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    3.6s remaining:    8.2s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    4.4s remaining:    2.7s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.6s remaining:    7.7s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.8s remaining:    2.7s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    3.4s remaining:    6.2s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    4.3s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   8 | elapsed:    3.3s remaining:    3.3s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of   8 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   9 | elapsed:    3.7s remaining:   13.1s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of   9 | elapsed:    4.1s remaining:    3.3s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of   9 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    1.9s remaining:   12.4s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    2.8s remaining:    4.2s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    3.6s remaining:    1.8s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.4s remaining:    7.3s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.5s remaining:    2.5s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.6s remaining:    7.7s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.1s remaining:   20.2s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.7s remaining:    7.0s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    5.2s remaining:    2.6s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.6s remaining:    7.8s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.2s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    3.0s remaining:    6.8s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    3.7s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  13 | elapsed:    2.8s remaining:    6.3s\n",
      "[Parallel(n_jobs=25)]: Done   8 out of  13 | elapsed:    3.3s remaining:    2.0s\n",
      "[Parallel(n_jobs=25)]: Done  13 out of  13 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    3.8s remaining:    6.7s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    4.1s remaining:    2.4s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.2s remaining:   20.9s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    3.8s remaining:    5.6s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    2.9s remaining:   18.9s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    5.1s remaining:    2.5s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    2.8s remaining:   18.1s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    3.9s remaining:    5.9s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.4s remaining:    2.2s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    4.4s remaining:    8.0s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    4.8s remaining:    2.7s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of  11 | elapsed:    3.8s remaining:    6.7s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  11 | elapsed:    4.4s remaining:    2.5s\n",
      "[Parallel(n_jobs=25)]: Done  11 out of  11 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.6s remaining:   23.2s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.6s remaining:    6.8s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    5.2s remaining:    2.6s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  16 | elapsed:    2.5s remaining:   17.2s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  16 | elapsed:    4.0s remaining:    5.1s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  16 | elapsed:    4.6s remaining:    1.5s\n",
      "[Parallel(n_jobs=25)]: Done  16 out of  16 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:    3.7s remaining:   23.9s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:    4.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:    4.7s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   4 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   7 | elapsed:    1.5s remaining:    3.8s\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   7 | elapsed:    2.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of   7 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of   6 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=25)]: Done   6 out of   6 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:    2.2s remaining:    6.5s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:    3.3s remaining:    2.3s\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  12 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   7 | elapsed:    2.2s remaining:    5.4s\n",
      "[Parallel(n_jobs=25)]: Done   4 out of   7 | elapsed:    2.9s remaining:    2.2s\n",
      "[Parallel(n_jobs=25)]: Done   7 out of   7 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of   5 | elapsed:    1.8s remaining:    2.8s\n",
      "[Parallel(n_jobs=25)]: Done   5 out of   5 | elapsed:    3.7s finished\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "# def get_all_features(a):\n",
    "#     try:\n",
    "#         orig = a.shape[0]\n",
    "#         feature_availability = [len(a)]\n",
    "#         feature_qual = [np.median(a[:,1]),np.percentile(a[:,1],80),np.percentile(a[:,1],20),iqr(a[:,1]),np.std(a[:,1]),skew(a[:,1]),kurtosis(a[:,1])]\n",
    "#         feature_activity = [np.median(a[:,2]),np.percentile(a[:,2],80),np.percentile(a[:,2],20),iqr(a[:,2]),np.std(a[:,2]),skew(a[:,2]),kurtosis(a[:,2])]\n",
    "#         a = a[np.where((a[:,0]>300)&(a[:,0]<1500)&(a[:,1]>.15))[0],:]\n",
    "#         if len(a)<3:\n",
    "#             return np.array([0]*30)\n",
    "#         feature_qual+=[np.median(a[:,1]),np.percentile(a[:,1],80),np.percentile(a[:,1],20),iqr(a[:,1]),np.std(a[:,1]),skew(a[:,1]),kurtosis(a[:,1])]\n",
    "#         feature_activity+=[np.median(a[:,2]),np.percentile(a[:,2],80),np.percentile(a[:,2],20),iqr(a[:,2]),np.std(a[:,2]),skew(a[:,2]),kurtosis(a[:,2])]\n",
    "#         feature_availability += [len(a)]\n",
    "# #         print(np.array(feature_activity+feature_availability+feature_qual))\n",
    "#         return np.array(feature_activity+feature_availability+feature_qual)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return np.array([0]*30)\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>120]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>20]\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+3*tt)&(heart_rate_window[:,0]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "    \n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,1]>.05)&(a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>40]\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    \n",
    "    data_day['mean'] = data_day['rr'].apply(lambda a:np.mean(a))\n",
    "    data_day['std'] = data_day['rr'].apply(lambda a:np.std(a))\n",
    "    m = np.percentile(data_day['mean'],70)\n",
    "    s = np.percentile(data_day['std'],30)\n",
    "    data_day['rr'] = data_day['rr'].apply(lambda a:(np.array(a)-m)/s)\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    \n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_weighted_rr_features(a))\n",
    "    stress_model = pickle.load(open('../models/stress_model_weighted_2.p','rb'))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    \n",
    "    feature_matrix = np.array(list(data_day['rr_weighted_features'].values))\n",
    "    quals1 = np.array(list(data_day['quality_mag'].values))\n",
    "    feature_matrix = normalize_daywise(feature_matrix,quals1)\n",
    "    stress_likelihood = stress_model.predict_proba(feature_matrix)[:,1]\n",
    "    data_day['stress_likelihood_ppg_qual'] = stress_likelihood\n",
    "    \n",
    "    feature_matrix = np.array(list(data_day['rr_weighted_features'].values))\n",
    "    feature_matrix = normalize_daywise(feature_matrix,[1]*len(quals1))\n",
    "    stress_likelihood = stress_model.predict_proba(feature_matrix)[:,1]\n",
    "    data_day['stress_likelihood_ppg'] = stress_likelihood\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "# def get_stress(data_day):\n",
    "#     stress_model = pickle.load(open('../models/stress_model_weighted.p','rb'))\n",
    "#     feature_matrix = np.array(list(data_day['rr_weighted_features'].values))\n",
    "#     quals1 = np.array(list(data_day['quality_mag'].values))\n",
    "#     feature_matrix = normalize_daywise(feature_matrix,quals1)\n",
    "# #     print(feature_matrix.shape)\n",
    "#     stress_likelihood = stress_model.predict_proba(feature_matrix)[:,1]\n",
    "#     data_day['stress_likelihood1'] = stress_likelihood\n",
    "    \n",
    "#     stress_model = pickle.load(open('../models/stress_model_weighted.p','rb'))\n",
    "#     feature_matrix = np.array(list(data_day['rr_weighted_features'].values))\n",
    "#     feature_matrix = normalize_daywise(feature_matrix,[1]*len(quals1))\n",
    "# #     print(feature_matrix.shape)\n",
    "#     stress_likelihood = stress_model.predict_proba(feature_matrix)[:,1]\n",
    "#     data_day['stress_likelihood2'] = stress_likelihood\n",
    "#     return data_day\n",
    "\n",
    "# def get_corr(data_day1):\n",
    "#     if data_day1.shape[0]<60:\n",
    "#         return np.zeros((0,7))\n",
    "#     data_day1['quality_mag_1'] = data_day1['quality_mag'].apply(lambda a:np.round(100*a)/100)\n",
    "#     all_corr = []\n",
    "#     for q in np.unique(data_day1['quality_mag_1'].values):\n",
    "#         tmp = data_day1[data_day1.quality_mag_1>=q]\n",
    "#         tmp2 = tmp[['stress_likelihood_ecg','stress_likelihood','stress_likelihood1','stress_likelihood2']].dropna()\n",
    "#         if tmp2.shape[0]<20:\n",
    "#             continue\n",
    "#         feature = np.array([q,\n",
    "#                    r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood'].values),\n",
    "#                    r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood1'].values),\n",
    "#                    r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood2'].values),\n",
    "#                    r2_score(tmp2['stress_likelihood'].values,tmp2['stress_likelihood1'].values),\n",
    "#                    tmp['stress_likelihood1'].dropna().shape[0],\n",
    "#                    data_day1['stress_likelihood_ecg'].dropna().shape[0]])\n",
    "#         all_corr.append(feature)\n",
    "#     return np.array(all_corr)\n",
    "\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','index','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>20]\n",
    "    data_day['mean'] = data_day['ecg_rr_array'].apply(lambda a:np.mean(a))\n",
    "    data_day['std'] = data_day['ecg_rr_array'].apply(lambda a:np.std(a))\n",
    "    m = np.percentile(data_day['mean'],70)\n",
    "    s = np.percentile(data_day['std'],30)\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array'].apply(lambda a:(np.array(a)-m)/s)\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:get_rr_features(a))\n",
    "    X = np.array(list(data_day['ecg_features']))\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    clf = pickle.load(open('../models/stress_model_ecg_2.p','rb'))\n",
    "    y_pred = clf.predict_proba(X)[:,1]\n",
    "    data_day['stress_likelihood_ecg'] = list(y_pred)\n",
    "    return data_day\n",
    "\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features',\n",
    "       'index', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'mean', 'std', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag', 'stress_likelihood_ppg_qual','stress_likelihood_ecg',\n",
    "       'stress_likelihood_ppg']\n",
    "    try:\n",
    "        ecg_columns = ['window', 'stress_likelihood_ecg']\n",
    "        a['index'] = a.index.values\n",
    "        a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "        a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "        if a['ecg_rr_array'].dropna().shape[0]<120:\n",
    "            stress_likelihood_ecg = 0\n",
    "        else:\n",
    "            a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "            a_ecg = a_ecg[ecg_columns]\n",
    "        a_ppg = parse_day_data(a)\n",
    "        a_ppg = parse_for_features(a_ppg)\n",
    "        if a_ppg.shape[0]<60:\n",
    "            return pd.DataFrame([],columns=columns)\n",
    "        if a_ecg.shape[0]<60:\n",
    "            a_ppg['stress_likelihood_ecg'] = np.nan\n",
    "            return a_ppg[columns]\n",
    "        a_ppg = pd.merge(a_ppg, a_ecg, how='left', left_on=['window'], right_on=['window'])\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.plot(a_ppg['ltime'],a_ppg['stress_likelihood_ecg'],'*')\n",
    "#     plt.plot(a_ppg['ltime'],a_ppg['stress_likelihood_ppg'],'o')\n",
    "#     plt.plot(a_ppg['ltime'],a_ppg['stress_likelihood_ppg_qual'],'s')\n",
    "#     plt.show()\n",
    "    \n",
    "        return a_ppg[columns]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "\n",
    "# def parse_each_day(a):\n",
    "#     columns = a.columns\n",
    "#     ecg_len = a['stress_likelihood_ecg'].dropna().shape[0]\n",
    "#     a = parse_day_data(a)\n",
    "#     a = parse_for_features(a)\n",
    "#     if a.shape[0]<200:\n",
    "#         return np.zeros((0,7)),np.zeros((0,4)),np.zeros((0,3)),a\n",
    "#     try:\n",
    "#         a = get_stress(a)\n",
    "#     except:\n",
    "#         return np.zeros((0,7)),np.zeros((0,4)),np.zeros((0,3)),a\n",
    "#     a['quality_mag_1'] = a['quality_mag'].apply(lambda a:np.round(100*a)/100)\n",
    "#     ppg_len = a['stress_likelihood1'].dropna().shape[0]\n",
    "#     a1 = a[['quality_mag_1','stress_likelihood1']].dropna()\n",
    "#     ff = []\n",
    "#     for q in np.unique(a1['quality_mag_1'].values):\n",
    "#         ff.append(np.array([q,a1[a1.quality_mag_1==q].shape[0],ecg_len,ppg_len]))\n",
    "#     all_corr = get_corr(a)\n",
    "#     if all_corr.shape[0]==0:\n",
    "#         return np.zeros((0,7)),np.array(ff).reshape(-1,4),np.zeros((0,3)),a\n",
    "#     tmp2 = a[['stress_likelihood_ecg','stress_likelihood','stress_likelihood1','stress_likelihood2']].dropna()\n",
    "#     try:\n",
    "#         tmp = np.array([r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood'].values),\n",
    "#                        r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood1'].values),\n",
    "#                        r2_score(tmp2['stress_likelihood_ecg'].values,tmp2['stress_likelihood2'].values)])\n",
    "#     except:\n",
    "#         return np.zeros((0,7)),np.array(ff).reshape(-1,4),np.zeros((0,3)),a\n",
    "#     if len(tmp[~np.isnan(tmp)])<3:\n",
    "#         return np.zeros((0,7)),np.array(ff).reshape(-1,4),np.zeros((0,3)),a\n",
    "#     else:\n",
    "#         return all_corr,np.array(ff).reshape(-1,4),tmp.reshape(-1,3),a\n",
    "\n",
    "def parse_each_participant(directory,d):\n",
    "    data = pickle.load(open(directory+d,'rb')).reset_index(drop=True)\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return 0\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return 0\n",
    "    final_output = pd.concat(final_output)\n",
    "    pickle.dump([final_output,ema],open(directory1+d,'wb'))\n",
    "    return 0\n",
    "directory = '../../cc3/rice_data/ecg_ppg_25_left3/'\n",
    "directory1 = '../../cc3/rice_data/ecg_ppg_25_left4/'\n",
    "# all_data = Parallel(n_jobs=30,verbose=2)(delayed(parse_each_participant)(directory,d) for d in os.listdir(directory)[:2] if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory,d) for d in os.listdir(directory) if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open('../../cc3/rice_data/ecg_ppg_25_left4/35109a64-411d-4768-9602-c0a3d519a088.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.concatenate([a[0] for a in all_data])\n",
    "yld = np.concatenate([a[1] for a in all_data])\n",
    "yld1 = yld[:,:2]\n",
    "yld = yld[:,2:]\n",
    "day_corr = np.concatenate([a[2] for a in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(yld)\n",
    "plt.ylabel('Minutes')\n",
    "plt.xticks(range(1,yld.shape[1]+1),['ECG YIELD','PPG YIELD'])\n",
    "plt.title('Stress yield across all participant days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(day_corr.shape)\n",
    "day_corr = day_corr[~np.isnan(day_corr).any(axis=1)]\n",
    "print(np.sum([a[4] for a in all_data]),'- Participant Days,',np.sum([a[3] for a in all_data]),'- Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylim([-1,1])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "#  plt.figure(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['corr_new'], by='quality', ax=ax,showfliers=True)\n",
    "plt.ylim([-3,1])\n",
    "plt.xticks(rotation=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['ppg_yield'], by='quality', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.groupby('quality').quantile([.25,.75]).loc[(0.2, 0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all1['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.load(open('../models/stress_model_ecg_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
