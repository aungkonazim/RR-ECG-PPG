{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "directory_left = '../../cc3/rice_data/ecg_ppg_5_left_final_v1/'\n",
    "directory_right = '../../cc3/rice_data/ecg_ppg_5_right_final_v1/'\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['rr_col']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:np.array([np.squeeze(b).reshape(-1) for b in a]).reshape(-1,4))\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[a[:,0].argsort()])\n",
    "    data_day['length'] = data_day['rr_col'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>=10]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[:,1:])\n",
    "    return data_day[['user','day','rr_col','ecg_rr_array']]\n",
    "\n",
    "\n",
    "def get_all_data(data,hand='left'):\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_day_data)(a) for a in data_all)\n",
    "#     final_output = [parse_day_data(a) for a in data_all]\n",
    "    final_output = pd.concat([a for a in final_output if a.shape[0]>0])\n",
    "    final_output['hand'] = hand\n",
    "    print(final_output.shape)\n",
    "    return final_output\n",
    "\n",
    "def parse_each_participant(directory_left,directory_right,d):\n",
    "    try:\n",
    "        left_data = get_all_data(pickle.load(open(directory_left+d,'rb')).reset_index(drop=True),'left')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        left_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_left = pd.DataFrame([],columns=['a','b'])\n",
    "    try:\n",
    "        right_data = get_all_data(pickle.load(open(directory_right+d,'rb')).reset_index(drop=True),'right')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        right_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_right = pd.DataFrame([],columns=['a','b'])\n",
    "    t = [a for a in [left_data,right_data] if a.shape[0]>0]\n",
    "    if len(t)==0:\n",
    "        return pd.DataFrame([])\n",
    "    data = pd.concat(t)\n",
    "    print(data.shape)\n",
    "    if data.shape[0]>0:\n",
    "        return data\n",
    "    return pd.DataFrame([])\n",
    "all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right)) if d[-1]=='p')\n",
    "# all_data = [parse_each_participant(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right))[:2] if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [a for a in all_data if a.shape[0]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user_day_hand'] = data.apply(lambda a:a['user']+a['day']+a['hand'],axis=1)\n",
    "data['user_day'] = data.apply(lambda a:a['user']+a['day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['sz'] = data['rr_col'].apply(lambda a:len(a))\n",
    "# data = data[data.sz>=10]\n",
    "data['likelihood_max1'] = data['rr_col'].apply(lambda a:np.percentile(a[:,1],90))\n",
    "# data['likelihood_max'] = data['rr_col'].apply(lambda a: np.percentile(a,100*(1-30/len(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data,open('../../cc3/rice_data/temp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('../../cc3/rice_data/temp.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_data(df):\n",
    "#     likelihoods = df.likelihood_max1.values\n",
    "#     if len(np.where(likelihoods>.3)[0])<30:\n",
    "#         return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "final_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_data1(df):\n",
    "    if df.day.unique().shape[0]<10:\n",
    "        return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "final_data = final_data.reset_index(drop=True)\n",
    "final_final_data = final_data.groupby(['user'],as_index=False).apply(get_data1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_data.user.unique().shape,final_final_data.user_day.unique().shape,final_final_data.user_day_hand.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "results = []\n",
    "for minimum  in [30]:\n",
    "#     x = np.arange(.1,.3,.02)\n",
    "    x = np.arange(0,.5,.05)\n",
    "    y = []\n",
    "    means1 = []\n",
    "    stds1 = []\n",
    "    means2 = []\n",
    "    stds2 = []\n",
    "    shapes = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,1]>=threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "        tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "        tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "        tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "        tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "        tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "        tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "        tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "        tmp1['diff_weighted'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80_weighted'])\n",
    "        means1.append(tmp1['diff_original'].mean())\n",
    "        means2.append(tmp1['diff_weighted'].mean())\n",
    "        stds1.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80']))\n",
    "        stds2.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80_weighted']))\n",
    "        shapes.append(tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0])\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1],means1[-1],means2[-1],stds1[-1],stds2[-1],shapes[-1])\n",
    "    results.append(np.vstack([x,y,means1,stds1,means2,stds2,shapes]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "results = []\n",
    "for minimum  in [30]:\n",
    "    x = np.arange(.002,.2,.005)\n",
    "#     x = np.arange(0,.5,.05)\n",
    "    y = []\n",
    "    means1 = []\n",
    "    stds1 = []\n",
    "    means2 = []\n",
    "    stds2 = []\n",
    "    shapes = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,2]<threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "        tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "        tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "        tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "        tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "        tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "        tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "        tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "        tmp1['diff_weighted'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80_weighted'])\n",
    "        means1.append(tmp1['diff_original'].mean())\n",
    "        means2.append(tmp1['diff_weighted'].mean())\n",
    "        stds1.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80']))\n",
    "        stds2.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80_weighted']))\n",
    "        shapes.append(tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0])\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]//tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1],means1[-1],means2[-1],stds1[-1],stds2[-1],shapes[-1])\n",
    "    results.append(np.vstack([x,y,means1,stds1,means2,stds2,shapes]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel,delayed\n",
    "minimum=30\n",
    "final_final_data['len'] = final_final_data['rr_col'].apply(lambda a:a.shape[0])\n",
    "final_final_data = final_final_data[final_final_data.len>=30]\n",
    "temp = deepcopy(final_final_data)\n",
    "temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,1],100*(1-minimum/a.shape[0])))\n",
    "temp['activity_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,2],100*(minimum/a.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['ecg_80'] = temp['ecg_rr_array'].apply(lambda a:np.percentile(a,80) if isinstance(a,list) and len(a)>=30 else np.nan)\n",
    "temp['ecg_len'] =  temp['ecg_rr_array'].apply(lambda a:len(a) if isinstance(a,list) and len(a)>=30 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,0]>0])\n",
    "temp['ecg_80'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in [3]:\n",
    "        try:\n",
    "            tmp = deepcopy(temp)\n",
    "            tmp = tmp[(tmp.likelihood_max>=threshold_qual) & (tmp.activity_max<=threshold_acl)]\n",
    "            tmp['rr_col'] = tmp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<threshold_acl))[0]])\n",
    "            tmp1 = tmp[['ecg_80','ecg_len','rr_col','user_day_hand']].dropna()\n",
    "            tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "            tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "            tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "            yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "            print(threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean())\n",
    "            all_results.append([threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean(),tmp.shape[0]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            all_results.append([threshold_qual,threshold_acl,-1,-1,0])\n",
    "#     pickle.dump(all_results,open('tmp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel,delayed\n",
    "def get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30):\n",
    "    temp = deepcopy(final_final_data)\n",
    "    temp['rr_col'] = temp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<threshold_acl))[0]])\n",
    "    temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "    tmp = temp[temp.len>=minimum]\n",
    "    tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "    tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "    tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "    tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "    tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "#     tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "#     tmp1['diff_weighted'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80_weighted'])\n",
    "    return [threshold_qual,threshold_acl,tmp.shape[0]/tmp.user_day_hand.unique().shape[0],mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80'].values),\n",
    "            mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80_weighted'].values),tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0],tmp.shape[0]]\n",
    "# all_data = Parallel(n_jobs=10,verbose=2)(delayed(get_yield_error)(final_final_data,threshold_acl,threshold_qual) for threshold_acl in np.arange(.002,.2,.005) for threshold_qual in np.arange(0,.5,.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_only_qual = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in np.logspace(-2.9,-.2,20):\n",
    "        try:\n",
    "            d = get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30)\n",
    "        except:\n",
    "            continue\n",
    "        all_results_only_qual.append(d)\n",
    "        data = pd.DataFrame(all_results_only_qual,columns=['Minimum Quality','Maximum Activity','Minutes per day','Original Error','Weighted Error','ECG Minutes per day','All Minutes'])\n",
    "        print(d)\n",
    "        pickle.dump(data,open('final_yield_accuracy_error.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "results = []\n",
    "for minimum  in [20,30,40]:\n",
    "    x = np.arange(.1,.3,.02)\n",
    "#     x = np.arange(0,.95,.05)\n",
    "    y = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,2]<threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1])\n",
    "    results.append(np.vstack([x,y]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results,open('results.p','wb'))\n",
    "np.arange(.002,.2,.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_20 = results[0]\n",
    "result_30 = results[1]\n",
    "result_40 = results[2]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(result_20[:,0],result_20[:,1],'-',label='p = 33.33%',linewidth=3)\n",
    "plt.plot(result_30[:,0],result_30[:,1],':',label='p = 50%',linewidth=3)\n",
    "plt.plot(result_40[:,0],result_40[:,1],'-.',label='p = 66.66%',linewidth=3)\n",
    "# plt.vlines(.05,0,585)\n",
    "# plt.vlines(.1,0,408)\n",
    "# plt.vlines(.15,0,306)\n",
    "# plt.vlines(.2,0,246)\n",
    "plt.xlabel('Minimum Signal Quality Threshold')\n",
    "plt.ylabel('Yield in Field \\n Minutes per participant-wrist day ')\n",
    "plt.xticks(np.arange(0,.95,.05),np.round(np.arange(0,.95,.05)*100)/100,rotation=60)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylim([0,700])\n",
    "ax = plt.gca()\n",
    "axy = ax.twinx()\n",
    "axy.plot(want[:,0],want[:,1],'--',c='r',marker='o',linewidth=2,markersize=10,label='LOSO F1 - Using Quality Weighted Features')\n",
    "axy.plot(not_want[:,0],not_want[:,1],'--',c='lime',marker='s',linewidth=2,markersize=10,label='LOSO F1 - Using Original Features')\n",
    "axy.set_ylabel('Lab Stress Classification Results \\n LOSO F1 Score',color='black')\n",
    "axy.tick_params(axis='y', colors='brown')\n",
    "axy.legend(loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "# want,not_1ant\n",
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))\n",
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_30 = result_30[result_30[:,0]<=.5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_want = np.hstack((want, result_30))\n",
    "all_data_not_want = np.hstack((not_want, result_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(all_data_want[:,-1],all_data_want[:,1],'--',marker='o',linewidth=3,markersize=10,label='Using Quality Weighted Features')\n",
    "plt.plot(all_data_not_want[:,-1],all_data_not_want[:,1],':',marker='s',linewidth=3,markersize=10,label='Using Original Features')\n",
    "plt.hlines(.675,0,263.35,color='black')\n",
    "plt.vlines(263.35,0,.675,linestyle='--',color='b')\n",
    "plt.vlines(192,0,.675,linestyle='--',color='b')\n",
    "plt.ylim([.63,.77])\n",
    "plt.xlim([80,700])\n",
    "plt.legend()\n",
    "plt.xlabel('Minutes per participant-wrist day ')\n",
    "plt.ylabel('Leave one subject F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_not_want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])\n",
    "final = []\n",
    "for i in range(11):\n",
    "    final.append([want[i][0],want[i][1],'Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "for i in range(11):\n",
    "    final.append([not_want[i][0],not_want[i][1],'Original Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "df = pd.DataFrame(final,columns=['Quality Threshold','F1 Score','Type of Features'])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.lineplot(x='Quality Threshold',y='F1 Score',hue='Type of Features',data=df, dashes=True)\n",
    "plt.plot(want[:,0],want[:,1],'--',marker='o',linewidth=3,markersize=20,label='Using Quality Weighted Features')\n",
    "plt.plot(not_want[:,0],not_want[:,1],'--',marker='s',linewidth=3,markersize=20,label='Using Normal Features')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Minimum Signal Quality')\n",
    "plt.legend()\n",
    "# plt.xticks(want[:,0],want[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[.1,.2,.1,.3,.4,.5,.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
