{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:   10.8s remaining:   13.5s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   13.3s remaining:    5.5s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   16.5s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2760, 11) (2760,) 2760 627 2760 2760 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 28 folds for each of 288 candidates, totalling 8064 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7842 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8064 out of 8064 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 array([[1896,  237],\n",
      "       [ 212,  415]]) 0.6489444878811571\n",
      " 0.6365030674846626 0.6618819776714514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:   10.7s remaining:   13.3s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   12.9s remaining:    5.3s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   15.8s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2667, 11) (2667,) 2667 584 2667 2667 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 28 folds for each of 288 candidates, totalling 8064 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7842 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8064 out of 8064 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 array([[1883,  200],\n",
      "       [ 185,  399]]) 0.6745562130177514\n",
      " 0.666110183639399 0.6832191780821918]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:   10.7s remaining:   13.4s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   12.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   15.5s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2557, 11) (2557,) 2557 553 2557 2557 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 27 folds for each of 288 candidates, totalling 7776 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7776 out of 7776 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 array([[1820,  184],\n",
      "       [ 152,  401]]) 0.7047451669595782\n",
      " 0.6854700854700855 0.7251356238698011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:   10.5s remaining:   13.1s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   12.3s remaining:    5.1s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   15.0s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2420, 11) (2420,) 2420 501 2420 2420 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   15.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 288 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7200 out of 7200 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 array([[1729,  190],\n",
      "       [ 153,  348]]) 0.6698748796920115\n",
      " 0.6468401486988847 0.6946107784431138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:   10.1s remaining:   12.7s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   12.2s remaining:    5.0s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   14.6s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2292, 11) (2292,) 2292 469 2292 2292 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   15.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 24 folds for each of 288 candidates, totalling 6912 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6912 out of 6912 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 array([[1669,  154],\n",
      "       [ 135,  334]]) 0.6980146290491119\n",
      " 0.6844262295081968 0.7121535181236673]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  32 out of  72 | elapsed:    9.8s remaining:   12.3s\n",
      "[Parallel(n_jobs=30)]: Done  51 out of  72 | elapsed:   11.4s remaining:    4.7s\n",
      "[Parallel(n_jobs=30)]: Done  70 out of  72 | elapsed:   13.9s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2150, 11) (2150,) 2150 420 2150 2150 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   14.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 24 folds for each of 288 candidates, totalling 6912 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6912 out of 6912 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 array([[1572,  158],\n",
      "       [ 127,  293]]) 0.6727898966704937\n",
      " 0.6496674057649667 0.6976190476190476]\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def get_f1(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "#             'svc__min_samples_leaf': [4],\n",
    "#             'svc__max_features': [.7,1],\n",
    "#             'svc__n_estimators': [100,200,300],\n",
    "#             'svc__criterion':['gini','entropy'],\n",
    "             'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[True]\n",
    "            }\n",
    "    my_score = make_scorer(f1Bias_scorer_CV,needs_proba=True)\n",
    "    clf = Pipeline([('svc',SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X[:,:],y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>bias] = 1\n",
    "    y_pred[y_pred<bias] = 0\n",
    "    y_pred = np.int64(y_pred)\n",
    "    return confusion_matrix(y,y_pred),f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred)\n",
    "    \n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "#     feature.append(np.sum(feature[-3:]))\n",
    "    return np.array(feature)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    \"\"\" Computes frequency domain features on RR interval data\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    RRints : list, shape = [n_samples,]\n",
    "           RR interval data\n",
    "    \n",
    "    band_type : string, optional\n",
    "             If band_type = None, the traditional frequency bands are used to compute \n",
    "             spectral power:\n",
    "           \n",
    "                 LF: 0.003 - 0.04 Hz\n",
    "                 HF: 0.04 - 0.15 Hz\n",
    "                 VLF: 0.15 - 0.4 Hz           \n",
    "           \n",
    "             If band_type is set to 'adapted', the bands are adjusted according to \n",
    "             the protocol laid out in:\n",
    "           \n",
    "             Long, Xi, et al. \"Spectral boundary adaptation on heart rate \n",
    "             variability for sleep and wake classification.\" International \n",
    "             Journal on Artificial Intelligence Tools 23.03 (2014): 1460002. \n",
    "                        \n",
    "    lf_bw : float, optional\n",
    "          Low frequency bandwidth centered around LF band peak frequency\n",
    "          when band_type is set to 'adapted'. Defaults to 0.11\n",
    "             \n",
    "    hf_bw : float, optional\n",
    "          High frequency bandwidth centered around HF band peak frequency\n",
    "          when band_type is set to 'adapted'. Defaults to 0.1\n",
    "          \n",
    "    plot : int, 1|0\n",
    "          Setting plot to 1 creates a matplotlib figure showing frequency\n",
    "          versus spectral power with color shading to indicate the VLF, LF,\n",
    "          and HF band bounds.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    freqDomainFeats : dict\n",
    "                   VLF_Power, LF_Power, HF_Power, LF/HF Ratio              \n",
    "    \"\"\"\n",
    "\n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = RRints\n",
    "    tss = tmStamps\n",
    "#     for c, rr in enumerate(RRints):        \n",
    "#         if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "#             NNs.append(rr)\n",
    "#             tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_ms(ecg_rr):\n",
    "    mean_col = []\n",
    "    std_col = []\n",
    "    i = 0\n",
    "    while i < len(ecg_rr):\n",
    "        start_ts = ecg_rr[i,0]\n",
    "        j = i\n",
    "        while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "            j+=1\n",
    "        mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "        std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "        i=j\n",
    "    m = np.percentile(mean_col,70)\n",
    "    s = np.percentile(std_col,30)\n",
    "    return m,s\n",
    "\n",
    "no_of_feature = 11\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    if len(user_col)==0:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "#     try:\n",
    "    participant = user_col[0]\n",
    "    feature_matrix = []\n",
    "    feature_matrix_quality = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    quality_col = []\n",
    "#         heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.1]\n",
    "    heart_rate_final = heart_rate_final[heart_rate_final[:,3]<.2]\n",
    "    ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "#     m,s = get_ms(heart_rate_final)\n",
    "#     heart_rate_final[:,1] = (heart_rate_final[:,1]-m)/s\n",
    "    for t in ts_array:\n",
    "        index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        heart_rate_window = heart_rate_final[index]\n",
    "        if np.median(heart_rate_window[:,2])<.1:\n",
    "            continue\n",
    "        label = get_label(label_data,t-20000,t+20000)\n",
    "        try:\n",
    "            r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "        except:\n",
    "            continue\n",
    "        index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "        if len(index)<40:\n",
    "            continue\n",
    "        feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "#         try:\n",
    "        feature_freq = frequencyDomain(heart_rate_window[:,1]/1000,heart_rate_window[:,0]/1000)\n",
    "#         except:\n",
    "#             continue\n",
    "#             print(feature_freq,feature_freq.values())\n",
    "        feature = list(feature)+list(feature_freq.values())\n",
    "        feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "        feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "        feature_matrix_quality.append(np.array(feature_quality).reshape(-1,4))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(label)\n",
    "        quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "    return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,4)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    for grp in np.unique(groups):\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "\n",
    "def get_XY(window):\n",
    "    final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.p','rb'))\n",
    "    duration = window\n",
    "#     final_output = [combine_data_sobc(*a) for a in final_data]\n",
    "    final_output = Parallel(n_jobs=30,verbose=4)(delayed(combine_data_sobc)(*a) for a in final_data)\n",
    "    X = np.zeros((0,no_of_feature))\n",
    "    X_qual = []\n",
    "    y = []\n",
    "    groups = []\n",
    "    qual = []\n",
    "    for m in final_output:\n",
    "        feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "        if len(feature_matrix)<50:\n",
    "            continue\n",
    "        quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "        ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "        rr_70th = np.percentile(ss,30)\n",
    "        rr_95th = np.percentile(ss,99)\n",
    "        index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "            feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "        tmp = StandardScaler().fit_transform(np.nan_to_num(feature_matrix))\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "#         print(X.shape)\n",
    "        X_qual.extend(list(quals1))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "        qual.extend(quality_col)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    X_qual = np.array(X_qual)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    groups = groups[~np.isnan(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    y = y[~np.isinf(X).any(axis=1)]\n",
    "    qual = np.array(qual)\n",
    "    qual = qual[~np.isinf(X).any(axis=1)]\n",
    "    groups = groups[~np.isinf(X).any(axis=1)]\n",
    "    X = X[~np.isinf(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "    X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "    print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))\n",
    "    m,f,p,r = get_f1(X,y,groups)\n",
    "    print(np.array([duration,m,f,p,r]))\n",
    "    return np.array([duration,m,f,p,r])\n",
    "\n",
    "# for window in np.arange(2,16,1):\n",
    "# results = Parallel(n_jobs=30,verbose=4)(delayed(get_XY)(window) for window in np.arange(2,16,1)[:1])\n",
    "results = [get_XY(window) for window in np.arange(3,9,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open('../data/rice/stress_ppg_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((0,no_of_feature))\n",
    "X_qual = []\n",
    "y = []\n",
    "groups = []\n",
    "qual = []\n",
    "for m in final_output:\n",
    "    feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "    if len(feature_matrix)<2:\n",
    "        continue\n",
    "    quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    tmp = StandardScaler().fit_transform(feature_matrix)\n",
    "    X = np.concatenate((X,feature_matrix))\n",
    "    X_qual.extend(list(quals1))\n",
    "    y.extend(label_col)\n",
    "    groups.extend(user_col)\n",
    "    qual.extend(quality_col)\n",
    "\n",
    "plt.hist(qual)\n",
    "plt.show()\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "                 'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "        if grp in not_wanted:\n",
    "            continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "X_qual = np.array(X_qual)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "y = y[~np.isinf(X).any(axis=1)]\n",
    "qual = np.array(qual)\n",
    "qual = qual[~np.isinf(X).any(axis=1)]\n",
    "groups = groups[~np.isinf(X).any(axis=1)]\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "\n",
    "X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [11,100,1000],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('svc', SVC())])\n",
    "# clf = make_pipeline(SMOTE(),SVC())\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted_2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y,cv):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "#         cv = self.cv\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "#         if self.verbose > 0:\n",
    "# #             if isinstance(parameter_iterable, Sized):\n",
    "#             n_candidates = len(parameter_iterable)\n",
    "#             print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "#                   \" {2} fits\".format(len(cv), n_candidates,\n",
    "#                                      n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=30, verbose=2,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': [11,1000],\n",
    "              'gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in [.25,.265,.26,.35]],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X,y,groups=groups)),\\\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=5, iid=False)\n",
    "grid_search.fit(X,y,cv = list(gkf.split(X,y,groups=groups)))\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X, y, gkf.split(X,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))\n",
    "# for grp in np.unique(groups):\n",
    "#     index = np.where(groups==grp)[0]\n",
    "#     print(Counter(y[index])[1]/Counter(y[index])[0],grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('/home/jupyter/mullah/cc3/rice_data/ecg_ppg_25_left4/','zip','/home/jupyter/mullah/cc3/rice_data/ecg_ppg_25_left4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
