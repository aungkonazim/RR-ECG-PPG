{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_features(a):\n",
    "    features = [np.mean(a),np.median(a),\n",
    "                np.percentile(a,75),np.percentile(a,25),\n",
    "                len(np.where(a>0)[0])/60,len(np.where(a>.25)[0])/60,len(np.where(a>.5)[0])/60,len(np.where(a>.75)[0])/60]\n",
    "    return features\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "no_of_feature = 7\n",
    "# no_of_feature = 2\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    try:\n",
    "        participant = user_col[0]\n",
    "        feature_matrix = []\n",
    "        feature_matrix_quality = []\n",
    "        user_col = []\n",
    "        label_col = []\n",
    "        quality_col = []\n",
    "        heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.05]\n",
    "        ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "        for t in ts_array:\n",
    "            index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            heart_rate_window = heart_rate_final[index]\n",
    "            if np.median(heart_rate_window[:,2])<.1:\n",
    "                continue\n",
    "            label = get_label(label_data,t-20000,t+20000)\n",
    "            try:\n",
    "                r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "            except:\n",
    "                continue\n",
    "            index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "            heart_rate_window = heart_rate_window[index]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            feature = ecg_feature_computation(heart_rate_window[:,0],\n",
    "                                              heart_rate_window[:,1])\n",
    "            feature_quality = get_features(heart_rate_window[:,2])\n",
    "            feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "            feature_matrix_quality.append(np.array(feature_quality).reshape(-1,8))\n",
    "            user_col.append(participant)\n",
    "            label_col.append(label)\n",
    "            quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "        return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,8)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  12 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=30)]: Done  28 out of  72 | elapsed:    6.6s remaining:   10.4s\n",
      "[Parallel(n_jobs=30)]: Done  43 out of  72 | elapsed:    7.3s remaining:    4.9s\n",
      "[Parallel(n_jobs=30)]: Done  58 out of  72 | elapsed:    7.8s remaining:    1.8s\n",
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3253, 7) (3253,) 3253 724 3253 3253 32\n"
     ]
    }
   ],
   "source": [
    "final_data = pickle.load(open('../data_users/data_sobc_25_secs.p','rb'))\n",
    "final_output = Parallel(n_jobs=30,verbose=5)(delayed(combine_data_sobc)(*a) for a in final_data)\n",
    "X = np.zeros((0,no_of_feature))\n",
    "X_qual = []\n",
    "y = []\n",
    "groups = []\n",
    "qual = []\n",
    "for m in final_output:\n",
    "    feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "    if len(feature_matrix)<2:\n",
    "        continue\n",
    "    quals1 = np.sqrt(np.sum(np.square(quals),axis=1))\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    tmp = StandardScaler().fit_transform(feature_matrix)\n",
    "    X = np.concatenate((X,feature_matrix))\n",
    "    X_qual.extend(list(quals1))\n",
    "    y.extend(label_col)\n",
    "    groups.extend(user_col)\n",
    "    qual.extend(quality_col)\n",
    "\n",
    "plt.hist(qual)\n",
    "plt.show()\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "                 'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "        if grp in not_wanted:\n",
    "            continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "X_qual = np.array(X_qual)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "y = y[~np.isinf(X).any(axis=1)]\n",
    "qual = np.array(qual)\n",
    "qual = qual[~np.isinf(X).any(axis=1)]\n",
    "groups = groups[~np.isinf(X).any(axis=1)]\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "\n",
    "X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 folds for each of 144 candidates, totalling 4608 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3026 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.854):\n",
      "{'svc__C': 100, 'svc__class_weight': {0: 0.35, 1: 0.65}, 'svc__gamma': 0.011048543456039806, 'svc__kernel': 'rbf', 'svc__probability': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [11,100,1],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('svc', SVC())])\n",
    "# clf = make_pipeline(SMOTE(),SVC())\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='accuracy',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2332  197]\n",
      " [ 264  460]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      2529\n",
      "           1       0.70      0.64      0.67       724\n",
      "\n",
      "    accuracy                           0.86      3253\n",
      "   macro avg       0.80      0.78      0.79      3253\n",
      "weighted avg       0.85      0.86      0.86      3253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "#     print(cv)\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "#         if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "#             n_candidates = len(parameter_iterable)\n",
    "#             print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "#                   \" {2} fits\".format(len(cv), n_candidates,\n",
    "#                                      n_candidates * len(cv)))\n",
    "        self.cv = self.verbose\n",
    "#         print(self.cv)\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=20, verbose=2,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=self.cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   0 out of   0 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e3c85a92271b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m grid_search = ModifiedGridSearchCV(svc, parameters, cv=list(gkf.split(X,y,groups=groups)),\\\n\u001b[0;32m     26\u001b[0m                                    n_jobs=20, scoring=f1Bias_scorer_CV, verbose=0, iid=False)\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-96e997ec0bd2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    178\u001b[0m                 for parameters in parameter_iterable)\n\u001b[0;32m    179\u001b[0m \u001b[1;31m#         print(out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': [11],\n",
    "              'gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .5)],\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in [.25,.265,.26]],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters, cv=list(gkf.split(X,y,groups=groups)),\\\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=0, iid=False)\n",
    "grid_search.fit(X,y)\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['063b1819-aaa3-4368-8430-6ce1a665c1d3',\n",
       "       '063b1819-aaa3-4368-8430-6ce1a665c1d3',\n",
       "       '063b1819-aaa3-4368-8430-6ce1a665c1d3', ...,\n",
       "       'ffaf3e69-ed8b-4522-9a3d-4f0f3400aa03',\n",
       "       'ffaf3e69-ed8b-4522-9a3d-4f0f3400aa03',\n",
       "       'ffaf3e69-ed8b-4522-9a3d-4f0f3400aa03'], dtype='<U36')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
