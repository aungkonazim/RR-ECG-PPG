{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "(3206, 7) (3206,) 3206 740 3206 3206 34\n",
      "Fitting 34 folds for each of 48 candidates, totalling 1632 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   54.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1632 out of 1632 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 array([[2181,  285],\n",
      "       [ 248,  492]]) 0.6486486486486486\n",
      " 0.6332046332046332 0.6648648648648648]\n",
      "list index out of range\n",
      "(3278, 7) (3278,) 3278 772 3278 3278 34\n",
      "Fitting 34 folds for each of 48 candidates, totalling 1632 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.3s\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def get_f1(X,y,groups):\n",
    "    paramGrid = {\n",
    "#             'svc__min_samples_leaf': [4],\n",
    "#             'svc__max_features': [.7,1],\n",
    "#             'svc__n_estimators': [100,200,300],\n",
    "#             'svc__criterion':['gini','entropy'],\n",
    "             'svc__kernel': ['rbf'],\n",
    "             'svc__C': [100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[True]\n",
    "            }\n",
    "    my_score = make_scorer(f1Bias_scorer_CV,needs_proba=True)\n",
    "    clf = Pipeline([('sts',StandardScaler()),('svc',SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X[:,:],y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>bias] = 1\n",
    "    y_pred[y_pred<bias] = 0\n",
    "    y_pred = np.int64(y_pred)\n",
    "    return confusion_matrix(y,y_pred),f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred)\n",
    "    \n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "#     feature.append(np.sum(feature[-3:]))\n",
    "    return np.array(feature)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_ms(ecg_rr):\n",
    "    mean_col = []\n",
    "    std_col = []\n",
    "    i = 0\n",
    "    while i < len(ecg_rr):\n",
    "        start_ts = ecg_rr[i,0]\n",
    "        j = i\n",
    "        while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "            j+=1\n",
    "        mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "        std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "        i=j\n",
    "    m = np.percentile(mean_col,70)\n",
    "    s = np.percentile(std_col,30)\n",
    "    return m,s\n",
    "\n",
    "no_of_feature = 7\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    try:\n",
    "        participant = user_col[0]\n",
    "        feature_matrix = []\n",
    "        feature_matrix_quality = []\n",
    "        user_col = []\n",
    "        label_col = []\n",
    "        quality_col = []\n",
    "#         heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.1]\n",
    "        heart_rate_final = heart_rate_final[heart_rate_final[:,3]<.25]\n",
    "        ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "        m,s = get_ms(heart_rate_final)\n",
    "        heart_rate_final[:,1] = (heart_rate_final[:,1]-m)/s\n",
    "        for t in ts_array:\n",
    "            index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            heart_rate_window = heart_rate_final[index]\n",
    "            if np.median(heart_rate_window[:,2])<.1:\n",
    "                continue\n",
    "            label = get_label(label_data,t-20000,t+20000)\n",
    "            try:\n",
    "                r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "            except:\n",
    "                continue\n",
    "            index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "            heart_rate_window = heart_rate_window[index]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "            feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "            feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "            feature_matrix_quality.append(np.array(feature_quality).reshape(-1,4))\n",
    "            user_col.append(participant)\n",
    "            label_col.append(label)\n",
    "            quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "        return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,4)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.array([])\n",
    "from sklearn import linear_model\n",
    "\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "#     not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "#                  'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "#         if grp in not_wanted:\n",
    "#             continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "\n",
    "def get_XY(window):\n",
    "    final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.p','rb'))\n",
    "    duration = window\n",
    "#     for a in final_data:\n",
    "#         print(a)\n",
    "    final_output = [combine_data_sobc(*a) for a in final_data]\n",
    "    X = np.zeros((0,no_of_feature))\n",
    "    X_qual = []\n",
    "    y = []\n",
    "    groups = []\n",
    "    qual = []\n",
    "    for m in final_output:\n",
    "        feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "        if len(feature_matrix)<10:\n",
    "            continue\n",
    "        quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "        ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "        rr_70th = np.percentile(ss,1)\n",
    "        rr_95th = np.percentile(ss,99)\n",
    "        index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "            feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "        tmp = StandardScaler().fit_transform(np.nan_to_num(feature_matrix))\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "        X_qual.extend(list(quals1))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "        qual.extend(quality_col)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    X_qual = np.array(X_qual)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    groups = groups[~np.isnan(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "    y = y[~np.isinf(X).any(axis=1)]\n",
    "    qual = np.array(qual)\n",
    "    qual = qual[~np.isinf(X).any(axis=1)]\n",
    "    groups = groups[~np.isinf(X).any(axis=1)]\n",
    "    X = X[~np.isinf(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "    X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "    print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))\n",
    "    m,f,p,r = get_f1(X,y,groups)\n",
    "    print(np.array([duration,m,f,p,r]))\n",
    "    return np.array([duration,m,f,p,r])\n",
    "\n",
    "# for window in np.arange(2,16,1):\n",
    "# results = Parallel(n_jobs=30,verbose=4)(delayed(get_XY)(window) for window in np.arange(2,16,1)[:1])\n",
    "results = [get_XY(window) for window in np.arange(3,10,1)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open('../data/rice/stress_ppg_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((0,no_of_feature))\n",
    "X_qual = []\n",
    "y = []\n",
    "groups = []\n",
    "qual = []\n",
    "for m in final_output:\n",
    "    feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "    if len(feature_matrix)<2:\n",
    "        continue\n",
    "    quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    tmp = StandardScaler().fit_transform(feature_matrix)\n",
    "    X = np.concatenate((X,feature_matrix))\n",
    "    X_qual.extend(list(quals1))\n",
    "    y.extend(label_col)\n",
    "    groups.extend(user_col)\n",
    "    qual.extend(quality_col)\n",
    "\n",
    "plt.hist(qual)\n",
    "plt.show()\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "                 'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "        if grp in not_wanted:\n",
    "            continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "X_qual = np.array(X_qual)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "y = y[~np.isinf(X).any(axis=1)]\n",
    "qual = np.array(qual)\n",
    "qual = qual[~np.isinf(X).any(axis=1)]\n",
    "groups = groups[~np.isinf(X).any(axis=1)]\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "\n",
    "X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [11,100,1000],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('svc', SVC())])\n",
    "# clf = make_pipeline(SMOTE(),SVC())\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted_2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y,cv):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "#         cv = self.cv\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "#         if self.verbose > 0:\n",
    "# #             if isinstance(parameter_iterable, Sized):\n",
    "#             n_candidates = len(parameter_iterable)\n",
    "#             print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "#                   \" {2} fits\".format(len(cv), n_candidates,\n",
    "#                                      n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=30, verbose=2,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': [11,1000],\n",
    "              'gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in [.25,.265,.26,.35]],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X,y,groups=groups)),\\\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=5, iid=False)\n",
    "grid_search.fit(X,y,cv = list(gkf.split(X,y,groups=groups)))\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X, y, gkf.split(X,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))\n",
    "# for grp in np.unique(groups):\n",
    "#     index = np.where(groups==grp)[0]\n",
    "#     print(Counter(y[index])[1]/Counter(y[index])[0],grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
