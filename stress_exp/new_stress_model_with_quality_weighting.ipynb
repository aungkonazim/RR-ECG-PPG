{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "#     feature.append(np.sum(feature[-3:]))\n",
    "    return np.array(feature)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_ms(ecg_rr):\n",
    "    mean_col = []\n",
    "    std_col = []\n",
    "    i = 0\n",
    "    while i < len(ecg_rr):\n",
    "        start_ts = ecg_rr[i,0]\n",
    "        j = i\n",
    "        while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "            j+=1\n",
    "        mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "        std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "        i=j\n",
    "    m = np.percentile(mean_col,70)\n",
    "    s = np.percentile(std_col,30)\n",
    "    return m,s\n",
    "\n",
    "no_of_feature = 7\n",
    "# no_of_feature = 2\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    try:\n",
    "        participant = user_col[0]\n",
    "        feature_matrix = []\n",
    "        feature_matrix_quality = []\n",
    "        user_col = []\n",
    "        label_col = []\n",
    "        quality_col = []\n",
    "        heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.05]\n",
    "        ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "        m,s = get_ms(heart_rate_final)\n",
    "        heart_rate_final[:,1] = (heart_rate_final[:,1]-m)/s\n",
    "        for t in ts_array:\n",
    "            index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            heart_rate_window = heart_rate_final[index]\n",
    "            if np.median(heart_rate_window[:,2])<.1:\n",
    "                continue\n",
    "            label = get_label(label_data,t-20000,t+20000)\n",
    "            try:\n",
    "                r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "            except:\n",
    "                continue\n",
    "            index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "            heart_rate_window = heart_rate_window[index]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "            feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "            feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "            feature_matrix_quality.append(np.array(feature_quality).reshape(-1,4))\n",
    "            user_col.append(participant)\n",
    "            label_col.append(label)\n",
    "            quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "        return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,4)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3253, 7) (3253,) 3253 724 3253 3253 32\n"
     ]
    }
   ],
   "source": [
    "final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_25_secs.p','rb'))\n",
    "# final_output = Parallel(n_jobs=30,verbose=5)(delayed(combine_data_sobc)(*a) for a in final_data)\n",
    "final_output = [combine_data_sobc(*a) for a in final_data]\n",
    "X = np.zeros((0,no_of_feature))\n",
    "X_qual = []\n",
    "y = []\n",
    "groups = []\n",
    "qual = []\n",
    "for m in final_output:\n",
    "    feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "    if len(feature_matrix)<2:\n",
    "        continue\n",
    "    quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    tmp = StandardScaler().fit_transform(feature_matrix)\n",
    "    X = np.concatenate((X,feature_matrix))\n",
    "    X_qual.extend(list(quals1))\n",
    "    y.extend(label_col)\n",
    "    groups.extend(user_col)\n",
    "    qual.extend(quality_col)\n",
    "\n",
    "plt.hist(qual)\n",
    "plt.show()\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "                 'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "        if grp in not_wanted:\n",
    "            continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "X_qual = np.array(X_qual)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "y = y[~np.isinf(X).any(axis=1)]\n",
    "qual = np.array(qual)\n",
    "qual = qual[~np.isinf(X).any(axis=1)]\n",
    "groups = groups[~np.isinf(X).any(axis=1)]\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "\n",
    "X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 folds for each of 144 candidates, totalling 4608 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   56.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.674):\n",
      "{'svc__C': 11, 'svc__class_weight': {0: 0.265, 1: 0.735}, 'svc__gamma': 0.011048543456039806, 'svc__kernel': 'rbf', 'svc__probability': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [11,100,1000],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('svc', SVC())])\n",
    "# clf = make_pipeline(SMOTE(),SVC())\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2243  286]\n",
      " [ 215  509]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      2529\n",
      "           1       0.64      0.70      0.67       724\n",
      "\n",
      "    accuracy                           0.85      3253\n",
      "   macro avg       0.78      0.79      0.78      3253\n",
      "weighted avg       0.85      0.85      0.85      3253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted_2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y,cv):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "#         cv = self.cv\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "#         if self.verbose > 0:\n",
    "# #             if isinstance(parameter_iterable, Sized):\n",
    "#             n_candidates = len(parameter_iterable)\n",
    "#             print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "#                   \" {2} fits\".format(len(cv), n_candidates,\n",
    "#                                      n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=30, verbose=2,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  36 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=30)]: Done  96 out of  96 | elapsed: 27.7min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModifiedGridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3aea4fdd8582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                    n_jobs=20, scoring=f1Bias_scorer_CV, verbose=5, iid=False)\n\u001b[1;32m     27\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModifiedGridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': [11,1000],\n",
    "              'gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in [.25,.265,.26,.35]],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X,y,groups=groups)),\\\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=5, iid=False)\n",
    "grid_search.fit(X,y,cv = list(gkf.split(X,y,groups=groups)))\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6709928617780662 0.2534510877955277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      2529\n",
      "           1       0.63      0.71      0.67       724\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      3253\n",
      "   macro avg       0.77      0.80      0.78      3253\n",
      "weighted avg       0.85      0.84      0.85      3253\n",
      "\n",
      "[[2229  300]\n",
      " [ 207  517]]\n",
      "0.653061224489796\n"
     ]
    }
   ],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X, y, gkf.split(X,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))\n",
    "# for grp in np.unique(groups):\n",
    "#     index = np.where(groups==grp)[0]\n",
    "#     print(Counter(y[index])[1]/Counter(y[index])[0],grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
