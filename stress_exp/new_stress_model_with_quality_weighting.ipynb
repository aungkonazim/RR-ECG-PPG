{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2557, 11) (2557,) 2557 553 2557 2557 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:   16.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 27 folds for each of 288 candidates, totalling 7776 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7776 out of 7776 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7211367673179396 0.33655668701482494 bias\n",
      "[5 array([[1837,  167],\n",
      "       [ 147,  406]]) 0.7211367673179396\n",
      " 0.7085514834205934 0.7341772151898734]\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "from copy import deepcopy\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=10,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    delta = 0.1\n",
    "    paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[True]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('svc', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "#     print(data)\n",
    "#     while len(feature_names)>1:\n",
    "#         results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "#         results = np.array(results)\n",
    "#         print(results,results.shape)\n",
    "#         ind_min = np.argmax(results[:,0])\n",
    "#         min_f1 = results[ind_min,0]\n",
    "#         min_index = np.int64(results[ind_min,1])\n",
    "#         name_feature = feature_names[min_index]\n",
    "#         data.append([name_feature,min_f1])\n",
    "#         X = np.delete(X,min_index,axis=1)\n",
    "#         feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "#         print(data)\n",
    "    return data    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_f1(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "#             'svc__min_samples_leaf': [4],\n",
    "#             'svc__max_features': [.7,1],\n",
    "#             'svc__n_estimators': [100,200,300],\n",
    "#             'svc__criterion':['gini','entropy'],\n",
    "             'svc__kernel': ['rbf'],\n",
    "             'svc__C': [1,10,100],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .25)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.4,.5]],\n",
    "             'svc__probability':[True]\n",
    "            }\n",
    "    my_score = make_scorer(f1Bias_scorer_CV,needs_proba=True)\n",
    "    clf = Pipeline([('svc',SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X[:,:],y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.set_params(svc__probability=True)\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    print(f1,bias,'bias')\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>=bias] = 1\n",
    "    y_pred[y_pred<bias] = 0\n",
    "    y_pred = np.int64(y_pred)\n",
    "    clf.fit(X,y)\n",
    "    return confusion_matrix(y,y_pred),f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred),clf\n",
    "    \n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "#     feature.append(np.sum(feature[-3:]))\n",
    "    return np.array(feature)\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = RRints\n",
    "    tss = tmStamps\n",
    "#     for c, rr in enumerate(RRints):        \n",
    "#         if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "#             NNs.append(rr)\n",
    "#             tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "#     a = a[:,0]\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_ms(ecg_rr):\n",
    "    mean_col = []\n",
    "    std_col = []\n",
    "    i = 0\n",
    "    while i < len(ecg_rr):\n",
    "        start_ts = ecg_rr[i,0]\n",
    "        j = i\n",
    "        while j<len(ecg_rr) and ecg_rr[j,0]-start_ts <= 60000:\n",
    "            j+=1\n",
    "        mean_col.append(np.mean(ecg_rr[i:j+1,1]))\n",
    "        std_col.append(np.std(ecg_rr[i:j+1,1]))\n",
    "        i=j\n",
    "    m = np.percentile(mean_col,70)\n",
    "    s = np.percentile(std_col,30)\n",
    "    return m,s\n",
    "\n",
    "no_of_feature = 11\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    if len(user_col)==0:\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.zeros((0,4))\n",
    "#     try:\n",
    "    participant = user_col[0]\n",
    "    feature_matrix = []\n",
    "    feature_matrix_quality = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    quality_col = []\n",
    "#         heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.1]\n",
    "    heart_rate_final = heart_rate_final[heart_rate_final[:,3]<.2]\n",
    "    ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "#     m,s = get_ms(heart_rate_final)\n",
    "#     heart_rate_final[:,1] = (heart_rate_final[:,1]-m)/s\n",
    "    for t in ts_array:\n",
    "        index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        heart_rate_window = heart_rate_final[index]\n",
    "        if np.median(heart_rate_window[:,2])<.1:\n",
    "            continue\n",
    "        label = get_label(label_data,t-20000,t+20000)\n",
    "        try:\n",
    "#             r,tt = np.mean(heart_rate_window[:,1]),np.std(heart_rate_window[:,1])\n",
    "            r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "        except:\n",
    "            continue\n",
    "        index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "        if len(index)<40:\n",
    "            continue\n",
    "        feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "#         try:\n",
    "        feature_freq = frequencyDomain(heart_rate_window[:,1]/1000,heart_rate_window[:,0]/1000)\n",
    "#         except:\n",
    "#             continue\n",
    "#             print(feature_freq,feature_freq.values())\n",
    "        feature = list(feature)+list(feature_freq.values())\n",
    "        feature_quality = get_quality_features(heart_rate_window[:,2])\n",
    "        feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "        feature_matrix_quality.append(np.array(feature_quality).reshape(-1,4))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(label)\n",
    "        quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "    return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,4)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    for grp in np.unique(groups):\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "\n",
    "def get_XY(window):\n",
    "    final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_'+str(window)+'_secs.p','rb'))\n",
    "    duration = window\n",
    "#     final_output = [combine_data_sobc(*a) for a in final_data]\n",
    "    final_output = Parallel(n_jobs=30,verbose=1)(delayed(combine_data_sobc)(*a) for a in final_data)\n",
    "    X = np.zeros((0,no_of_feature))\n",
    "    X_qual = []\n",
    "    y = []\n",
    "    groups = []\n",
    "    qual = []\n",
    "    for m in final_output:\n",
    "        feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "        if len(feature_matrix)<50:\n",
    "            continue\n",
    "#         quals1 = np.array([1]*feature_matrix.shape[0])\n",
    "        quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "        ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "        rr_70th = np.percentile(ss,20)\n",
    "        rr_95th = np.percentile(ss,99)\n",
    "        index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "            feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "        tmp = StandardScaler().fit_transform(np.nan_to_num(feature_matrix))\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "#         print(X.shape)\n",
    "        X_qual.extend(list(quals1))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "        qual.extend(quality_col)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    X_qual = np.array(X_qual)\n",
    "    y = y[~np.isnan(X).any(axis=1)]\n",
    "    groups = groups[~np.isnan(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "    X = X[~np.isnan(X).any(axis=1)]\n",
    "    y = y[~np.isinf(X).any(axis=1)]\n",
    "    qual = np.array(qual)\n",
    "    qual = qual[~np.isinf(X).any(axis=1)]\n",
    "    groups = groups[~np.isinf(X).any(axis=1)]\n",
    "    X = X[~np.isinf(X).any(axis=1)]\n",
    "    X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "    X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "    pickle.dump([X,y,groups,X_qual],open('lab_data.p','wb'))\n",
    "    index = np.array([0,1,4,5,6,8])\n",
    "    print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))\n",
    "#     data  = get_results_backward_elimination(X[:,index],y,groups)\n",
    "#     print(data)\n",
    "    m,f,p,r,clf = get_f1(X[:,index],y,groups)    \n",
    "    print(np.array([duration,m,f,p,r]))\n",
    "    return np.array([duration,m,f,p,r]),clf\n",
    "\n",
    "# for window in np.arange(2,16,1):\n",
    "# results = Parallel(n_jobs=30,verbose=4)(delayed(get_XY)(window) for window in np.arange(2,16,1)[:1])\n",
    "results = [get_XY(window) for window in np.arange(5,6,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results[0][1],open('../models/stress_ppg_final.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][1]\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({'font.size':20})\n",
    "feature_importances = np.array([['all features', 0.7104795737122557], \n",
    "                     ['mean', 0.7176781002638521], \n",
    "                     ['lf-hf', 0.7166813768755516], \n",
    "                     ['inverse_hr', 0.7165775401069518], \n",
    "                     ['vlf', 0.7184986595174263], \n",
    "                     ['hf', 0.7225691347011596], \n",
    "                     ['20th', 0.717720391807658], \n",
    "                     ['var', 0.7109170305676856], \n",
    "                     ['iqr', 0.6998254799301921], \n",
    "                     ['lf', 0.6993865030674846], \n",
    "                     ['80th', 0.6678832116788321]])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(feature_importances[:6,0],[np.float(a) for a in feature_importances[:6,1]],c='r',linewidth=4)\n",
    "plt.plot(feature_importances[5:,0],[np.float(a) for a in feature_importances[5:,1]],c='g',linewidth=4)\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('Most Important Feature = Heart Rate')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "feature_importances[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open('../data/rice/stress_ppg_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_day_5/'\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "[['all', 0.7104795737122557], ['mean', 0.7176781002638521], ['lf-hf', 0.7166813768755516], ['median', 0.7165775401069518], ['vlf', 0.7184986595174263], ['hf', 0.7225691347011596], ['20th', 0.717720391807658], ['var', 0.7109170305676856], ['iqr', 0.6998254799301921], ['lf', 0.6993865030674846], ['80th', 0.6678832116788321]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,groups,qual = [],[],[]\n",
    "for f in os.listdir(directory1):\n",
    "    if f[-1]!='p':\n",
    "        continue\n",
    "    data = pickle.load(open(directory1+f,'rb'))[0]\n",
    "    print(data.shape,data[['final_feature_matrix','quality_mag']].dropna().shape)\n",
    "    ind = np.arange(data.shape[0])\n",
    "    ind = np.random.choice(ind,25)\n",
    "    X.append(np.array(list(data['final_feature_matrix']))[ind])\n",
    "    groups.extend([f]*ind.shape[0])\n",
    "    qual.extend(list(np.array(data['quality_mag'])[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([np.concatenate(X),np.array(groups),np.array(qual)],open('field_data.p','wb'))\n",
    "np.concatenate(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,auc,classification_report,make_scorer,roc_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV,StratifiedKFold\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer\n",
    "\n",
    "plt.rcParams.update({'font.size':20})\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "def f1Bias_scorer_CV(y_true,y_pred, ret_bias=False):\n",
    "    probs = y_true\n",
    "    y = y_pred\n",
    "    if not ret_bias:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        return auc(fpr,tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0) and (precision[i]>=recall[i]-.05):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "\n",
    "def get_f1(X,y,X_test,y_test):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    paramGrid = {\n",
    "        'svc__class_weight':[{-1:delta,1:1-delta} for delta in np.arange(0,1,.1)],\n",
    "        'svc__bootstrap': [True, False],\n",
    "         'svc__max_features': [1],\n",
    "         'svc__n_estimators': [100,200,300,400,500]\n",
    "    }\n",
    "    clf = Pipeline([('sts',StandardScaler()),('svc',RandomForestClassifier())])\n",
    "    gkf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X, y)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.fit(X,y)\n",
    "    probs = clf.predict_proba(X_test)[:,1]\n",
    "    f1,bias = f1Bias_scorer_CV(probs, y_test, ret_bias=True)\n",
    "#     print(f1,bias,'bias')\n",
    "    y_pred = probs\n",
    "    y_pred[y_pred>=bias] = 1\n",
    "    y_pred[y_pred<bias] = -1\n",
    "    y_pred = np.int64(y_pred)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(16,8))\n",
    "    tmp = np.int64(confusion_matrix(y_test,y_pred))\n",
    "    sns.heatmap(tmp,annot=True,fmt='g',annot_kws={\"fontsize\":25})\n",
    "    plt.xticks([.5,1.5],['LAB','FIELD'])\n",
    "    plt.yticks([0.5,1.5],['LAB','FIELD'])\n",
    "    plt.show()\n",
    "#     print(accuracy_score(y_test,y_pred),np.mean(y),np.mean(y_test))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    return y_test,y_pred,confusion_matrix(y_test,y_pred),accuracy_score(y_test,y_pred),precision_score(y_test,y_pred),recall_score(y_test,y_pred),f1_score(y_test,y_pred)\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    \n",
    "    delta = 0.1\n",
    "    paramGrid = {'rf__kernel': ['rbf'],\n",
    "                 'rf__C': [100,10,200],\n",
    "                 'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .25)],\n",
    "                 'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3]],\n",
    "                 'rf__probability':[True]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "    print(data)\n",
    "    while len(feature_names)>1:\n",
    "        results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "        results = np.array(results)\n",
    "        print(results,results.shape)\n",
    "        ind_min = np.argmax(results[:,0])\n",
    "        min_f1 = results[ind_min,0]\n",
    "        min_index = np.int64(results[ind_min,1])\n",
    "        name_feature = feature_names[min_index]\n",
    "        data.append([name_feature,min_f1])\n",
    "        X = np.delete(X,min_index,axis=1)\n",
    "        feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "        print(data)\n",
    "    return data\n",
    "\n",
    "def partition_participants_lab(X,y,groups,qual,stop = 25):\n",
    "    lpgo = LeavePGroupsOut(n_groups=6)\n",
    "    lpgo.get_n_splits(X, y, groups)\n",
    "    count = 0\n",
    "    for train_index, test_index in lpgo.split(X, y, groups):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        qual_train, qual_test = qual[train_index], qual[test_index]\n",
    "        stress_train,stress_test = y[train_index],y[test_index]\n",
    "        groups_train,groups_test = groups[train_index],groups[test_index]\n",
    "        count+=1\n",
    "        if count==stop:\n",
    "            break\n",
    "#     print(X_train.shape,X_test.shape)\n",
    "    return [X_train,qual_train,groups_train,stress_train],[X_test,qual_test,groups_test,stress_test]\n",
    "\n",
    "def partition_participants_field(X,groups,qual,stop = 25):\n",
    "    lpgo = LeavePGroupsOut(n_groups=25)\n",
    "    lpgo.get_n_splits(X, qual, groups)\n",
    "    count = 0\n",
    "    for train_index, test_index in lpgo.split(X, qual, groups):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        qual_train, qual_test = qual[train_index], qual[test_index]\n",
    "        groups_train,groups_test = groups[train_index],groups[test_index]\n",
    "        count+=1\n",
    "        if count==stop:\n",
    "            break\n",
    "    \n",
    "    return [X_train,qual_train,groups_train],[X_test,qual_test,groups_test]\n",
    "\n",
    "def get_train_data(train_lab,train_field):\n",
    "    X_train,qual_train,groups_train,stress_train = train_lab\n",
    "    X_train_field,qual_train_field,groups_train_field = train_field\n",
    "    y_train = np.array([-1]*X_train.shape[0]+[1]*X_train_field.shape[0])\n",
    "    X_train = np.concatenate([X_train,X_train_field])\n",
    "    qual = np.array(list(qual_train) + list(qual_train_field))\n",
    "    groups_train = np.array(list(groups_train) + list(groups_train_field))\n",
    "#     print(X_train.shape,y_train.shape,qual.shape,np.unique(y_train))\n",
    "    return X_train,y_train,qual,groups_train\n",
    "\n",
    "def get_results(stop):\n",
    "    X_lab,y_lab,groups_lab,qual_lab = pickle.load(open('lab_data.p','rb'))\n",
    "    X_field,groups_field,qual_field = pickle.load(open('field_data.p','rb'))\n",
    "    train_lab,test_lab = partition_participants_lab(X_lab,y_lab,groups_lab,qual_lab,stop=stop)\n",
    "    train_field,test_field = partition_participants_field(X_field,groups_field,qual_field,stop=stop)\n",
    "    X_train,y_train,qual_train,groups_train = get_train_data(train_lab,train_field)\n",
    "    X_test,y_test,qual_test,groups_test = get_train_data(test_lab,test_field)\n",
    "    y_test,y_pred,conf,acc,precision,recall,f1 = get_f1(np.nan_to_num(X_train),y_train,np.nan_to_num(X_test),y_test)\n",
    "    return y_test,y_pred,qual_test,conf,acc,precision,recall,f1\n",
    "output = [get_results(i) for i in range(2,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(output,open('results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('results.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all,y_pred_all,qual_test_all = [],[],[]\n",
    "conf = np.zeros((2,2))\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in results:\n",
    "    y_test_all.extend(list(a[0]))\n",
    "    y_pred_all.extend(list(a[1]))\n",
    "    qual_test_all.extend(list(a[2]))\n",
    "    conf+=a[3]\n",
    "    scores.append(np.array(a[4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are  ~25 participants in lab and ~93 participants in field. \n",
    "Since each lab participant has around 100 instances (rows), we randomly sample 25 instances from every participant in field to balance the data.\n",
    "\n",
    "Now we obtain results by partitioning participants in train and test fold. (6 lab & 25 field participants in test fold). \n",
    "\n",
    "In training, we perform 10 fold stratified cross validation to maximize the AUC score. The random forest model found is then applied to the test fold. 98 such runs are made shuffling the participant-ids to create the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(16,8))\n",
    "tmp = np.int64(confusion_matrix(y_test_all,y_pred_all))//98\n",
    "sns.heatmap(tmp,annot=True,fmt='g',annot_kws={\"fontsize\":35})\n",
    "plt.xticks([.5,1.5],['LAB','FIELD'])\n",
    "plt.yticks([0.5,1.5],['LAB','FIELD'])\n",
    "plt.title('Average Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.boxplot(scores)\n",
    "plt.xticks([1,2,3,4],['accuracy','precision','recall','f1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.scatter(scores[:,1],scores[:,2],c=scores[:,3])\n",
    "plt.xlabel('PRECISION')\n",
    "plt.ylabel('RECALL')\n",
    "plt.title('PRECISION vs. RECALL (F1 in colorbar)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all,y_pred_all,qual_test_all = np.array(y_test_all),np.array(y_pred_all),np.array(qual_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "qscores,x = [],[]\n",
    "for i in np.arange(0,100,10):\n",
    "    index = np.where((qual_test_all>=i/100)&(qual_test_all<(i+10)/100))[0]\n",
    "    y = y_test_all[index]\n",
    "    y_pred = y_pred_all[index]\n",
    "    qscores.append(f1_score(y,y_pred))\n",
    "    x.append(str(np.mean(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.bar(x,np.array(qscores))\n",
    "plt.xlabel('Minute Level Quality')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models.cifar as models\n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 and 100 Training')\n",
    "# Datasets\n",
    "parser.add_argument('-d', '--dataset', default='cifar10', type=str)\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--train-batch', default=128, type=int, metavar='N',\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--test-batch', default=100, type=int, metavar='N',\n",
    "                    help='test batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--drop', '--dropout', default=0, type=float,\n",
    "                    metavar='Dropout', help='Dropout ratio')\n",
    "parser.add_argument('--schedule', type=int, nargs='+', default=[150, 225],\n",
    "                        help='Decrease learning rate at these epochs.')\n",
    "parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "# Checkpoints\n",
    "parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH',\n",
    "                    help='path to save checkpoint (default: checkpoint)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "# Architecture\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                        ' | '.join(model_names) +\n",
    "                        ' (default: resnet20)')\n",
    "parser.add_argument('--depth', type=int, default=20, help='Model depth.')\n",
    "parser.add_argument('--widen-factor', type=int, default=10, help='Widen factor. 10')\n",
    "parser.add_argument('--growthRate', type=int, default=12, help='Growth rate for DenseNet.')\n",
    "parser.add_argument('--compressionRate', type=int, default=2, help='Compression Rate (theta) for DenseNet.')\n",
    "# Miscs\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "\n",
    "# Random Erasing\n",
    "parser.add_argument('--p', default=0, type=float, help='Random Erasing probability')\n",
    "parser.add_argument('--sh', default=0.4, type=float, help='max erasing area')\n",
    "parser.add_argument('--r1', default=0.3, type=float, help='aspect of erasing area')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
