{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "from ecg import ecg_feature_computation\n",
    "import math\n",
    "from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_features(a):\n",
    "    features = [len(a)/60,np.median(a),\n",
    "                len(np.where(a>0)[0])/60,len(np.where(a>.5)[0])/60,\n",
    "                len(np.where(a>.4)[0])/60,len(np.where(a>.6)[0])/60,\n",
    "                len(np.where(a>.8)[0])/60,len(np.where(a==0)[0])/60]\n",
    "    return np.array(features)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "no_of_feature = 7\n",
    "# no_of_feature = 2\n",
    "from scipy.stats import variation\n",
    "def combine_data_sobc(feature_matrix,user_col,label_col,quality_col,heart_rate_final,label_data):\n",
    "    try:\n",
    "        participant = user_col[0]\n",
    "        feature_matrix = []\n",
    "        feature_matrix_quality = []\n",
    "        user_col = []\n",
    "        label_col = []\n",
    "        quality_col = []\n",
    "        heart_rate_final = heart_rate_final[heart_rate_final[:,2]>.05]\n",
    "        ts_array = np.arange(heart_rate_final[0,0],heart_rate_final[-1,0],60000)\n",
    "        for t in ts_array:\n",
    "            index = np.where((heart_rate_final[:,0]>=t-30000)&(heart_rate_final[:,0]<t+30000))[0]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "            heart_rate_window = heart_rate_final[index]\n",
    "            if np.median(heart_rate_window[:,2])<.1:\n",
    "                continue\n",
    "            label = get_label(label_data,t-20000,t+20000)\n",
    "            try:\n",
    "                r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,2]>.25,1],heart_rate_window[heart_rate_window[:,2]>.25,2])\n",
    "            except:\n",
    "                continue\n",
    "            index = np.where((heart_rate_window[:,1]<r+3*tt)&(heart_rate_window[:,1]>r-3*tt))[0]\n",
    "            heart_rate_window = heart_rate_window[index]\n",
    "            if len(index)<30:\n",
    "                continue\n",
    "#             feature = ecg_feature_computation(heart_rate_window[:,0],\n",
    "#                                               heart_rate_window[:,1])\n",
    "            feature = get_weighted_rr_features(heart_rate_window[:,1:])\n",
    "            feature_quality = get_features(heart_rate_window[:,2])\n",
    "            feature_matrix.append(np.array(feature).reshape(-1,no_of_feature))\n",
    "            feature_matrix_quality.append(np.array(feature_quality).reshape(-1,8))\n",
    "            user_col.append(participant)\n",
    "            label_col.append(label)\n",
    "            quality_col.append(np.median(heart_rate_window[:,2]))\n",
    "        return np.array(feature_matrix).reshape(-1,no_of_feature),user_col,label_col,quality_col,heart_rate_final,np.array(feature_matrix_quality).reshape(-1,8)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.zeros((0,no_of_feature)),[],[],[],[],np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  12 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=30)]: Done  28 out of  72 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=30)]: Done  43 out of  72 | elapsed:    0.7s remaining:    0.5s\n",
      "[Parallel(n_jobs=30)]: Done  58 out of  72 | elapsed:    0.7s remaining:    0.2s\n",
      "[Parallel(n_jobs=30)]: Done  72 out of  72 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3253, 7) (3253,) 3253 724 3253 3253 32\n"
     ]
    }
   ],
   "source": [
    "final_data = pickle.load(open('/home/jupyter/mullah/Test/data_yield/data/data_sobc_25_secs.p','rb'))\n",
    "final_output = Parallel(n_jobs=30,verbose=5)(delayed(combine_data_sobc)(*a) for a in final_data)\n",
    "X = np.zeros((0,no_of_feature))\n",
    "X_qual = []\n",
    "y = []\n",
    "groups = []\n",
    "qual = []\n",
    "for m in final_output:\n",
    "    feature_matrix,user_col,label_col,quality_col,hr,quals = m\n",
    "    if len(feature_matrix)<2:\n",
    "        continue\n",
    "    quals1 = np.sqrt(np.sum(np.square(quals),axis=1)/quals.shape[1])\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    tmp = StandardScaler().fit_transform(feature_matrix)\n",
    "    X = np.concatenate((X,feature_matrix))\n",
    "    X_qual.extend(list(quals1))\n",
    "    y.extend(label_col)\n",
    "    groups.extend(user_col)\n",
    "    qual.extend(quality_col)\n",
    "\n",
    "plt.hist(qual)\n",
    "plt.show()\n",
    "from sklearn import linear_model\n",
    "def get_only_stress_no_stress(X,groups,y,qual,X_qual):\n",
    "    y = np.int64(y)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,groups,y,qual,X_qual = X[index,:],groups[index],y[index],qual[index],X_qual[index]\n",
    "    ind = []\n",
    "    not_wanted = ['ae1238c0-6146-491b-b199-ead784386c5b',\n",
    "                 'bb3e2113-ca54-4f36-9ecf-d5fff354521f']\n",
    "    for grp in np.unique(groups):\n",
    "        if grp in not_wanted:\n",
    "            continue\n",
    "        tmp = np.where(groups==grp)[0]\n",
    "        if len(np.unique(y[tmp]))>1:\n",
    "            ind.extend(list(tmp))\n",
    "    ind = np.int64(np.array(ind))\n",
    "    return X[ind],y[ind],groups[ind],qual[ind],X_qual[ind]\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "X_qual = np.array(X_qual)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "y = y[~np.isinf(X).any(axis=1)]\n",
    "qual = np.array(qual)\n",
    "qual = qual[~np.isinf(X).any(axis=1)]\n",
    "groups = groups[~np.isinf(X).any(axis=1)]\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "X_qual = X_qual[~np.isinf(X).any(axis=1)]\n",
    "\n",
    "X,y,groups,qual,X_qual = get_only_stress_no_stress(X,groups,y,qual,X_qual)\n",
    "print(X.shape,X_qual.shape,len(y),len(y[y==1]),len(groups),len(qual),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 folds for each of 144 candidates, totalling 4608 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.856):\n",
      "{'svc__C': 1000, 'svc__class_weight': {0: 0.35, 1: 0.65}, 'svc__gamma': 0.015625, 'svc__kernel': 'rbf', 'svc__probability': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "delta = 0.1\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "paramGrid = {'svc__kernel': ['rbf'],\n",
    "             'svc__C': [11,100,1000],\n",
    "             'svc__gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "             'svc__class_weight': [{0: w, 1: 1 - w} for w in [.2,.265,.3,.35]],\n",
    "             'svc__probability':[False]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('svc', SVC())])\n",
    "# clf = make_pipeline(SMOTE(),SVC())\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='accuracy',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2339  190]\n",
      " [ 277  447]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      2529\n",
      "           1       0.70      0.62      0.66       724\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      3253\n",
      "   macro avg       0.80      0.77      0.78      3253\n",
      "weighted avg       0.85      0.86      0.85      3253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 folds for each of 96 candidates, totalling 3072 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  32 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=20)]: Done  77 out of  96 | elapsed:  6.0min remaining:  1.5min\n",
      "[Parallel(n_jobs=20)]: Done  96 out of  96 | elapsed:  9.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=11, cache_size=2000, class_weight={0: 0.25, 1: 0.75}, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0078125, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': [11,1000],\n",
    "              'gamma': [np.power(2,np.float(x)) for x in np.arange(-8, -2, .5)],\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in [.25,.265,.26,.35]],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X,y,groups=groups)),\\\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=5, iid=False)\n",
    "grid_search.fit(X,y)\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6693069306930693 0.2635087386250891\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      2529\n",
      "           1       0.64      0.70      0.67       724\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      3253\n",
      "   macro avg       0.78      0.79      0.78      3253\n",
      "weighted avg       0.85      0.85      0.85      3253\n",
      "\n",
      "[[2245  284]\n",
      " [ 217  507]]\n",
      "0.6274509803921569\n"
     ]
    }
   ],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X, y, gkf.split(X,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))\n",
    "# for grp in np.unique(groups):\n",
    "#     index = np.where(groups==grp)[0]\n",
    "#     print(Counter(y[index])[1]/Counter(y[index])[0],grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_weighted.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
