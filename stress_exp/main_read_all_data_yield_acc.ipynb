{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "directory_left = '../../cc3/rice_data/ecg_ppg_5_left_final_v1/'\n",
    "directory_right = '../../cc3/rice_data/ecg_ppg_5_right_final_v1/'\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['rr_col']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:np.array([np.squeeze(b).reshape(-1) for b in a]).reshape(-1,4))\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[a[:,0].argsort()])\n",
    "    data_day['length'] = data_day['rr_col'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>=10]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[:,1:])\n",
    "    return data_day[['user','day','rr_col','ecg_rr_array']]\n",
    "\n",
    "\n",
    "def get_all_data(data,hand='left'):\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_day_data)(a) for a in data_all)\n",
    "#     final_output = [parse_day_data(a) for a in data_all]\n",
    "    final_output = pd.concat([a for a in final_output if a.shape[0]>0])\n",
    "    final_output['hand'] = hand\n",
    "    print(final_output.shape)\n",
    "    return final_output\n",
    "\n",
    "def parse_each_participant(directory_left,directory_right,d):\n",
    "    try:\n",
    "        left_data = get_all_data(pickle.load(open(directory_left+d,'rb')).reset_index(drop=True),'left')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        left_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_left = pd.DataFrame([],columns=['a','b'])\n",
    "    try:\n",
    "        right_data = get_all_data(pickle.load(open(directory_right+d,'rb')).reset_index(drop=True),'right')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        right_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_right = pd.DataFrame([],columns=['a','b'])\n",
    "    t = [a for a in [left_data,right_data] if a.shape[0]>0]\n",
    "    if len(t)==0:\n",
    "        return pd.DataFrame([])\n",
    "    data = pd.concat(t)\n",
    "    print(data.shape)\n",
    "    if data.shape[0]>0:\n",
    "        return data\n",
    "    return pd.DataFrame([])\n",
    "all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right)) if d[-1]=='p')\n",
    "# all_data = [parse_each_participant(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right))[:2] if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [a for a in all_data if a.shape[0]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user_day_hand'] = data.apply(lambda a:a['user']+a['day']+a['hand'],axis=1)\n",
    "data['user_day'] = data.apply(lambda a:a['user']+a['day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['sz'] = data['rr_col'].apply(lambda a:len(a))\n",
    "# data = data[data.sz>=10]\n",
    "data['likelihood_max1'] = data['rr_col'].apply(lambda a:np.percentile(a[:,1],90))\n",
    "# data['likelihood_max'] = data['rr_col'].apply(lambda a: np.percentile(a,100*(1-30/len(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data,open('../../cc3/rice_data/temp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62,), (808,), (1253,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('../../cc3/rice_data/temp.p','rb'))\n",
    "\n",
    "import numpy as np\n",
    "def get_data(df):\n",
    "#     likelihoods = df.likelihood_max1.values\n",
    "#     if len(np.where(likelihoods>.3)[0])<30:\n",
    "#         return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "final_data = data\n",
    "\n",
    "import pandas as pd\n",
    "def get_data1(df):\n",
    "    if df.day.unique().shape[0]<10:\n",
    "        return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "final_data = final_data.reset_index(drop=True)\n",
    "final_final_data = final_data.groupby(['user'],as_index=False).apply(get_data1).reset_index(drop=True)\n",
    "final_final_data['len'] = final_final_data['rr_col'].apply(lambda a:a.shape[0])\n",
    "final_final_data = final_final_data[final_final_data.len>=30]\n",
    "\n",
    "def fix_rr(a):\n",
    "    a[a[:,0]==0,1] = 0\n",
    "    return a\n",
    "final_final_data['rr_col'] = final_final_data['rr_col'].apply(lambda a:fix_rr(a))\n",
    "final_final_data.user.unique().shape,final_final_data.user_day.unique().shape,final_final_data.user_day_hand.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel,delayed\n",
    "def get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30):\n",
    "    temp = deepcopy(final_final_data)\n",
    "    temp['rr_col'] = temp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<threshold_acl))[0]])\n",
    "    temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "    tmp = temp[temp.len>=minimum]\n",
    "    tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "    tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "    tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "    tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "    tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "    return [threshold_qual,threshold_acl,tmp.shape[0]/tmp.user_day_hand.unique().shape[0],mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80'].values),\n",
    "            mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80_weighted'].values),tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0],tmp.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_only_qual = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in np.logspace(-2.9,-.2,10):\n",
    "        try:\n",
    "            d = get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30)\n",
    "        except:\n",
    "            continue\n",
    "        all_results_only_qual.append(d)\n",
    "        data = pd.DataFrame(all_results_only_qual,columns=['Minimum Quality','Maximum Activity','Minutes per day','Original Error','Weighted Error','ECG Minutes per day','All Minutes'])\n",
    "        print(d)\n",
    "        pickle.dump(data,open('final_yield_accuracy_error_final.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel,delayed\n",
    "minimum=30\n",
    "final_final_data['len'] = final_final_data['rr_col'].apply(lambda a:a.shape[0])\n",
    "final_final_data = final_final_data[final_final_data.len>=30]\n",
    "temp = deepcopy(final_final_data)\n",
    "temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,1],100*(1-minimum/a.shape[0])))\n",
    "temp['activity_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,2],100*(minimum/a.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['ecg_80'] = temp['ecg_rr_array'].apply(lambda a:np.percentile(a,80) if isinstance(a,list) and len(a)>=30 else np.nan)\n",
    "temp['ecg_len'] =  temp['ecg_rr_array'].apply(lambda a:len(a) if isinstance(a,list) and len(a)>=30 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,0]>0])\n",
    "temp['ecg_80'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in [3]:\n",
    "        try:\n",
    "            tmp = deepcopy(temp)\n",
    "            tmp = tmp[(tmp.likelihood_max>=threshold_qual) & (tmp.activity_max<=threshold_acl)]\n",
    "            tmp['rr_col'] = tmp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<threshold_acl))[0]])\n",
    "            tmp1 = tmp[['ecg_80','ecg_len','rr_col','user_day_hand']].dropna()\n",
    "            tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "            tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "            tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "            yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "            print(threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean())\n",
    "            all_results.append([threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean(),tmp.shape[0]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            all_results.append([threshold_qual,threshold_acl,-1,-1,0])\n",
    "#     pickle.dump(all_results,open('tmp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "cannot do a non-empty take from an empty axes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-11-127217c140a4>\", line 15, in get_yield_error\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/pandas/core/series.py\", line 3591, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"pandas/_libs/lib.pyx\", line 2217, in pandas._libs.lib.map_infer\n  File \"<ipython-input-11-127217c140a4>\", line 15, in <lambda>\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 3707, in percentile\n    a, q, axis, out, overwrite_input, interpolation, keepdims)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 3826, in _quantile_unchecked\n    interpolation=interpolation)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 3405, in _ureduce\n    r = func(a, **kwargs)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 3941, in _quantile_ureduce_func\n    x1 = take(ap, indices_below, axis=axis) * weights_below\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 189, in take\n    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\n  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 56, in _wrapfunc\n    return getattr(obj, method)(*args, **kwds)\nIndexError: cannot do a non-empty take from an empty axes.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-127217c140a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     return [threshold_qual,threshold_acl,tmp.shape[0]/tmp.user_day_hand.unique().shape[0],mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80'].values),\n\u001b[1;32m     19\u001b[0m             mean_absolute_error(tmp1['ecg_80'].values,tmp1['ppg_80_weighted'].values),tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0],tmp.shape[0]]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_yield_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold_acl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold_qual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold_acl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold_qual\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: cannot do a non-empty take from an empty axes."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_only_qual = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in np.logspace(-2.9,-.2,10):\n",
    "        try:\n",
    "            d = get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30)\n",
    "        except:\n",
    "            continue\n",
    "        all_results_only_qual.append(d)\n",
    "        data = pd.DataFrame(all_results_only_qual,columns=['Minimum Quality','Maximum Activity','Minutes per day','Original Error','Weighted Error','ECG Minutes per day','All Minutes'])\n",
    "        print(d)\n",
    "        pickle.dump(data,open('final_yield_accuracy_error.p','wb'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('final_yield_accuracy_error.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minimum Quality</th>\n",
       "      <th>Maximum Activity</th>\n",
       "      <th>Minutes per day</th>\n",
       "      <th>Original Error</th>\n",
       "      <th>Weighted Error</th>\n",
       "      <th>ECG Minutes per day</th>\n",
       "      <th>All Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>221.749229</td>\n",
       "      <td>193.563020</td>\n",
       "      <td>194.990591</td>\n",
       "      <td>21.664336</td>\n",
       "      <td>215762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>228.542247</td>\n",
       "      <td>129.801501</td>\n",
       "      <td>132.121630</td>\n",
       "      <td>22.177632</td>\n",
       "      <td>246140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>228.831046</td>\n",
       "      <td>84.655319</td>\n",
       "      <td>86.377262</td>\n",
       "      <td>29.250522</td>\n",
       "      <td>266817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>241.184846</td>\n",
       "      <td>69.997279</td>\n",
       "      <td>71.649696</td>\n",
       "      <td>41.405405</td>\n",
       "      <td>289663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>255.358075</td>\n",
       "      <td>65.359392</td>\n",
       "      <td>66.806710</td>\n",
       "      <td>52.852853</td>\n",
       "      <td>313069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Minimum Quality  Maximum Activity  Minutes per day  Original Error  \\\n",
       "0              0.0          0.001259       221.749229      193.563020   \n",
       "1              0.0          0.001746       228.542247      129.801501   \n",
       "2              0.0          0.002422       228.831046       84.655319   \n",
       "3              0.0          0.003360       241.184846       69.997279   \n",
       "4              0.0          0.004660       255.358075       65.359392   \n",
       "\n",
       "   Weighted Error  ECG Minutes per day  All Minutes  \n",
       "0      194.990591            21.664336       215762  \n",
       "1      132.121630            22.177632       246140  \n",
       "2       86.377262            29.250522       266817  \n",
       "3       71.649696            41.405405       289663  \n",
       "4       66.806710            52.852853       313069  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "results = []\n",
    "for minimum  in [20,30,40]:\n",
    "    x = np.arange(.1,.3,.02)\n",
    "#     x = np.arange(0,.95,.05)\n",
    "    y = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,2]<threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1])\n",
    "    results.append(np.vstack([x,y]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "results = []\n",
    "for minimum  in [30]:\n",
    "    x = np.arange(.002,.2,.005)\n",
    "#     x = np.arange(0,.5,.05)\n",
    "    y = []\n",
    "    means1 = []\n",
    "    stds1 = []\n",
    "    means2 = []\n",
    "    stds2 = []\n",
    "    shapes = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,2]<threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "        tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "        tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "        tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "        tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "        tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "        tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "        tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "        tmp1['diff_weighted'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80_weighted'])\n",
    "        means1.append(tmp1['diff_original'].mean())\n",
    "        means2.append(tmp1['diff_weighted'].mean())\n",
    "        stds1.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80']))\n",
    "        stds2.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80_weighted']))\n",
    "        shapes.append(tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0])\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]//tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1],means1[-1],means2[-1],stds1[-1],stds2[-1],shapes[-1])\n",
    "    results.append(np.vstack([x,y,means1,stds1,means2,stds2,shapes]).T)\n",
    "    print(list(zip(x,y)))\n",
    "    \n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "results = []\n",
    "for minimum  in [30]:\n",
    "#     x = np.arange(.1,.3,.02)\n",
    "    x = np.arange(0,.5,.05)\n",
    "    y = []\n",
    "    means1 = []\n",
    "    stds1 = []\n",
    "    means2 = []\n",
    "    stds2 = []\n",
    "    shapes = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,1]>=threshold])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[temp.len>=minimum]\n",
    "        tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "        tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "        tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "        tmp1['ecg_80'] = tmp1['ecg_rr_array'].apply(lambda a:np.percentile(a,80))\n",
    "        tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "        tmp1['ppg_80_weighted'] = tmp1['rr_col'].apply(lambda a:np.percentile(np.repeat(a[:,0],np.int64(np.round(100*a[:,1]))),80))\n",
    "        tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "        tmp1['diff_weighted'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80_weighted'])\n",
    "        means1.append(tmp1['diff_original'].mean())\n",
    "        means2.append(tmp1['diff_weighted'].mean())\n",
    "        stds1.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80']))\n",
    "        stds2.append(r2_score(tmp1['ecg_80'],tmp1['ppg_80_weighted']))\n",
    "        shapes.append(tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0])\n",
    "#         temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a,100*(1-minimum/len(a))))\n",
    "#         tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1],means1[-1],means2[-1],stds1[-1],stds2[-1],shapes[-1])\n",
    "    results.append(np.vstack([x,y,means1,stds1,means2,stds2,shapes]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_20 = results[0]\n",
    "result_30 = results[1]\n",
    "result_40 = results[2]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(result_20[:,0],result_20[:,1],'-',label='p = 33.33%',linewidth=3)\n",
    "plt.plot(result_30[:,0],result_30[:,1],':',label='p = 50%',linewidth=3)\n",
    "plt.plot(result_40[:,0],result_40[:,1],'-.',label='p = 66.66%',linewidth=3)\n",
    "# plt.vlines(.05,0,585)\n",
    "# plt.vlines(.1,0,408)\n",
    "# plt.vlines(.15,0,306)\n",
    "# plt.vlines(.2,0,246)\n",
    "plt.xlabel('Minimum Signal Quality Threshold')\n",
    "plt.ylabel('Yield in Field \\n Minutes per participant-wrist day ')\n",
    "plt.xticks(np.arange(0,.95,.05),np.round(np.arange(0,.95,.05)*100)/100,rotation=60)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylim([0,700])\n",
    "ax = plt.gca()\n",
    "axy = ax.twinx()\n",
    "axy.plot(want[:,0],want[:,1],'--',c='r',marker='o',linewidth=2,markersize=10,label='LOSO F1 - Using Quality Weighted Features')\n",
    "axy.plot(not_want[:,0],not_want[:,1],'--',c='lime',marker='s',linewidth=2,markersize=10,label='LOSO F1 - Using Original Features')\n",
    "axy.set_ylabel('Lab Stress Classification Results \\n LOSO F1 Score',color='black')\n",
    "axy.tick_params(axis='y', colors='brown')\n",
    "axy.legend(loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "# want,not_1ant\n",
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))\n",
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_30 = result_30[result_30[:,0]<=.5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_want = np.hstack((want, result_30))\n",
    "all_data_not_want = np.hstack((not_want, result_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(all_data_want[:,-1],all_data_want[:,1],'--',marker='o',linewidth=3,markersize=10,label='Using Quality Weighted Features')\n",
    "plt.plot(all_data_not_want[:,-1],all_data_not_want[:,1],':',marker='s',linewidth=3,markersize=10,label='Using Original Features')\n",
    "plt.hlines(.675,0,263.35,color='black')\n",
    "plt.vlines(263.35,0,.675,linestyle='--',color='b')\n",
    "plt.vlines(192,0,.675,linestyle='--',color='b')\n",
    "plt.ylim([.63,.77])\n",
    "plt.xlim([80,700])\n",
    "plt.legend()\n",
    "plt.xlabel('Minutes per participant-wrist day ')\n",
    "plt.ylabel('Leave one subject F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_not_want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])\n",
    "final = []\n",
    "for i in range(11):\n",
    "    final.append([want[i][0],want[i][1],'Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "for i in range(11):\n",
    "    final.append([not_want[i][0],not_want[i][1],'Original Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "df = pd.DataFrame(final,columns=['Quality Threshold','F1 Score','Type of Features'])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.lineplot(x='Quality Threshold',y='F1 Score',hue='Type of Features',data=df, dashes=True)\n",
    "plt.plot(want[:,0],want[:,1],'--',marker='o',linewidth=3,markersize=20,label='Using Quality Weighted Features')\n",
    "plt.plot(not_want[:,0],not_want[:,1],'--',marker='s',linewidth=3,markersize=20,label='Using Normal Features')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Minimum Signal Quality')\n",
    "plt.legend()\n",
    "# plt.xticks(want[:,0],want[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[.1,.2,.1,.3,.4,.5,.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
