{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "directory_left = '../../cc3/rice_data/ecg_ppg_5_left_final/'\n",
    "directory_right = '../../cc3/rice_data/ecg_ppg_5_right_final/'\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "#     data_day['rr_col'] = data_day['rr_col'].apply(lambda a:np.array([np.squeeze(b).reshape(-1) for b in a]).reshape(-1,4))\n",
    "#     data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[a[:,0].argsort()])\n",
    "#     data_day['length'] = data_day['rr_col'].apply(lambda a:a.shape[0])\n",
    "#     data_day = data_day[data_day.length>=10]\n",
    "#     if data_day.shape[0]<30:\n",
    "#         return pd.DataFrame([],columns=data_day.columns)\n",
    "#     data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[:,1:])\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>=10]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day[['user','day','rr_col','ecg_rr_array']]\n",
    "\n",
    "\n",
    "def get_all_data(data,hand='left'):\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_day_data)(a) for a in data_all)\n",
    "#     final_output = [parse_day_data(a) for a in data_all]\n",
    "    final_output = pd.concat([a for a in final_output if a.shape[0]>0])\n",
    "    final_output['hand'] = hand\n",
    "    print(final_output.shape)\n",
    "    return final_output\n",
    "\n",
    "def parse_each_participant(directory_left,directory_right,d):\n",
    "    if d in os.listdir(directory_left):\n",
    "        left_data = get_all_data(pickle.load(open(directory_left+d,'rb')).reset_index(drop=True),'left')\n",
    "    else:\n",
    "        left_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_left = pd.DataFrame([],columns=['a','b'])\n",
    "    if d in os.listdir(directory_right):\n",
    "        right_data = get_all_data(pickle.load(open(directory_right+d,'rb')).reset_index(drop=True),'right')\n",
    "    else:\n",
    "        right_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_right = pd.DataFrame([],columns=['a','b'])\n",
    "    t = [a for a in [left_data,right_data] if a.shape[0]>0]\n",
    "    if len(t)==0:\n",
    "        return pd.DataFrame([])\n",
    "    data = pd.concat(t)\n",
    "    print(data.shape)\n",
    "    if data.shape[0]>0:\n",
    "        return data\n",
    "    return pd.DataFrame([])\n",
    "\n",
    "all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right)) if d[-1]=='p')\n",
    "# all_data = [parse_each_participant(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right))[:2] if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [a for a in all_data if a.shape[0]>0]\n",
    "\n",
    "data = pd.concat(all_data)\n",
    "\n",
    "data.shape\n",
    "\n",
    "data['user_day_hand'] = data.apply(lambda a:a['user']+a['day']+a['hand'],axis=1)\n",
    "data['user_day'] = data.apply(lambda a:a['user']+a['day'],axis=1)\n",
    "\n",
    "# data['sz'] = data['rr_col'].apply(lambda a:len(a))\n",
    "# data = data[data.sz>=10]\n",
    "data['likelihood_max1'] = data['rr_col'].apply(lambda a:np.percentile(a[:,1],90))\n",
    "# data['likelihood_max'] = data['rr_col'].apply(lambda a: np.percentile(a,100*(1-30/len(a))))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data,open('../../cc3/rice_data/temp1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# user_day_hands = pickle.load(open('tmp_user.p','rb'))\n",
    "# import pickle\n",
    "data = pickle.load(open('../../cc3/rice_data/temp1.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  48 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=40)]: Done 208 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=40)]: Done 432 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=40)]: Done 720 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=40)]: Done 1072 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=40)]: Done 1488 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=40)]: Done 1968 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=40)]: Done 2065 out of 2065 | elapsed:   39.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((48,), (594,), (1028,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data[data.user_day_hand.isin(user_day_hands)]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel,delayed\n",
    "def get_data(df):\n",
    "    likelihoods = df.likelihood_max1.values\n",
    "    if len(np.where(likelihoods>.5)[0])<60:\n",
    "        return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "final_data = pd.concat(Parallel(n_jobs=40,verbose=3)(delayed(get_data)(df) for i,df in data.groupby('user_day_hand',as_index=False)))\n",
    "# final_data = data\n",
    "import pandas as pd\n",
    "def get_data1(df):\n",
    "    if df.day.unique().shape[0]<10:\n",
    "        return pd.DataFrame([],columns=df.columns)\n",
    "    return df\n",
    "\n",
    "final_data = final_data.reset_index(drop=True)\n",
    "final_final_data = final_data.groupby(['user'],as_index=False).apply(get_data1).reset_index(drop=True)\n",
    "\n",
    "def fix_rr(a):\n",
    "    return a[(a[:,0]>0)&(a[:,0]>400)&(a[:,0]<1200),:]\n",
    "#     a[a[:,2]>10^-3,1] = 0\n",
    "#     return a[(a[:,0]>400)&(a[:,0]<1200)]\n",
    "#     return a[np.where(a[:,0]>0)[0],:]\n",
    "final_final_data['rr_col'] = final_final_data['rr_col'].apply(lambda a:fix_rr(a))\n",
    "final_final_data['len'] = final_final_data['rr_col'].apply(lambda a:a.shape[0])\n",
    "final_final_data = final_final_data[final_final_data.len>=10]\n",
    "final_final_data['len1'] = final_final_data['rr_col'].apply(lambda a:np.std(a[:,2]))\n",
    "final_final_data = final_final_data[final_final_data.len1>=0.001174897554939529]\n",
    "final_final_data.user.unique().shape,final_final_data.user_day.unique().shape,final_final_data.user_day_hand.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20190722', '20190723', '20190724', '20190725', '20190726',\n",
       "       '20190727', '20190729', '20190730', '20190731', '20190801',\n",
       "       '20190802', '20190803', '20190926', '20190927', '20190928',\n",
       "       '20190929', '20190930', '20191001', '20191003', '20191004',\n",
       "       '20191006', '20191008', '20191101', '20191102', '20191103',\n",
       "       '20191104', '20191105', '20191106', '20191107', '20191108',\n",
       "       '20191109', '20191110', '20191111', '20191112', '20191113',\n",
       "       '20191114', '20190603', '20190604', '20190605', '20190606',\n",
       "       '20190607', '20190608', '20190609', '20190610', '20190611',\n",
       "       '20190612', '20190613', '20190614', '20190701', '20190702',\n",
       "       '20190703', '20190704', '20190705', '20190709', '20190710',\n",
       "       '20190713', '20190716', '20190717', '20190706', '20190707',\n",
       "       '20190708', '20190711', '20190712', '20190714', '20190715',\n",
       "       '20190718', '20190615', '20190616', '20190617', '20190618',\n",
       "       '20190619', '20190620', '20190804', '20190805', '20190807',\n",
       "       '20190808', '20190809', '20190811', '20190812', '20190531',\n",
       "       '20190601', '20190621', '20190622', '20190623', '20190624',\n",
       "       '20190625', '20190626', '20190627', '20190628', '20190520',\n",
       "       '20190521', '20190522', '20190523', '20190524', '20190525',\n",
       "       '20190526', '20190527', '20190528', '20190529', '20190629',\n",
       "       '20190630', '20190530', '20190917', '20190918', '20190919',\n",
       "       '20190921', '20190922', '20190923', '20190924', '20190925',\n",
       "       '20190719', '20190721', '20190728', '20190806', '20190810',\n",
       "       '20190813', '20190814', '20190815', '20190913', '20190914',\n",
       "       '20190915', '20190916', '20190920', '20190602', '20190912',\n",
       "       '20190816', '20190817', '20190819', '20190821', '20190822',\n",
       "       '20190412', '20190413', '20190415', '20190416', '20190417',\n",
       "       '20190418', '20190419', '20190420', '20190422', '20190423',\n",
       "       '20190425', '20191002', '20191005', '20191007', '20191010',\n",
       "       '20190818', '20190820', '20190823', '20190824', '20190825',\n",
       "       '20190826', '20190829', '20190830', '20190831', '20190901',\n",
       "       '20190902', '20190903', '20190904', '20190905', '20190907',\n",
       "       '20190908', '20190909', '20190911', '20190827', '20190828',\n",
       "       '20191115', '20191116', '20191117', '20191118', '20191119',\n",
       "       '20191120', '20191009', '20191011', '20191013', '20191014',\n",
       "       '20191015', '20191016', '20191017', '20190906', '20190910'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle.dump(final_final_data.user_day_hand.unique(),open('user_day_hand.p','wb'))\n",
    "# final_final_data.head()\n",
    "final_final_data.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "feature_names = ['var','iqr','std','rmssd','mean','median','80','20','heart rate']\n",
    "# feature_names = ['mean','std']\n",
    "def get_features(a):\n",
    "    m,s = weighted_avg_and_std(a[:,0], a[:,1])\n",
    "    a = a[:,0]\n",
    "    return np.array([np.percentile(a,70)-np.percentile(a,30),iqr(a),np.std(a),np.sqrt(np.mean(np.diff(a)** 2)),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "# def get_features(a):\n",
    "#     return np.array([np.mean(a),iqr(a)])\n",
    "\n",
    "def get_features_ecg(a):\n",
    "#     a = a[:,0]\n",
    "    return np.array([np.percentile(a,70)-np.percentile(a,30),iqr(a),np.std(a),np.sqrt(np.mean(np.diff(a)** 2)),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)*2, weights=weights)\n",
    "    return average, variance\n",
    "\n",
    "\n",
    "def weighted_percentile(data, weights, perc):\n",
    "    \"\"\"\n",
    "    perc : percentile in [0-1]!\n",
    "    \"\"\"\n",
    "    ix = np.argsort(data)\n",
    "    data = data[ix] # sort data\n",
    "    weights = weights[ix] # sort weights\n",
    "    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) # 'like' a CDF function\n",
    "    return np.interp(perc, cdf, data)\n",
    "\n",
    "from copy import deepcopy\n",
    "def get_weighted_rr_features(a,weight=True):\n",
    "    m,s = weighted_avg_and_std(deepcopy(a[:,0]), deepcopy(a[:,1]))\n",
    "    p75 = weighted_percentile(deepcopy(a[:,0]),deepcopy(a[:,1]),.75)\n",
    "    p25 = weighted_percentile(deepcopy(a[:,0]),deepcopy(a[:,1]),.25)\n",
    "    p50 = weighted_percentile(deepcopy(a[:,0]),deepcopy(a[:,1]),.5)\n",
    "    p80 = weighted_percentile(deepcopy(a[:,0]),deepcopy(a[:,1]),.80)\n",
    "    p20 = weighted_percentile(deepcopy(a[:,0]),deepcopy(a[:,1]),.20)\n",
    "    return np.array([s,p75-p25,\n",
    "                     np.sqrt(s),np.sqrt(np.average(np.diff(a[:,0])** 2,weights=a[1:,1])),\n",
    "                     m,p50,p80,p20,\n",
    "                     60000/p50])\n",
    "\n",
    "\n",
    "def get_yield_error1(final_final_data):\n",
    "    minimum=30\n",
    "    all_results = []\n",
    "    for threshold_qual in np.arange(0,.95,.05):\n",
    "        for threshold_acl in [300]:\n",
    "            temp = deepcopy(final_final_data)\n",
    "            temp['rr_col'] = temp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<=threshold_acl))[0]])\n",
    "            temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "            tmp = temp[temp.len>=minimum]\n",
    "            if tmp.shape[0]<50:\n",
    "                continue\n",
    "#             tmp['minn'] = tmp['rr_col'].apply(lambda a:np.percentile(a[:,1],80))\n",
    "#             tmp = tmp[tmp.minn>=.5]\n",
    "#             if tmp.shape[0]<50:\n",
    "#                 continue\n",
    "            tmp1 = tmp[['ecg_rr_array','rr_col','user_day_hand']].dropna()\n",
    "            tmp1['ecg_len'] =  tmp1['ecg_rr_array'].apply(lambda a:len(a))\n",
    "            tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "            if tmp1.shape[0]<50:\n",
    "                continue\n",
    "            tmp1['ecg_features'] = tmp1['ecg_rr_array'].apply(lambda a:get_features_ecg(a))\n",
    "            tmp1['ppg_features'] = tmp1['rr_col'].apply(lambda a:get_features(a))\n",
    "            corrs = []\n",
    "            for i,name in enumerate(feature_names):\n",
    "                tmp1['ecg_'+name] = tmp1['ecg_features'].apply(lambda a:a[i])\n",
    "                tmp1['ppg_'+name] = tmp1['ppg_features'].apply(lambda a:a[i])\n",
    "#                 print(tmp1['ppg_'+name])\n",
    "                values = tmp1[['ecg_'+name,'ppg_'+name]].dropna().values\n",
    "                corrs.append(pearsonr(values[:,0],values[:,1])[0])\n",
    "#             if np.mean(corrs)<.1:\n",
    "#                 return pd.DataFrame([],columns=['Minimum Quality','Maximum Activity','Minutes per day']+feature_names+['ECG Minutes per day','All Minutes'])\n",
    "            d1 = [threshold_qual,threshold_acl,tmp.shape[0]/tmp.user_day_hand.unique().shape[0]]\n",
    "            d2 = corrs\n",
    "            d3 = [tmp1.shape[0]/tmp1.user_day_hand.unique().shape[0],tmp.shape[0]]\n",
    "            d = d1+d2+d3\n",
    "            all_results.append(d)\n",
    "    return pd.DataFrame(all_results,columns=['Minimum Quality','Maximum Activity','Minutes per day']+feature_names+['ECG Minutes per day','All Minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = Parallel(n_jobs=40,verbose=2)(delayed(get_yield_error1)(df) for i,df in final_final_data.groupby(['user'],as_index=False))\n",
    "# data = [get_yield_error1(df) for i,df in final_final_data.groupby('user',as_index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.concat(data2)\n",
    "data1['All Minutes'] = data1['All Minutes'].apply(lambda a:np.int64(a))\n",
    "data1['Minutes per day'] = data1['Minutes per day'].apply(lambda a:np.int64(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data1,open('all_results_yield.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activity = data1[data1['Minimum Quality']==0].groupby('Maximum Activity').mean().reset_index()\n",
    "likelihood = data1.groupby('Minimum Quality').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "feature_names = ['80','heart rate','20','median','mean']\n",
    "feature_names2 = ['80th Percentile','Heart Rate','20th Percentile','Median','Mean']\n",
    "for j,name in enumerate(feature_names):\n",
    "    for i,row in data1.iterrows():\n",
    "        all_data.append([row['Minimum Quality'],feature_names2[j],row[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corr = pd.DataFrame(all_data,columns=['Minimum Quality Threshold(5 secs)','Minute Level Feature','Pearson Correlation with ECG Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(style='dark')\n",
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.lineplot(x='Minimum Quality Threshold(5 secs)',y='Pearson Correlation with ECG Features',hue='Minute Level Feature',\n",
    "             data=final_corr,err_style=\"bars\",palette='dark',linewidth=4)\n",
    "plt.legend(ncol=3,loc='upper left')\n",
    "plt.ylim([0.06,1.1])\n",
    "# plt.savefig('features1.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(style='dark')\n",
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.lineplot(x='Minimum Quality Threshold(5 secs)',y='Pearson Correlation with ECG Features',hue='Minute Level Feature',\n",
    "             data=final_corr,err_style=\"bars\",palette='dark',linewidth=4)\n",
    "plt.legend(ncol=3,loc='upper left')\n",
    "plt.ylim([0.06,1.1])\n",
    "# plt.savefig('features1.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_act = activity['Maximum Activity'].values\n",
    "y_act = activity['Minutes per day'].values\n",
    "x_like = likelihood['Minimum Quality'].values\n",
    "y_like = likelihood['Minutes per day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([x_act,y_act,x_like,y_like],open('../code_stress/thresholds.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_like20 = likelihood1['Minimum Quality'].values\n",
    "y_like20 = likelihood1['Minutes per day'].values\n",
    "x_like30 = likelihood2['Minimum Quality'].values\n",
    "y_like30 = likelihood2['Minutes per day'].values\n",
    "x_like40 = likelihood3['Minimum Quality'].values\n",
    "y_like40 = likelihood3['Minutes per day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([x_like20,y_like20,x_like30,y_like30,x_like40,y_like40],open('../code_stress/minutes_perday.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory_likelihood = '../code_stress/likelihood_threshold_lab_f12.p'\n",
    "directory_acl = '../code_stress/likelihood_threshold_lab_f1_not_weighted.p'\n",
    "results_likelihood = pickle.load(open(directory_likelihood,'rb'))\n",
    "results_likelihood_not_weighted = pickle.load(open(directory_acl,'rb'))\n",
    "x_like20,y_like20,x_like30,y_like30,x_like40,y_like40 = pickle.load(open('../code_stress/minutes_perday.p','rb'))\n",
    "\n",
    "def get_f1(results_likelihood,types='likelihood'):\n",
    "    thresholds = results_likelihood[0][:,:]\n",
    "    results = np.array([i[0][np.array([3,4,5])] for i in results_likelihood[1]])\n",
    "    if types=='likelihood':\n",
    "        thresholds[:,2] =  results[:,0]\n",
    "    else:\n",
    "        thresholds[:,0] = thresholds[:,2]\n",
    "        thresholds[:,2] =  results[:,0]\n",
    "    return thresholds\n",
    "\n",
    "data_likelihood = get_f1(results_likelihood,types='likelihood')\n",
    "data_likelihood_not_weighted = get_f1(results_likelihood_not_weighted,types='likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(x_like20,y_like20,'-',label='p = 33.33%',linewidth=3)\n",
    "plt.plot(x_like30,y_like30,':',label='p = 50%',linewidth=3)\n",
    "plt.plot(x_like40,y_like40,'-.',label='p = 66.66%',linewidth=3)\n",
    "# plt.vlines(.05,0,585)\n",
    "# plt.vlines(.1,0,408)\n",
    "# plt.vlines(.15,0,306)\n",
    "# plt.vlines(.2,0,246)\n",
    "plt.xlabel('Minimum Signal Quality Threshold')\n",
    "plt.ylabel('Yield in Field \\n Minutes per participant-wrist day ')\n",
    "plt.xticks(np.arange(0,.95,.05),np.round(np.arange(0,.95,.05)*100)/100,rotation=60)\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()\n",
    "plt.ylim([0,750])\n",
    "ax = plt.gca()\n",
    "axy = ax.twinx()\n",
    "axy.plot(data_likelihood[:-1,0],data_likelihood[:-1,2],'--',c='r',marker='o',linewidth=2,markersize=10,label='LOSO F1 - Using Quality Integrated Features')\n",
    "axy.plot(data_likelihood_not_weighted[:-1,0],data_likelihood_not_weighted[:-1,2],'--',c='lime',marker='s',linewidth=2,markersize=10,label='LOSO F1 - Not Using Quality Integration')\n",
    "axy.set_ylabel('Lab Stress Classification Results \\n LOSO F1 Score',color='black')\n",
    "axy.tick_params(axis='y', colors='brown')\n",
    "axy.set_ylim([.64,.80])\n",
    "axy.legend(loc='upper center')\n",
    "plt.savefig('yield_accuracy_tradeoff_final.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data1 = pickle.load(open('all_results_yield.p','rb'))\n",
    "results = data1.groupby(['Minimum Quality','Maximum Activity'],as_index=False).median().sort_values('Minimum Quality').reset_index(drop=True)\n",
    "\n",
    "results2_75 = data1.groupby(['Minimum Quality','Maximum Activity'],as_index=False).std().sort_values('Minimum Quality').reset_index(drop=True)\n",
    "\n",
    "results2_75.fillna(3,inplace=True)\n",
    "results2_75['Original Error'][results2_75['Original Error']<1] = 3\n",
    "\n",
    "results['show'] = [\"{:.0f}\".format(results['Original Error'].loc[i])+'$\\pm$'+\"{:.0f}\".format(results2_75['Original Error'].loc[i]) for i,row in results.iterrows()]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "a = pd.pivot_table(results,columns='Minimum Quality',index='Maximum Activity',values='show',aggfunc=lambda x:''.join(x))\n",
    "plt.figure(figsize=(30,12))\n",
    "plt.rcParams.update({'font.size':30})\n",
    "sns.heatmap(pd.pivot_table(results,columns='Minimum Quality',index='Maximum Activity',values='Original Error',aggfunc='median'),\n",
    "            annot=a,cmap=\"Reds\",fmt='',linewidths=1, linecolor='black',cbar=False)\n",
    "plt.xticks(np.array(range(len(np.arange(0,.65,.05))))+.5,[np.round(100*a)/100 for a in np.arange(0,.65,.05)],fontsize=40,rotation=60)\n",
    "plt.yticks(np.array(range(len(np.logspace(-2.9,-.2,10))))+.5,[str(np.round(10000*(a/2))/10000)+'g' for a in np.logspace(-2.9,-.2,10)],fontsize=40,rotation=0)\n",
    "plt.xlabel('Minimum Signal Quality Threshold (5 second)',fontsize=35)\n",
    "plt.ylabel('Maximum Allowaed Activity Variation',fontsize=35)\n",
    "# plt.title('Yield in Field \\n Minutes per day')\n",
    "plt.title('Mean Absolute Error of Heart Rate Estimation \\n (Ground truth Derived from ECG, Per minute, Unit = ms)',fontsize=35)\n",
    "# plt.savefig('yield_minutes_per_day.pdf',dps=1e6)\n",
    "plt.savefig('error_miliseconds.pdf',dps=1e6,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pickle.load(open('all_results_yield.p','rb'))\n",
    "results = data1.groupby(['Minimum Quality','Maximum Activity'],as_index=False).mean().sort_values('Minutes per day').reset_index(drop=True)\n",
    "\n",
    "results2_75 = data1.groupby(['Minimum Quality','Maximum Activity'],as_index=False).std().sort_values('Minimum Quality').reset_index(drop=True)\n",
    "\n",
    "# results2_75.fillna(3,inplace=True)\n",
    "# results2_75['Minutes per day'][results2_75['Minutes per day']<1] = 3\n",
    "\n",
    "results['show'] = [\"{:.0f}\".format(results['Minutes per day'].loc[i])+'$\\pm$'+\"{:.0f}\".format(results2_75['Minutes per day'].loc[i]) for i,row in results.iterrows()]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "a = pd.pivot_table(results,columns='Minimum Quality',index='Maximum Activity',values='show',aggfunc=lambda x:''.join(x))\n",
    "plt.figure(figsize=(30,12))\n",
    "plt.rcParams.update({'font.size':30})\n",
    "sns.heatmap(pd.pivot_table(results,columns='Minimum Quality',index='Maximum Activity',values='Original Error',aggfunc='median'),\n",
    "            annot=a,cmap=\"Reds\",fmt='',linewidths=1, linecolor='black',cbar=False)\n",
    "plt.xticks(np.array(range(len(np.arange(0,.65,.05))))+.5,[np.round(100*a)/100 for a in np.arange(0,.65,.05)],fontsize=40)\n",
    "plt.yticks(np.array(range(len(np.logspace(-2.9,-.2,10))))+.5,[str(np.round(10000*(a/2))/10000)+'g' for a in np.logspace(-2.9,-.2,10)],fontsize=40,rotation=0)\n",
    "plt.xlabel('Minimum Signal Quality Threshold (5 second)',fontsize=35)\n",
    "plt.ylabel('Maximum Allowaed Activity Variation',fontsize=35)\n",
    "# plt.title('Yield in Field \\n Minutes per day')\n",
    "plt.title('Mean Absolute Error of Heart Rate Estimation \\n (Ground truth Derived from ECG, Per minute, Unit = ms)',fontsize=35)\n",
    "# plt.savefig('yield_minutes_per_day.pdf',dps=1e6)\n",
    "# plt.savefig('error_miliseconds.pdf',dps=1e6,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.sort_values('Minimum Quality').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# data  = pickle.load(open('final_yield_accuracy_error_final2.p','rb'))\n",
    "from copy import deepcopy\n",
    "data2 = deepcopy(results)\n",
    "import pandas as pd\n",
    "a = pd.pivot_table(data2,columns='Minimum Quality',index='Maximum Activity',values='Original Error',aggfunc='mean')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(a,annot=True,fmt='.1f',xticklabels=1,yticklabels=1,cbar=False,linewidths=1, linecolor='black',cmap=\"Reds\")\n",
    "plt.xticks(np.array(range(len(np.arange(0,.6,.05))))+.5,[np.round(100*a)/100 for a in np.arange(0,.6,.05)],fontsize=30)\n",
    "plt.yticks(np.array(range(len(np.logspace(-2.9,-.2,10))))+.5,[str(np.round(10000*(a/2))/10000)+'g' for a in np.logspace(-2.9,-.2,10)],fontsize=30)\n",
    "plt.xlabel('Minimum Signal Quality Threshold')\n",
    "plt.ylabel('Maximum Allowaed Activity Variation')\n",
    "# plt.title('Yield in Field \\n Minutes per day')\n",
    "plt.title('Mean Absolute Error of Heart Rate Estimation \\n (Ground truth Derived from ECG, Per minute, Unit = BPM)')\n",
    "# plt.savefig('yield_minutes_per_day.pdf',dps=1e6)\n",
    "# plt.savefig('error_miliseconds3.pdf',dps=1e6,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# a = pd.pivot_table(data,columns='Minimum Quality',index='Maximum Activity',values='Original Error',aggfunc='mean')\n",
    "# plt.rcParams.update({'font.size':32})\n",
    "# plt.figure(figsize=(25,12))\n",
    "# sns.heatmap(a,annot=True,fmt='.0f',xticklabels=1,yticklabels=2,cbar=False,linewidths=1, linecolor='black')\n",
    "# plt.xticks(np.array(range(len(np.arange(0,.6,.05))))+.5,[np.round(100*a)/100 for a in np.arange(0,.6,.05)])\n",
    "# plt.yticks(np.array(range(len(np.logspace(-2.9,-.2,10))))+.5,[str(np.round(10000*(a/2))/10000)+'g' for a in np.logspace(-2.9,-.2,10)])\n",
    "# plt.xlabel('Minimum Signal Quality Threshold',fontsize=30)\n",
    "# plt.ylabel('Maximum Allowaed Activity Variation',fontsize=30)\n",
    "# plt.title('Mean Absolute Error with ECG derived 80th percentile \\n (miliseconds)')\n",
    "# plt.savefig('error_miliseconds.pdf',dps=1e6)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# data2['Minutes per day'] = 100*data2['Minutes per day']/data2['Minutes per day'].max()\n",
    "\n",
    "a = pd.pivot_table(data2,columns='Minimum Quality',index='Maximum Activity',values='Minutes per day',aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(a,annot=True,fmt='.1f',xticklabels=1,yticklabels=1,cbar=False,linewidths=1, linecolor='black',cmap=\"YlGnBu\")\n",
    "plt.xticks(np.array(range(len(np.arange(0,.6,.05))))+.5,[np.round(100*a)/100 for a in np.arange(0,.6,.05)])\n",
    "plt.yticks(np.array(range(len(np.logspace(-2.9,-.2,10))))+.5,[str(np.round(10000*(a/2))/10000)+'g' for a in np.logspace(-2.9,-.2,10)])\n",
    "plt.xlabel('Minimum Signal Quality Threshold',fontsize=30)\n",
    "plt.ylabel('Maximum Allowaed Activity Variation')\n",
    "plt.title('Yield in Field \\n Percentage Available')\n",
    "plt.savefig('yield_percentage.pdf',dps=1e6,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from joblib import Parallel,delayed\n",
    "minimum=30\n",
    "final_final_data['len'] = final_final_data['rr_col'].apply(lambda a:a.shape[0])\n",
    "final_final_data = final_final_data[final_final_data.len>=30]\n",
    "temp = deepcopy(final_final_data)\n",
    "temp['likelihood_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,1],100*(1-minimum/a.shape[0])))\n",
    "temp['activity_max'] = temp['rr_col'].apply(lambda a: np.percentile(a[:,2],100*(minimum/a.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['ecg_80'] = temp['ecg_rr_array'].apply(lambda a:np.percentile(a,80) if isinstance(a,list) and len(a)>=30 else np.nan)\n",
    "temp['ecg_len'] =  temp['ecg_rr_array'].apply(lambda a:len(a) if isinstance(a,list) and len(a)>=30 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,0]>0])\n",
    "temp['ecg_80'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in [3]:\n",
    "        try:\n",
    "            tmp = deepcopy(temp)\n",
    "            tmp = tmp[(tmp.likelihood_max>=threshold_qual) & (tmp.activity_max<=threshold_acl)]\n",
    "            tmp['rr_col'] = tmp['rr_col'].apply(lambda a:a[np.where((a[:,1]>=threshold_qual)&(a[:,2]<threshold_acl))[0]])\n",
    "            tmp1 = tmp[['ecg_80','ecg_len','rr_col','user_day_hand']].dropna()\n",
    "            tmp1 = tmp1[tmp1.ecg_len>=minimum]\n",
    "            tmp1['ppg_80'] = tmp1['rr_col'].apply(lambda a:np.percentile(a[:,0],80))\n",
    "            tmp1['diff_original'] = np.abs(tmp1['ecg_80']-tmp1['ppg_80'])\n",
    "            yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "            print(threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean())\n",
    "            all_results.append([threshold_qual,threshold_acl,yield_,tmp1['diff_original'].mean(),tmp.shape[0]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            all_results.append([threshold_qual,threshold_acl,-1,-1,0])\n",
    "#     pickle.dump(all_results,open('tmp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_only_qual = []\n",
    "for threshold_qual in np.arange(0,.6,.05):\n",
    "    for threshold_acl in np.logspace(-2.9,-.2,10):\n",
    "        try:\n",
    "            d = get_yield_error(final_final_data,threshold_acl,threshold_qual,minimum=30)\n",
    "        except:\n",
    "            continue\n",
    "        all_results_only_qual.append(d)\n",
    "        data = pd.DataFrame(all_results_only_qual,columns=['Minimum Quality','Maximum Activity','Minutes per day','Original Error','Weighted Error','ECG Minutes per day','All Minutes'])\n",
    "        print(d)\n",
    "        pickle.dump(data,open('final_yield_accuracy_error.p','wb'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('final_yield_accuracy_error.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = deepcopy(final_final_data)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "results = []\n",
    "def fix(a,minimum):\n",
    "    if a.shape[0]<minimum:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.percentile(a,100*(1-minimum/(len(a)+1)))\n",
    "for minimum  in [20,30,40]:\n",
    "#     x = np.arange(.1,.3,.02)\n",
    "    x = np.arange(0,.95,.05)\n",
    "    y = []\n",
    "    for threshold in x:\n",
    "        temp = deepcopy(final_final_data)\n",
    "        temp['rr_col'] = temp['rr_col'].apply(lambda a:a[a[:,1]>=threshold,1])\n",
    "        temp['len'] = temp['rr_col'].apply(lambda a:a.shape[0])\n",
    "        tmp = temp[(temp.len>minimum)]\n",
    "        temp['likelihood_max'] = temp['rr_col'].apply(lambda a: fix(a,minimum))\n",
    "        temp = temp[['likelihood_max','user_day_hand']].dropna()\n",
    "        tmp = temp[temp.likelihood_max>=threshold]\n",
    "        yield_ = tmp.shape[0]/tmp.user_day_hand.unique().shape[0]\n",
    "        y.append(yield_)\n",
    "        print(threshold,y[-1])\n",
    "    results.append(np.vstack([x,y]).T)\n",
    "    print(list(zip(x,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_20 = results[0]\n",
    "result_30 = results[1]\n",
    "result_40 = results[2]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(result_20[:,0],result_20[:,1],'-',label='p = 33.33%',linewidth=3)\n",
    "plt.plot(result_30[:,0],result_30[:,1],':',label='p = 50%',linewidth=3)\n",
    "plt.plot(result_40[:,0],result_40[:,1],'-.',label='p = 66.66%',linewidth=3)\n",
    "# plt.vlines(.05,0,585)\n",
    "# plt.vlines(.1,0,408)\n",
    "# plt.vlines(.15,0,306)\n",
    "# plt.vlines(.2,0,246)\n",
    "plt.xlabel('Minimum Signal Quality Threshold')\n",
    "plt.ylabel('Yield in Field \\n Minutes per participant-wrist day ')\n",
    "plt.xticks(np.arange(0,.95,.05),np.round(np.arange(0,.95,.05)*100)/100,rotation=60)\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()\n",
    "plt.ylim([0,750])\n",
    "ax = plt.gca()\n",
    "axy = ax.twinx()\n",
    "axy.plot(want[:,0],want[:,1],'--',c='r',marker='o',linewidth=2,markersize=10,label='LOSO F1 - Using Quality Weighted Features')\n",
    "axy.plot(not_want[:,0],not_want[:,1],'--',c='lime',marker='s',linewidth=2,markersize=10,label='LOSO F1 - Not Using Quality Weighting')\n",
    "axy.set_ylabel('Lab Stress Classification Results \\n LOSO F1 Score',color='black')\n",
    "axy.tick_params(axis='y', colors='brown')\n",
    "axy.set_ylim([.64,.79])\n",
    "axy.legend(loc='upper center')\n",
    "plt.savefig('yield_accuracy_tradeoff_final1.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "# want,not_1ant\n",
    "want = pickle.load(open('../data/stress_with_quality_without_normalization7.p','rb'))\n",
    "not_want = pickle.load(open('../data/stress_without_quality_without_normalization7.p','rb'))\n",
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_30 = result_30[result_30[:,0]<=.5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_want = np.hstack((want, result_30))\n",
    "all_data_not_want = np.hstack((not_want, result_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(all_data_want[:,-1],all_data_want[:,1],'--',marker='o',linewidth=3,markersize=10,label='Using Quality Weighted Features')\n",
    "plt.plot(all_data_not_want[:,-1],all_data_not_want[:,1],':',marker='s',linewidth=3,markersize=10,label='Using Original Features')\n",
    "plt.hlines(.675,0,243,color='black')\n",
    "plt.vlines(243,0,.675,linestyle='--',color='b')\n",
    "plt.vlines(182,0,.675,linestyle='--',color='b')\n",
    "plt.ylim([.63,.77])\n",
    "plt.xlim([80,700])\n",
    "plt.legend()\n",
    "plt.xlabel('Minutes per participant-wrist day ')\n",
    "plt.ylabel('Leave one subject F1 Score')\n",
    "plt.savefig('tradeoff_vertical.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_not_want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0,3,4,5]\n",
    "want =np.array([np.array(i[0])[index] for i in want])\n",
    "\n",
    "not_want = np.array([np.array(i[0])[index] for i in not_want])\n",
    "final = []\n",
    "for i in range(11):\n",
    "    final.append([want[i][0],want[i][1],'Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([want[i][0],want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "for i in range(11):\n",
    "    final.append([not_want[i][0],not_want[i][1],'Original Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][2],'Precision','Using Quality Weighted Features'])\n",
    "#     final.append([not_want[i][0],not_want[i][3],'Recall','Using Quality Weighted Features'])\n",
    "\n",
    "df = pd.DataFrame(final,columns=['Quality Threshold','F1 Score','Type of Features'])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\"font.size\":20})\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.lineplot(x='Quality Threshold',y='F1 Score',hue='Type of Features',data=df, dashes=True)\n",
    "plt.plot(want[:,0],want[:,1],'--',marker='o',linewidth=3,markersize=20,label='Using Quality Weighted Features')\n",
    "plt.plot(not_want[:,0],not_want[:,1],'--',marker='s',linewidth=3,markersize=20,label='Using Normal Features')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Minimum Signal Quality')\n",
    "plt.legend()\n",
    "# plt.xticks(want[:,0],want[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[.1,.2,.1,.3,.4,.5,.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.logspace(-2.90,-.2,10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
