{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10883, 20) 4bf6078d-afcd-432a-a8a5-8b5e8a4eda9e.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:    4.3s remaining:    7.8s\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:    4.8s remaining:    2.7s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9484, 31)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>20]\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+3*tt)&(heart_rate_window[:,0]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "    \n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,1]>.05)&(a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>30]\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_weighted_rr_features(a))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>20]\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:get_rr_features(smooth(a)))\n",
    "    return data_day\n",
    "\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag', 'ecg_rr_array_final', 'ecg_features']\n",
    "    ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "    a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "    a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "    if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "        a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "        a_ecg = a_ecg[ecg_columns]\n",
    "    a_ppg = parse_day_data(a)\n",
    "    a_ppg = parse_for_features(a_ppg)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = pd.merge(a_ppg, a_ecg, how='left',left_on=['window'],right_on=['window'])\n",
    "    if a_ppg.shape[0]<60:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    print(a_ppg.shape)\n",
    "    a_ppg = get_ecg_stress(a_ppg)\n",
    "    print(a_ppg.shape)\n",
    "    a_ppg = get_ppg_stress(a_ppg)\n",
    "    print(a_ppg.shape)\n",
    "    return a_ppg\n",
    "\n",
    "def get_ppg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    quals1 = np.array(list(a['quality_mag'].values))\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<10:\n",
    "        a['stress_likelihood_ppg'] = np.nan\n",
    "        return a\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a['stress_likelihood_ppg'] = probs\n",
    "    return a\n",
    "\n",
    "def get_ecg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    if len(feature_matrix)<10:\n",
    "        a['stress_likelihood_ecg'] = np.nan\n",
    "        return a\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a_ecg = a_ecg.drop(['ecg_features'],axis=1)\n",
    "    a = pd.merge(a, a_ecg, how='left', left_on=['window'], right_on=['window'])\n",
    "    return a\n",
    "    \n",
    "def parse_each_participant(directory,d):\n",
    "    data = pickle.load(open(directory+d,'rb')).reset_index(drop=True)\n",
    "    print(data.shape,d)\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return 0\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return 0\n",
    "    final_output = pd.concat(final_output)\n",
    "    final_output['stress_likelihood_ppg_qual'] = final_output['stress_likelihood_ppg']\n",
    "    print(final_output.shape)\n",
    "#     print(final_output.shape,final_output.columns)\n",
    "    pickle.dump([final_output,ema],open(directory1+d,'wb'))\n",
    "    return 0\n",
    "\n",
    "directory = '../../cc3/rice_data/ecg_ppg_25_left3/'\n",
    "directory1 = '../../cc3/rice_data/ecg_ppg_25_left5/'\n",
    "# all_data = Parallel(n_jobs=30,verbose=2)(delayed(parse_each_participant)(directory,d) for d in os.listdir(directory)[:2] if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory,d) for d in os.listdir(directory) if d=='4bf6078d-afcd-432a-a8a5-8b5e8a4eda9e.p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>20]\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+3*tt)&(heart_rate_window[:,0]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "    \n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,1]>.05)&(a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>30]\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_weighted_rr_features(a))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>20]\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:get_rr_features(a))\n",
    "    return data_day\n",
    "\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag','window', 'ecg_rr_array_final', 'ecg_features']\n",
    "    ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "    a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "    a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "    if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "        a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "        a_ecg = a_ecg[ecg_columns]\n",
    "    a_ppg = parse_day_data(a)\n",
    "    a_ppg = parse_for_features(a_ppg)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "#     a_ecg = a_ecg.rename({'window': 'window1'}, axis=1)\n",
    "    a_ppg = pd.merge(a_ppg, a_ecg, how='left', left_on=['window'],right_on=['window'],suffixes=('', '_7'))\n",
    "#     print(a_ppg.columns,'r')\n",
    "    return a_ppg[columns]\n",
    "\n",
    "def get_both_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    print(a_ecg.columns,'--'*20)\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a = a.drop(['ecg_features'],axis=1)\n",
    "#     print(a.columns,a_ecg.columns)\n",
    "#     a_ecg = a_ecg.rename({'window': 'window1'}, axis=1)\n",
    "    print(a_ecg.columns)\n",
    "    a = pd.merge(a, a_ecg, how='left', on=['window'])\n",
    "#     a = a.drop(['window1'],axis=1)\n",
    "#     plt.figure(figsize=(16,10))\n",
    "#     plt.plot(a['timestamp'],a['stress_likelihood_ecg'])\n",
    "#     plt.show()\n",
    "    return a\n",
    "    \n",
    "def parse_each_participant(directory,d):\n",
    "    data = pickle.load(open(directory+d,'rb')).reset_index(drop=True)\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return 0\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return 0\n",
    "    final_output = pd.concat(final_output)\n",
    "    print(final_output.columns)\n",
    "    final_output = get_both_stress(final_output)\n",
    "#     pickle.dump([final_output,ema],open(directory1+d,'wb'))\n",
    "    return 0\n",
    "directory = '../../cc3/rice_data/ecg_ppg_25_left3/'\n",
    "directory1 = '../../cc3/rice_data/ecg_ppg_25_left5/'\n",
    "# all_data = Parallel(n_jobs=30,verbose=2)(delayed(parse_each_participant)(directory,d) for d in os.listdir(directory)[:2] if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory,d) for d in os.listdir(directory) if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.concatenate([a[0] for a in all_data])\n",
    "yld = np.concatenate([a[1] for a in all_data])\n",
    "yld1 = yld[:,:2]\n",
    "yld = yld[:,2:]\n",
    "day_corr = np.concatenate([a[2] for a in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(yld)\n",
    "plt.ylabel('Minutes')\n",
    "plt.xticks(range(1,yld.shape[1]+1),['ECG YIELD','PPG YIELD'])\n",
    "plt.title('Stress yield across all participant days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(day_corr.shape)\n",
    "day_corr = day_corr[~np.isnan(day_corr).any(axis=1)]\n",
    "print(np.sum([a[4] for a in all_data]),'- Participant Days,',np.sum([a[3] for a in all_data]),'- Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylim([-1,1])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "#  plt.figure(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['corr_new'], by='quality', ax=ax,showfliers=True)\n",
    "plt.ylim([-3,1])\n",
    "plt.xticks(rotation=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['ppg_yield'], by='quality', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.groupby('quality').quantile([.25,.75]).loc[(0.2, 0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all1['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.load(open('../models/stress_model_ecg_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
