{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   5 out of  14 | elapsed:   37.2s remaining:  1.1min\n",
      "[Parallel(n_jobs=25)]: Done   9 out of  14 | elapsed:   42.0s remaining:   23.3s\n",
      "[Parallel(n_jobs=25)]: Done  14 out of  14 | elapsed:   55.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.02007684e+04 7.57805135e+01 8.27580289e+02 8.17039141e+02\n",
      " 8.69992088e+02 7.73150168e+02 7.35758230e+01 6.20705434e-02\n",
      " 2.43335611e-01 5.31352817e-01 3.81919420e-01] [1.75243688e+04 4.34598600e+01 2.65510720e+01 3.48250429e+01\n",
      " 4.45963774e+01 3.88542048e+01 3.29728399e+00 6.30927279e-02\n",
      " 1.39978053e-01 1.12791213e-01 3.08609131e-01] fdddb3bd-bb88-458f-bcc8-e50bb3f87742\n",
      "(4096, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   3 out of  12 | elapsed:   23.1s remaining:  1.2min\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  12 | elapsed:   38.3s remaining:   27.4s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d2552775d2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.intersect1d(os.listdir(directory_left),os.listdir(directory_right)) if d[-1]=='p')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparse_each_participant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-d2552775d2b3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.intersect1d(os.listdir(directory_left),os.listdir(directory_right)) if d[-1]=='p')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparse_each_participant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-d2552775d2b3>\u001b[0m in \u001b[0;36mparse_each_participant\u001b[0;34m(directory_left, directory_right, d)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_each_participant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mleft_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mema_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_left\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mright_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mema_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_right\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_data\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d2552775d2b3>\u001b[0m in \u001b[0;36mget_all_data\u001b[0;34m(data, hand)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_each_day_ppg_ecg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;31m#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_output\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 747, in _queue_management_worker\n",
      "    recursive_terminate(p)\n",
      "  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 28, in recursive_terminate\n",
      "    _recursive_terminate_without_psutil(process)\n",
      "  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 53, in _recursive_terminate_without_psutil\n",
      "    _recursive_terminate(process.pid)\n",
      "  File \"/cerebralcortex/kessel_jupyter_virtualenv/cc3_high_performance/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 94, in _recursive_terminate\n",
      "    stderr=None\n",
      "  File \"/usr/lib64/python3.6/subprocess.py\", line 356, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"/usr/lib64/python3.6/subprocess.py\", line 438, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['pgrep', '-P', '76148']' died with <Signals.SIGINT: 2>.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a,n=60):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/n,len(a[a>.6])/n]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>20]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+2*tt)&(heart_rate_window[:,0]>r-2*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    \n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = []\n",
    "    tss = []\n",
    "    for c, rr in enumerate(RRints):        \n",
    "        if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "            NNs.append(rr)\n",
    "            tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "    \n",
    "def get_features_all(a):\n",
    "    tmp = list(frequencyDomain(np.array(a[:,0])/1000,np.cumsum(a[:,0])/1000).values())\n",
    "    return np.array(list(get_weighted_rr_features(a))+list(tmp))\n",
    "\n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2)&(a[:,1]>.05))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>20]\n",
    "    nn = data_day.length1.max()\n",
    "    if nn>60:\n",
    "        nn = 120\n",
    "    else:\n",
    "        nn = 60\n",
    "    print(data_day.shape,'rr')\n",
    "    if data_day.shape[0]<15:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_features_all(a))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a,n=nn))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>30]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=['ecg_rr_array','ltime','window','count_ecg','ecg_rr_array_final','ecg_features'])\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:np.array(list(get_rr_features(a))+list(frequencyDomain(np.array(a)/1000,\n",
    "                                                                                                                           np.cumsum(a)/1000).values())))\n",
    "    return data_day\n",
    "\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def weighted_mean(self, weights, **kwargs):\n",
    "    weights = self._shallow_copy(weights)\n",
    "    window = self._get_window(weights)\n",
    "\n",
    "    def _get_weighted_mean(X, Y):\n",
    "        X = X.astype('float64')\n",
    "        Y = Y.astype('float64')\n",
    "        sum_f = lambda x: x.rolling(window, self.min_periods, center=self.center).sum(**kwargs)\n",
    "        return sum_f(X * Y) / sum_f(Y)\n",
    "\n",
    "    return _flex_binary_moment(self._selected_obj, weights._selected_obj,\n",
    "                               _get_weighted_mean, pairwise=True)\n",
    "\n",
    "_Rolling_and_Expanding.weighted_mean = weighted_mean\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    try:\n",
    "        columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "           'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "           'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "           'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "           'rr_weighted_features', 'quality_mag', 'ecg_rr_array_final', 'ecg_features']\n",
    "        ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "    #     a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "        a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "        if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "            a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "            a_ecg = a_ecg[ecg_columns]\n",
    "        a_ppg = parse_day_data(a)\n",
    "        if a_ppg.shape[0]==0:\n",
    "            return pd.DataFrame([],columns=columns)\n",
    "        a_ppg = parse_for_features(a_ppg)\n",
    "        if a_ppg.shape[0]==0:\n",
    "            return pd.DataFrame([],columns=columns)\n",
    "        a_ppg = pd.merge(a_ppg, a_ecg, how='left',left_on=['window'],right_on=['window'])\n",
    "        if a_ppg.shape[0]<60:\n",
    "            return pd.DataFrame([],columns=columns)\n",
    "#         a_ppg = get_ecg_stress(a_ppg)\n",
    "#         a_ppg = get_ppg_stress(a_ppg)\n",
    "        return a_ppg\n",
    "    except:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "\n",
    "def get_ppg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ppg_final.p','rb'))\n",
    "    quals1 = np.array(list(a['quality_mag'].values))\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "    rr_70th = np.percentile(ss,30)\n",
    "    rr_95th = np.percentile(ss,99.9)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "#         if i==2:\n",
    "#             print(m,s)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    probs = clf.predict_proba(np.nan_to_num(feature_matrix))[:,1]\n",
    "    a['stress_likelihood_ppg'] = probs\n",
    "#     a1 = a[['time','stress_likelihood_ecg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.plot(a1['time'],a1['stress_likelihood_ecg'],'*-k')\n",
    "#     a1 = a[['time','stress_likelihood_ppg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "#     a1['stress_likelihood_ppg_qual'] = a1['stress_likelihood_ppg'].rolling(window = 11).weighted_mean(a['quality_mag'])\n",
    "#     from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#     from sklearn.gaussian_process.kernels import RBF\n",
    "#     y = a1['stress_likelihood_ppg']\n",
    "#     m = np.mean(y)\n",
    "#     y = y \n",
    "#     X = a[['time','quality_mag']].values\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "#     X[:,0] = X[:,0] - np.mean(X[:,0])\n",
    "#     print(X.shape)\n",
    "#     gpr = GaussianProcessRegressor(kernel=RBF(length_scale=20),random_state=0).fit(X, y)\n",
    "#     X_pred = X\n",
    "#     X_pred[:,1] = np.mean(X[:,1])\n",
    "#     y1 = gpr.predict(X_pred,return_std=False)\n",
    "# #     plt.scatter(a1['time'],a1['stress_likelihood_ppg'],c=a['quality_mag'])\n",
    "#     plt.plot(a1['time'],a1['stress_likelihood_ppg_qual'],'o-r')\n",
    "#     plt.plot(a1['time'],y1,'o-k')\n",
    "#     a = a.sort_values('time').reset_index(drop=True)\n",
    "#     plt.bar(a1['time'],a1['quality_mag'],500,color='blue')\n",
    "#     plt.plot(a['time'],a['stress_likelihood_ecg'],'s-k')\n",
    "#     plt.show()\n",
    "    return a\n",
    "\n",
    "def get_ecg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ecg'] = np.nan\n",
    "        return a\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],70)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    print(means,stds,a['user'].values[0])\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a_ecg = a_ecg.drop(['ecg_features'],axis=1)\n",
    "    print(a_ecg.dropna().shape)\n",
    "    a = pd.merge(a, a_ecg, how='left', left_on=['window'], right_on=['window'])\n",
    "    return a\n",
    "\n",
    "def get_all_data(data,hand='left'):\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = pd.concat(final_output)\n",
    "    final_output = get_ecg_stress(final_output)\n",
    "    final_output = get_ppg_stress(final_output)\n",
    "    final_output['stress_likelihood_ppg_qual'] = final_output['stress_likelihood_ppg']\n",
    "    final_output['hand'] = hand\n",
    "    return final_output,ema\n",
    "\n",
    "def parse_each_participant(directory_left,directory_right,d):\n",
    "    left_data,ema_left = get_all_data(pickle.load(open(directory_left+d,'rb')).reset_index(drop=True),'left')\n",
    "    right_data,ema_right = get_all_data(pickle.load(open(directory_right+d,'rb')).reset_index(drop=True),'right')\n",
    "    data = pd.concat([a for a in [left_data,right_data] if a.shape[0]>0])\n",
    "    print(left_data.shape,right_data.shape,data.shape,left_data.columns)\n",
    "    if data.shape[0]>0:\n",
    "        pickle.dump([data,ema_left,ema_right],open(directory1+d,'wb'))\n",
    "        print('saved','-'*30)\n",
    "    return 0\n",
    "\n",
    "directory_left = '../../cc3/rice_data/ecg_ppg_5_left_final/'\n",
    "directory_right = '../../cc3/rice_data/ecg_ppg_5_right_final/'\n",
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_total_5/'\n",
    "import os\n",
    "if not os.path.isdir(directory1):\n",
    "    os.makedirs(directory1)\n",
    "# all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.intersect1d(os.listdir(directory_left),os.listdir(directory_right)) if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory_left,directory_right,d) for d in np.intersect1d(os.listdir(directory_left),os.listdir(directory_right))[::-1] if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done  67 out of  94 | elapsed:    6.2s remaining:    2.5s\n",
      "[Parallel(n_jobs=30)]: Done  94 out of  94 | elapsed:    9.3s finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def weighted_mean(self, weights, **kwargs):\n",
    "    weights = self._shallow_copy(weights)\n",
    "    window = self._get_window(weights)\n",
    "\n",
    "    def _get_weighted_mean(X, Y):\n",
    "        X = X.astype('float64')\n",
    "        Y = Y.astype('float64')\n",
    "        sum_f = lambda x: x.rolling(window, self.min_periods, center=self.center).sum(**kwargs)\n",
    "        return sum_f(X * Y) / sum_f(Y)\n",
    "\n",
    "    return _flex_binary_moment(self._selected_obj, weights._selected_obj,\n",
    "                               _get_weighted_mean, pairwise=True)\n",
    "\n",
    "def dump_data(directory_ema,directory1,directory2,f):\n",
    "    if f not in os.listdir(directory_ema):\n",
    "        return 0\n",
    "    data = pickle.load(open(directory1+f,'rb'))\n",
    "    a = data[0]\n",
    "    this_participant = []\n",
    "    stress_all = a[['time','ltime','user','stress_likelihood_ppg','quality_mag','day','activity','hand','rr_weighted_features']].dropna()\n",
    "    stress_all['hr'] = stress_all['rr_weighted_features'].apply(lambda a:a[-5])\n",
    "    stress_days = [a for i,a in stress_all.groupby(['hand','day'],as_index=False) if a.shape[0]>180]\n",
    "    _Rolling_and_Expanding.weighted_mean = weighted_mean\n",
    "    for a in stress_days:\n",
    "        a = a.sort_values('time').reset_index(drop=True)\n",
    "        a['stress_likelihood_ppg_qual'] = a['stress_likelihood_ppg'].rolling(window = 7).weighted_mean(a['quality_mag'])\n",
    "        a['qual'] = a['quality_mag'].rolling(window = 7).weighted_mean(a['quality_mag'])\n",
    "        a['hr'] = a['hr'].rolling(window = 7).weighted_mean(a['quality_mag'])\n",
    "        this_participant.append(a)\n",
    "    data = pd.concat(this_participant)\n",
    "    ema = pickle.load(open(directory_ema+f,'rb'))\n",
    "#     print(ema.columns)\n",
    "    ema = ema.sort_values('score').reset_index(drop=True)\n",
    "    ema['label'][ema['score']<=np.mean(ema['score'])] = 0\n",
    "    ema['label'][ema['score']>np.mean(ema['score'])] = 1\n",
    "#     ema['happy'] = ema['all_scores'].apply(lambda a:a[0])\n",
    "#     ema['joyful'] = ema['all_scores'].apply(lambda a:a[1])\n",
    "#     ema['nervous'] = ema['all_scores'].apply(lambda a:a[2])\n",
    "#     ema['sad'] = ema['all_scores'].apply(lambda a:a[3])\n",
    "#     ema['angry'] = ema['all_scores'].apply(lambda a:a[4])\n",
    "#     labels = ema[['happy','joyful','nervous','sad','angry']].values\n",
    "#     for j in range(labels.shape[1]):\n",
    "#         t = labels[:,j]\n",
    "#         t[t<=np.mean(t)] = 0\n",
    "#         t[t!=0] = 1\n",
    "#         labels[:,j] = t\n",
    "#     labels = np.sum(labels,axis=1)\n",
    "#     labels[labels<=2] = 0\n",
    "#     labels[labels!=0] = 1\n",
    "#     ema['label'] = labels\n",
    "#     ind_max = np.argmax(diff)\n",
    "#     if ind_max<3:\n",
    "#         continue\n",
    "#     ema['label'] = [0]*(ind_max-1) + [1]*(ema.shape[0]-ind_max+1)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "# #     plt.hist(data['stress_likelihood_ecg'])\n",
    "#     plt.hist(data['stress_likelihood_ppg_qual'])\n",
    "#     plt.show()\n",
    "#     plt.figure(figsize=(18,10))\n",
    "#     plt.hist(data['stress_likelihood_ppg_qual'],50)\n",
    "#     plt.show()\n",
    "    pickle.dump([data,ema],open(directory2+f,'wb'))\n",
    "    return 0\n",
    "\n",
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_total_5/'\n",
    "directory2 = '../../cc3/rice_data/after_ema_parsing/ecg_ppg_final_weighted_total_5/'\n",
    "if not os.path.isdir(directory2):\n",
    "    os.makedirs(directory2)\n",
    "directory_ema = '../../cc3/rice_data/ecg_ppg_ema_final/'\n",
    "from joblib import Parallel,delayed\n",
    "output = Parallel(n_jobs=30,verbose=3)(delayed(dump_data)(directory_ema,directory1,directory2,f) for f in os.listdir(directory1) if f[-1]=='p')    \n",
    "# output = [dump_data(directory_ema,directory1,directory2,f) for f in os.listdir(directory1) if f[-1]=='p']   \n",
    "\n",
    "#         plt.figure(figsize=(18,10))\n",
    "#         plt.plot(a['time'],a['stress_likelihood_ppg_qual'],'*-r')\n",
    "#         plt.plot(a['time'],a['hr']/np.max(a['hr']),'o-c')\n",
    "#         plt.title(np.max(a['hr']))\n",
    "#         plt.bar(a['time'],a['qual'],50)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "import matplotlib.pyplot as plt\n",
    "directory2 = '../../cc3/rice_data/after_ema_parsing/ecg_ppg_final_weighted_total_5/'\n",
    "count = 0\n",
    "df_col = []\n",
    "ema_day = []\n",
    "for f in os.listdir(directory2):\n",
    "    if f[-1]!='p':\n",
    "        continue\n",
    "    data,ema = pickle.load(open(directory2+f,'rb'))\n",
    "    print(ema['label'].unique())\n",
    "    for day in ema['day'].unique():\n",
    "        ema_day.append(ema[ema.day==day])\n",
    "        if ema_day[-1].shape[0]<1:\n",
    "            ema_day = ema_day[:-1]\n",
    "            continue\n",
    "        if data[data.day==day]['stress_likelihood_ppg_qual'].dropna().shape[0]<120:\n",
    "            ema_day = ema_day[:-1]\n",
    "            continue\n",
    "        df_col.append(data[data.day==day])\n",
    "#         count+=ema_day[-1].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  22 | elapsed:   33.0s remaining:  3.5min\n",
      "[Parallel(n_jobs=30)]: Done  11 out of  22 | elapsed:  1.6min remaining:  1.6min\n",
      "[Parallel(n_jobs=30)]: Done  19 out of  22 | elapsed:  2.7min remaining:   25.7s\n",
      "[Parallel(n_jobs=30)]: Done  22 out of  22 | elapsed:  3.2min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "import numpy as np\n",
    "def get_data_data(duration,df_col,ema_day):\n",
    "    all_data = []\n",
    "    all_emas = []\n",
    "    all_users = []\n",
    "    hands = []\n",
    "    for i in range(len(df_col)):\n",
    "        data = df_col[i].sort_values('time').reset_index(drop=True)\n",
    "        ema = ema_day[i].sort_values('time').reset_index(drop=True)\n",
    "        for hand in ['left','right']:\n",
    "            for j,row in ema.iterrows():\n",
    "                temp_data = data[(data.time>=row['time']-duration*60) & (data.time<row['time']) & (data.hand==hand)].sort_values('time').reset_index(drop=True)\n",
    "                if temp_data.shape[0]>duration/3 and temp_data['time'].values[-1]-temp_data['time'].values[0]>duration*60/3:\n",
    "                    all_data.append(temp_data)\n",
    "                    all_emas.append(row)\n",
    "                    all_users.append(row['user'])\n",
    "                    hands.append(hand)\n",
    "    return [duration,all_data,all_emas,all_users,hands]\n",
    "\n",
    "data_data = Parallel(n_jobs=30,verbose=3)(delayed(get_data_data)(duration,df_col,ema_day) for duration in np.arange(5,225,10))\n",
    "pickle.dump(data_data,open('../data/data_emas_all_duration_total.p','wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy [pylab.py:160]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab as pb\n",
    "import GPy \n",
    "%pylab inline\n",
    "def get_predictions(X,Y,error):\n",
    "    X = (X - np.mean(X))/np.std(X)\n",
    "    mm = np.mean(Y)\n",
    "    ss = np.std(Y)\n",
    "    Y = (Y-np.mean(Y))/ss\n",
    "    kern =  GPy.kern.RBF(input_dim=1) + GPy.kern.MLP(1) \n",
    "    Y_meta = {'output_index':np.arange(len(Y))[:,None]}\n",
    "    m = GPy.models.GPHeteroscedasticRegression(X[:,None],Y[:,None],kern,Y_metadata=Y_meta)\n",
    "    m['.*het_Gauss.variance'] = np.abs(error)[:,None] #Set the noise parameters to the error in Y\n",
    "    m.het_Gauss.variance.fix() #We can fix the noise term, since we already know it\n",
    "    m.optimize()\n",
    "    preds,varss  = m.predict(m.X,full_cov=False,Y_metadata=None,kern=None,likelihood=None,include_likelihood=False)\n",
    "    return preds*ss+mm,varss\n",
    "len(df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 554 out of 554 | elapsed:  6.8min finished\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def save_pdf(data,ema,i):\n",
    "    for hand in ['left','right']:\n",
    "        g= deepcopy(data[data.hand==hand]).dropna()\n",
    "#         g['hour'] = g['ltime'].apply(lambda a:int(a.hour))\n",
    "#         print(g.hour.values)\n",
    "#         g = g[(g.hour>=6) & (g.hour<20)]\n",
    "        if g.shape[0]<100:\n",
    "            continue\n",
    "        g['ltime_str'] = g['ltime'].apply(lambda a:a.strftime(\"%H:%M\"))\n",
    "        preds,varss = get_predictions(g['time'].values,g['stress_likelihood_ppg_qual'].values,(1.1-g['quality_mag'].values)/2)\n",
    "        plt.figure(figsize=(19,10))\n",
    "        plt.title(g['user'].values[0] +'--'+ g['day'].values[0]+'--'+g['hand'].values[0]+' wrist')\n",
    "        plt.plot((g['time']-np.min(g['time']))/60,g['stress_likelihood_ppg_qual'],'*-r',linewidth=1,label='Stress Likelihood')\n",
    "        plt.plot((g['time']-np.min(g['time']))/60,preds,'*-k',linewidth=3,label='Stress Likelihood Post Processed by GP')\n",
    "#         plt.plot((g['time']-np.min(g['time']))/60,g['hr']/60,'s-g',linewidth=3,label='Normalized Heart Rate w.r.t. 60 BPM')\n",
    "        #         plt.plot((g['time']-np.min(g['time']))/60,g['stress_likelihood_ppg'],'y',linestyle='--')\n",
    "        plt.bar((g['time']-np.min(g['time']))/60,g['quality_mag'],4,alpha=1,label='Minute Level Quality Metric')\n",
    "        #         for j,row in ema.iterrows():\n",
    "        #             print(row['label'])\n",
    "        #             if np.int64(row['label'])==1:\n",
    "        row = ema[ema.label==1]\n",
    "        if row.shape[0]>0:\n",
    "            plt.bar((row['time']-np.min(g['time']))/60,[1]*row.shape[0],10,alpha=1,label='Stress EMA',color='darkred')\n",
    "        #             else:\n",
    "        row = ema[ema.label==0]\n",
    "        if row.shape[0]>0:\n",
    "            plt.bar((row['time']-np.min(g['time']))/60,[1]*row.shape[0],10,alpha=1,label='Not Stress EMA',color='darkgreen')\n",
    "        plt.legend(ncol=5)\n",
    "        plt.xticks(np.array((g['time']-np.min(g['time']))/60)[np.arange(g.shape[0])%10==0],g['ltime_str'][np.arange(g.shape[0])%10==0],rotation=60)\n",
    "        plt.ylim([0,1.3])\n",
    "        plt.xlabel('Time of Day')\n",
    "    #     plt.ylabel('Likelihood Values')\n",
    "        plt.savefig('../pics_day/'+g['user'].values[0] +'--'+ g['day'].values[0]+'--'+g['hand'].values[0]+' wrist'+'.pdf',dps=1e6)\n",
    "        plt.close('all')\n",
    "    return 0\n",
    "from joblib import Parallel,delayed\n",
    "ouput = Parallel(n_jobs=-1,verbose=2)(delayed(save_pdf)(df_col[i].sort_values('time').reset_index(drop=True),\n",
    "                                                        ema_day[i].sort_values('time').reset_index(drop=True),\n",
    "                                                       i) for i in range(len(df_col)))\n",
    "# ouput = [save_pdf(df_col[i].sort_values('time').reset_index(drop=True),\n",
    "#                                                         ema_day[i].sort_values('time').reset_index(drop=True),\n",
    "#                                                        i) for i in range(len(df_col))]\n",
    "    #         plt.bar((g['time']-np.min(g['time']))/60,g['activity_f'],.2,alpha=.5,color='r')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "import os\n",
    "filenames = os.listdir('../pics_day/')\n",
    "filenames = sorted([a for a in filenames if a[-1]=='f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = PdfFileMerger()\n",
    "for filename in filenames:\n",
    "    merger.append(PdfFileReader(open('../pics_day/'+filename, 'rb')))\n",
    "\n",
    "merger.write(\"result-output_total_normalization.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
