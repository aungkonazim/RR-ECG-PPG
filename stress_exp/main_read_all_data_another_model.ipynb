{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>30]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+3*tt)&(heart_rate_window[:,0]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    \n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = []\n",
    "    tss = []\n",
    "    for c, rr in enumerate(RRints):        \n",
    "        if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "            NNs.append(rr)\n",
    "            tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "    \n",
    "def get_features_all(a):\n",
    "    tmp = list(frequencyDomain(np.array(a[:,0])/1000,np.cumsum(a[:,0])/1000).values())\n",
    "    return np.array(list(get_weighted_rr_features(a))+list(tmp))\n",
    "\n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,1]>.05)&(a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>30]\n",
    "    print(data_day.shape,'rr')\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_features_all(a))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>30]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=['ecg_rr_array','ltime','window','count_ecg','ecg_rr_array_final','ecg_features'])\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:np.array(list(get_rr_features(a))+list(frequencyDomain(np.array(a)/1000,\n",
    "                                                                                                                           np.cumsum(a)/1000).values())))\n",
    "    return data_day\n",
    "\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag', 'ecg_rr_array_final', 'ecg_features']\n",
    "    ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "#     a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "    a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "    if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "        a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "        a_ecg = a_ecg[ecg_columns]\n",
    "    a_ppg = parse_day_data(a)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = parse_for_features(a_ppg)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = pd.merge(a_ppg, a_ecg, how='left',left_on=['window'],right_on=['window'])\n",
    "    if a_ppg.shape[0]<60:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = get_ecg_stress(a_ppg)\n",
    "    a_ppg = get_ppg_stress(a_ppg)\n",
    "    return a_ppg\n",
    "\n",
    "def get_ppg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ppg_final.p','rb'))\n",
    "    quals1 = np.array(list(a['quality_mag'].values))\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "    rr_70th = np.percentile(ss,40)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a['stress_likelihood_ppg'] = probs\n",
    "    a1 = a[['time','stress_likelihood_ecg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "    plt.figure(figsize=(16,8))\n",
    "#     plt.plot(a1['time'],a1['stress_likelihood_ecg'],'*-k')\n",
    "    a1 = a[['time','stress_likelihood_ppg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    y = a1['stress_likelihood_ppg']\n",
    "    X = a[['time','quality_mag']].values\n",
    "    X[:,0] = X[:,0] - np.mean(X[:,0])\n",
    "    print(X.shape)\n",
    "    gpr = GaussianProcessRegressor(kernel=RBF(length_scale=20),random_state=0).fit(X, y)\n",
    "    X_pred = X\n",
    "    X_pred[:,1] = 1\n",
    "    y1 = gpr.predict(X_pred,return_std=False)\n",
    "#     plt.scatter(a1['time'],a1['stress_likelihood_ppg'],c=a['quality_mag'])\n",
    "    plt.plot(a1['time'],a1['stress_likelihood_ppg'],'o-r')\n",
    "    plt.plot(a1['time'],y1,'o-k')\n",
    "    plt.bar(a1['time'],a1['quality_mag'],500,color='blue')\n",
    "    plt.show()\n",
    "    return a\n",
    "\n",
    "def get_ecg_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ecg'] = np.nan\n",
    "        return a\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a_ecg = a_ecg.drop(['ecg_features'],axis=1)\n",
    "    a = pd.merge(a, a_ecg, how='left', left_on=['window'], right_on=['window'])\n",
    "    return a\n",
    "    \n",
    "def parse_each_participant(directory,d):\n",
    "    data = pickle.load(open(directory+d,'rb')).reset_index(drop=True)\n",
    "    print(data.shape,d)\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return 0\n",
    "#     final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "    final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return 0\n",
    "    final_output = pd.concat(final_output)\n",
    "    final_output['stress_likelihood_ppg_qual'] = final_output['stress_likelihood_ppg']\n",
    "    print(final_output.shape,final_output.columns)\n",
    "# #     print(final_output.shape,final_output.columns)\n",
    "# #     pickle.dump([final_output,ema],open(directory1+d,'wb'))\n",
    "    return 0\n",
    "\n",
    "directory = '../../cc3/rice_data/ecg_ppg_5_right_final/'\n",
    "directory1 = '../../cc3/rice_data/ecg_ppg_25_left8/'\n",
    "# all_data = Parallel(n_jobs=30,verbose=2)(delayed(parse_each_participant)(directory,d) for d in os.listdir(directory)[:2] if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory,d) for d in os.listdir(directory)[:5] if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.externals.joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a):\n",
    "    feature = [np.percentile(a,50),np.mean(a),\n",
    "               len(a[a>.2])/60,len(a[a>.6])/60]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>20]\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "    try:\n",
    "        r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "        index = np.where((heart_rate_window[:,0]<r+3*tt)&(heart_rate_window[:,0]>r-3*tt))[0]\n",
    "        heart_rate_window = heart_rate_window[index]\n",
    "    except:\n",
    "        pass\n",
    "    if heart_rate_window.shape[0]>10:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "    \n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,1]>.05)&(a[:,0]>300)&(a[:,0]<1500)&(a[:,2]<.2))[0],:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>30]\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_weighted_rr_features(a))\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>20]\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:get_rr_features(a))\n",
    "    return data_day\n",
    "\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'activity', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag','window', 'ecg_rr_array_final', 'ecg_features']\n",
    "    ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "    a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "    a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "    if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "        a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "        a_ecg = a_ecg[ecg_columns]\n",
    "    a_ppg = parse_day_data(a)\n",
    "    a_ppg = parse_for_features(a_ppg)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "#     a_ecg = a_ecg.rename({'window': 'window1'}, axis=1)\n",
    "    a_ppg = pd.merge(a_ppg, a_ecg, how='left', left_on=['window'],right_on=['window'],suffixes=('', '_7'))\n",
    "#     print(a_ppg.columns,'r')\n",
    "    return a_ppg[columns]\n",
    "\n",
    "def get_both_stress(a):\n",
    "    clf = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    print(a_ecg.columns,'--'*20)\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a = a.drop(['ecg_features'],axis=1)\n",
    "#     print(a.columns,a_ecg.columns)\n",
    "#     a_ecg = a_ecg.rename({'window': 'window1'}, axis=1)\n",
    "    print(a_ecg.columns)\n",
    "    a = pd.merge(a, a_ecg, how='left', on=['window'])\n",
    "#     a = a.drop(['window1'],axis=1)\n",
    "#     plt.figure(figsize=(16,10))\n",
    "#     plt.plot(a['timestamp'],a['stress_likelihood_ecg'])\n",
    "#     plt.show()\n",
    "    return a\n",
    "    \n",
    "def parse_each_participant(directory,d):\n",
    "    data = pickle.load(open(directory+d,'rb')).reset_index(drop=True)\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return 0\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return 0\n",
    "    final_output = pd.concat(final_output)\n",
    "    print(final_output.columns)\n",
    "    final_output = get_both_stress(final_output)\n",
    "#     pickle.dump([final_output,ema],open(directory1+d,'wb'))\n",
    "    return 0\n",
    "directory = '../../cc3/rice_data/ecg_ppg_25_left3/'\n",
    "directory1 = '../../cc3/rice_data/ecg_ppg_25_left5/'\n",
    "# all_data = Parallel(n_jobs=30,verbose=2)(delayed(parse_each_participant)(directory,d) for d in os.listdir(directory)[:2] if d[-1]=='p')\n",
    "all_data = [parse_each_participant(directory,d) for d in os.listdir(directory) if d[-1]=='p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.concatenate([a[0] for a in all_data])\n",
    "yld = np.concatenate([a[1] for a in all_data])\n",
    "yld1 = yld[:,:2]\n",
    "yld = yld[:,2:]\n",
    "day_corr = np.concatenate([a[2] for a in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(yld)\n",
    "plt.ylabel('Minutes')\n",
    "plt.xticks(range(1,yld.shape[1]+1),['ECG YIELD','PPG YIELD'])\n",
    "plt.title('Stress yield across all participant days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(day_corr.shape)\n",
    "day_corr = day_corr[~np.isnan(day_corr).any(axis=1)]\n",
    "print(np.sum([a[4] for a in all_data]),'- Participant Days,',np.sum([a[3] for a in all_data]),'- Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylim([-1,1])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.boxplot(day_corr[~np.isnan(day_corr).any(axis=1)][:,np.array([0,1,2])])\n",
    "plt.ylabel('Pearson Correlation')\n",
    "plt.xticks(range(1,day_corr.shape[1]+1),['Original cStress','cStress with Weighted Features and weighted normalization','cStress with Weighted Features'],rotation=10)\n",
    "plt.title('Correlation with ECG For Different Modes of Normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "#  plt.figure(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data1,columns=['quality','corr_orig','corr_new','corr_new1','corr_between','ppg_yield','ecg_yield'])\n",
    "data_all1 = pd.DataFrame(yld1,columns=['quality','ppg_yield'])\n",
    "\n",
    "corr_25 = data_all.groupby('quality').quantile(.5)\n",
    "x = corr_25.index.values\n",
    "x1 = np.unique(data_all1['quality'].values)\n",
    "y = []\n",
    "for a in x1:\n",
    "    y.append(data_all1[data_all1.quality>=a]['ppg_yield'].sum()/60/np.sum([a[4] for a in all_data]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':20})\n",
    "fig, ax1 = plt.subplots(figsize=(20,12))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x,corr_25['corr_orig'].loc[x],label='Original Cstress')\n",
    "ax1.plot(x,corr_25['corr_new'].loc[x],label='Weighted Normalization with weighted features')\n",
    "ax1.plot(x,corr_25['corr_new1'].loc[x],label='Original Cstress using Weighted Features')\n",
    "# ax1.plot(x,corr_25['corr_between'].loc[x],label='Original Normalization using auto Features')\n",
    "ax2.plot(x1,y,label='PPG Yield')\n",
    "ax1.grid()\n",
    "# ax1.plot(x,corr_75['corr_orig'].loc[x],label='Original 75th')\n",
    "# ax1.plot(x,corr_75['corr_new'].loc[x],label='Weighted 75th')\n",
    "ax1.legend(fontsize=20)\n",
    "ax1.set_xlabel('Quality Metric')\n",
    "ax2.set_ylabel('Median Hours per Participant Day', color='g')\n",
    "ax1.set_ylabel('Median Correlation Across all Participant Days', color='b')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['corr_new'], by='quality', ax=ax,showfliers=True)\n",
    "plt.ylim([-3,1])\n",
    "plt.xticks(rotation=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.suptitle('')\n",
    "c = data_all.boxplot(column=['ppg_yield'], by='quality', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.groupby('quality').quantile([.25,.75]).loc[(0.2, 0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all1['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.load(open('../models/stress_model_ecg_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cerebral Cortex (Python 3)",
   "language": "python",
   "name": "python3-cerebralcortex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
