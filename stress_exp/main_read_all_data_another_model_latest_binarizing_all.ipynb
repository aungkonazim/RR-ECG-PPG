{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87a2bf88-ef4e-4bd5-96b6-eda8faac6a8e.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  16 | elapsed:   49.0s remaining:  5.7min\n",
      "[Parallel(n_jobs=25)]: Done   7 out of  16 | elapsed:  2.2min remaining:  2.8min\n",
      "[Parallel(n_jobs=25)]: Done  12 out of  16 | elapsed:  3.2min remaining:  1.1min\n",
      "[Parallel(n_jobs=25)]: Done  16 out of  16 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=25)]: Using backend LokyBackend with 25 concurrent workers.\n",
      "[Parallel(n_jobs=25)]: Done   2 out of  15 | elapsed:  1.1min remaining:  6.9min\n",
      "[Parallel(n_jobs=25)]: Done   6 out of  15 | elapsed:  1.3min remaining:  2.0min\n",
      "[Parallel(n_jobs=25)]: Done  10 out of  15 | elapsed:  2.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=25)]: Done  15 out of  15 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9785, 42) (10414, 42) (20199, 42) Index(['window', 'ltime', 'likelihood_max_array', 'power', 'activity',\n",
      "       'rr_array', 'time', 'timestamp', 'localtime', 'ecg_rr_array', 'day',\n",
      "       'version', 'user', 'quality_features', 'activity_features',\n",
      "       'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col', 'length1',\n",
      "       'indicator', 'rr_col1', 'rr_features', 'rr_weighted_features',\n",
      "       'feature_sum', 'rr_original_features', 'quality_mag',\n",
      "       'ecg_rr_array_final', 'ecg_features', 'stress_likelihood_ecg', '60',\n",
      "       '100', 'final_feature_matrix', 'stress_likelihood_ppg_qual',\n",
      "       'stress_likelihood_ppg_no_norm_qual',\n",
      "       'stress_likelihood_ppg_no_norm_qual_all_features',\n",
      "       'stress_likelihood_ppg_no_norm_no_qual',\n",
      "       'stress_likelihood_ppg_no_norm_no_qual_all_features', '60_all',\n",
      "       '100_all', 'hand'],\n",
      "      dtype='object')\n",
      "saved ------------------------------\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import iqr,skew,kurtosis\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from joblib import Parallel,delayed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    \n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = []\n",
    "    tss = []\n",
    "    for c, rr in enumerate(RRints):        \n",
    "        if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "            NNs.append(rr)\n",
    "            tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- Numpy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    if np.sum(weights)==0:\n",
    "        return np.mean(values),np.std(values)\n",
    "    average = np.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_weighted_rr_features(a):\n",
    "    a = np.repeat(a[:,0],np.int64(np.round(100*a[:,1])))\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "\n",
    "def get_quality_features(a,n=60):\n",
    "    feature = [np.percentile(a,50),np.mean(a),np.percentile(a,10),np.percentile(a,90),len(a),\n",
    "               len(a[a>.2])/n,len(a[a>.6])/n]\n",
    "    return np.array(feature)\n",
    "\n",
    "def get_daywise(data):\n",
    "    return [a for i,a in data.groupby(['user','day'],as_index=False) if a[['likelihood_max_array','rr_array']].dropna().shape[0]>60]\n",
    "\n",
    "def parse_day_data(data_day):\n",
    "    data_day['likelihood_max_array'] = data_day['likelihood_max_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['likelihood'] = data_day['likelihood_max_array'].apply(lambda a:np.max(a,axis=1))\n",
    "    data_day['likelihood_ind'] = data_day['likelihood_max_array'].apply(lambda a:np.argmax(a,axis=1))\n",
    "    data_day['rr_array'] = data_day['rr_array'].apply(lambda a:np.squeeze(a).reshape(-1,3))\n",
    "    data_day['length'] = data_day['rr_array'].apply(lambda a:a.shape[0])\n",
    "    data_day = data_day[data_day.length>=30]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['time'] = data_day['ltime'].apply(lambda a:datetime.timestamp(a))\n",
    "    indexes = data_day['likelihood_ind'].values\n",
    "    rr_arrays = data_day['rr_array'].values\n",
    "    rrs = []\n",
    "    for i,rr in enumerate(rr_arrays):\n",
    "        index = indexes[i]\n",
    "        frr = np.squeeze(np.array([rr[i,index[i]] for i in range(rr.shape[0])]))\n",
    "        rrs.append(frr)\n",
    "    data_day['rr'] = rrs\n",
    "    data_day['rr_col'] = data_day.apply(lambda a: np.vstack([np.squeeze(a['rr']),np.squeeze(a['likelihood']),np.squeeze(a['activity'])]).T,\n",
    "                     axis=1)\n",
    "    return data_day\n",
    "\n",
    "def remove_3sd(heart_rate_window):\n",
    "    temp = deepcopy(heart_rate_window)\n",
    "#     try:\n",
    "#         r,tt = weighted_avg_and_std(heart_rate_window[heart_rate_window[:,1]>.25,0],heart_rate_window[heart_rate_window[:,1]>.25,1])\n",
    "#         index = np.where((heart_rate_window[:,0]<r+2*tt)&(heart_rate_window[:,0]>r-2*tt))[0]\n",
    "#         heart_rate_window = heart_rate_window[index]\n",
    "#     except:\n",
    "#         pass\n",
    "    if heart_rate_window.shape[0]>=30:\n",
    "        return [heart_rate_window,'Available']\n",
    "    else:\n",
    "        return [temp[:10],'Not Available']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def get_features_all(a):\n",
    "    try:\n",
    "        tmp = list(frequencyDomain(np.array(a[:,0])/1000,np.cumsum(a[:,0])/1000).values())\n",
    "        return np.array(list(get_weighted_rr_features(a))+list(tmp))\n",
    "    except:\n",
    "        return np.zeros((11,))\n",
    "\n",
    "def get_features_all1(a):\n",
    "    try:\n",
    "        tmp = list(frequencyDomain(np.array(a[:,0])/1000,np.cumsum(a[:,0])/1000).values())\n",
    "        return np.array(list(get_rr_features(a[:,0]))+list(tmp))\n",
    "    except:\n",
    "        return np.zeros((11,))\n",
    "\n",
    "def parse_for_features(data_day):\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[np.where((a[:,0]>400)&(a[:,0]<1200)&(a[:,2]>0.001174897554939529))[0],:])\n",
    "    data_day['activity'] = data_day['rr_col'].apply(lambda a:a[:,-1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[:,:2])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:remove_3sd(a))\n",
    "    data_day['length1'] = data_day['rr_col'].apply(lambda a:a[0].shape[0])\n",
    "    data_day = data_day[data_day.length1>30]\n",
    "    nn = data_day.length1.max()\n",
    "    if nn>60:\n",
    "        nn = 120\n",
    "    else:\n",
    "        nn = 60\n",
    "    print(data_day.shape,'rr',data_day.dropna().shape)\n",
    "    if data_day.shape[0]<15:\n",
    "        return pd.DataFrame([],columns=data_day.columns)\n",
    "    data_day['activity_features'] = data_day['activity'].apply(lambda a:get_quality_features(a))\n",
    "    data_day['indicator'] = data_day['rr_col'].apply(lambda a:a[1])\n",
    "    data_day['rr_col'] = data_day['rr_col'].apply(lambda a:a[0])\n",
    "    data_day['likelihood'] = data_day['rr_col'].apply(lambda a:a[:,1])\n",
    "    data_day['rr'] = data_day['rr_col'].apply(lambda a:a[:,0])\n",
    "    data_day['rr_col1'] = data_day.apply(lambda a:np.vstack([list(a['rr']),list(a['likelihood'])]).T,axis=1)\n",
    "    data_day['rr_features'] = data_day['rr'].apply(lambda a:get_rr_features(a))\n",
    "    data_day['rr_weighted_features'] = data_day['rr_col1'].apply(lambda a:get_features_all(a))\n",
    "    data_day['feature_sum'] = data_day['rr_weighted_features'].apply(lambda a:np.sum(a))\n",
    "    data_day = data_day[data_day.feature_sum!=0]\n",
    "    data_day['rr_original_features'] = data_day['rr_col1'].apply(lambda a:get_features_all1(a))\n",
    "    data_day['feature_sum'] = data_day['rr_original_features'].apply(lambda a:np.sum(a))\n",
    "    data_day = data_day[data_day.feature_sum!=0]\n",
    "    data_day['quality_features'] = data_day['likelihood'].apply(lambda a:get_quality_features(a,n=nn))\n",
    "    data_day['quality_mag'] = data_day['quality_features'].apply(lambda a:np.sum(a)/len(a))\n",
    "    return data_day\n",
    "\n",
    "def normalize_daywise(feature_matrix,quals1):\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[:,i], quals1)\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    return feature_matrix\n",
    "\n",
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def parse_day_data_ecg(data_day):\n",
    "    data_day = data_day[['ecg_rr_array','ltime','window']].dropna()\n",
    "    data_day['count_ecg'] = data_day['ecg_rr_array'].apply(lambda a:len(a))\n",
    "    data_day = data_day[data_day.count_ecg>30]\n",
    "    if data_day.shape[0]<30:\n",
    "        return pd.DataFrame([],columns=['ecg_rr_array','ltime','window','count_ecg','ecg_rr_array_final','ecg_features'])\n",
    "    data_day['ecg_rr_array_final'] = data_day['ecg_rr_array']\n",
    "    data_day['ecg_features'] = data_day['ecg_rr_array_final'].apply(lambda a:np.array(list(get_rr_features(a))+list(frequencyDomain(np.array(a)/1000,\n",
    "                                                                                                                           np.cumsum(a)/1000).values())))\n",
    "    return data_day\n",
    "\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def weighted_mean(self, weights, **kwargs):\n",
    "    weights = self._shallow_copy(weights)\n",
    "    window = self._get_window(weights)\n",
    "\n",
    "    def _get_weighted_mean(X, Y):\n",
    "        X = X.astype('float64')\n",
    "        Y = Y.astype('float64')\n",
    "        sum_f = lambda x: x.rolling(window, self.min_periods, center=self.center).sum(**kwargs)\n",
    "        return sum_f(X * Y) / sum_f(Y)\n",
    "\n",
    "    return _flex_binary_moment(self._selected_obj, weights._selected_obj,\n",
    "                               _get_weighted_mean, pairwise=True)\n",
    "\n",
    "_Rolling_and_Expanding.weighted_mean = weighted_mean\n",
    "def parse_each_day_ppg_ecg(a):\n",
    "#     try:\n",
    "    columns = ['window', 'ltime', 'likelihood_max_array', 'rr_array',\n",
    "       'time', 'timestamp', 'likelihood_mean', 'localtime', 'ecg_rr_array',\n",
    "       'day', 'version', 'user', 'quality_features', 'activity_features', 'likelihood', 'likelihood_ind', 'length', 'rr', 'rr_col',\n",
    "       'length1', 'indicator', 'rr_col1', 'rr_features',\n",
    "       'rr_weighted_features', 'quality_mag', 'ecg_rr_array_final', 'ecg_features','activity','activity_features']\n",
    "    ecg_columns = ['window', 'ecg_rr_array_final','ecg_features']\n",
    "#     a = a.drop(['stress_likelihood', 'stress_likelihood_ecg'],axis=1)\n",
    "    a_ecg = pd.DataFrame([],columns=ecg_columns)\n",
    "    if a['ecg_rr_array'].dropna().shape[0]>60:\n",
    "        a_ecg = parse_day_data_ecg(deepcopy(a))\n",
    "        a_ecg = a_ecg[ecg_columns]\n",
    "    a_ppg = parse_day_data(a)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = parse_for_features(a_ppg)\n",
    "    if a_ppg.shape[0]==0:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = pd.merge(a_ppg, a_ecg, how='left',left_on=['window'],right_on=['window'])\n",
    "    if a_ppg.shape[0]<60:\n",
    "        return pd.DataFrame([],columns=columns)\n",
    "    a_ppg = get_ecg_stress(a_ppg)\n",
    "    a_ppg = get_ppg_stress(a_ppg)\n",
    "    return a_ppg\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return pd.DataFrame([],columns=columns)\n",
    "clf_ppg = pickle.load(open('../models/stress_ppg_final_0_qual.p','rb'))\n",
    "clf_ppg1 = pickle.load(open('../models/stress_ppg_final_no_quality_0_qual.p','rb'))\n",
    "clf_ppg2 = pickle.load(open('../models/stress_ppg_final_no_quality_all_features_0_qual.p','rb'))\n",
    "clf_ppg3 = pickle.load(open('../models/stress_ppg_final_all_features_0_qual.p','rb'))\n",
    "def get_ppg_stress(a):\n",
    "    quals1 = np.array(list(a['quality_mag'].values))\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg_qual'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1))+100)\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    a['60'] = rr_70th\n",
    "    a['100'] = rr_95th\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    a['final_feature_matrix'] = list(feature_matrix)\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    probs = clf_ppg.predict_proba(np.nan_to_num(feature_matrix[:,index]))[:,1]\n",
    "    a['stress_likelihood_ppg_qual'] = probs\n",
    "    \n",
    "    \n",
    "    quals1 = np.array([1]*a.shape[0])\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg_no_norm_qual'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1))+100)\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    probs = clf_ppg.predict_proba(np.nan_to_num(feature_matrix[:,index]))[:,1]\n",
    "    a['stress_likelihood_ppg_no_norm_qual'] = probs\n",
    "    \n",
    "    quals1 = np.array([1]*a.shape[0])\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg_no_norm_qual_all_features'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1))+100)\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    probs = clf_ppg3.predict_proba(np.nan_to_num(feature_matrix[:,:]))[:,1]\n",
    "    a['stress_likelihood_ppg_no_norm_qual_all_features'] = probs\n",
    "    \n",
    "    quals1 = np.array([1]*a.shape[0])\n",
    "    feature_matrix = np.array(list(a['rr_original_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg_no_norm_no_qual'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1))+100)\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    probs = clf_ppg1.predict_proba(np.nan_to_num(feature_matrix[:,index]))[:,1]\n",
    "    a['stress_likelihood_ppg_no_norm_no_qual'] = probs\n",
    "    \n",
    "    quals1 = np.array([1]*a.shape[0])\n",
    "    feature_matrix = np.array(list(a['rr_original_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ppg_no_norm_no_qual_all_features'] = np.nan\n",
    "        return a\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1))+100)\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    for i in range(feature_matrix.shape[1]):\n",
    "        m,s = weighted_avg_and_std(feature_matrix[index,i], quals1[index])\n",
    "        feature_matrix[:,i]  = (feature_matrix[:,i] - m)/s\n",
    "    index = np.array([0,1,3,4,5,6,8])\n",
    "    probs = clf_ppg2.predict_proba(np.nan_to_num(feature_matrix[:,:]))[:,1]\n",
    "    a['stress_likelihood_ppg_no_norm_no_qual_all_features'] = probs\n",
    "    \n",
    "    \n",
    "#     print(a.shape)\n",
    "#     a1 = a[['time','stress_likelihood_ecg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.plot(a1['time'],a1['stress_likelihood_ecg'],'*-k')\n",
    "#     a1 = a[['time','stress_likelihood_ppg','quality_mag']].dropna().sort_values('time').reset_index(drop=True)\n",
    "#     a1['stress_likelihood_ppg_qual'] = a1['stress_likelihood_ppg'].rolling(window = 11).weighted_mean(a['quality_mag'])\n",
    "#     from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#     from sklearn.gaussian_process.kernels import RBF\n",
    "#     y = a1['stress_likelihood_ppg']\n",
    "#     m = np.mean(y)\n",
    "#     y = y \n",
    "#     X = a[['time','quality_mag']].values\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "#     X[:,0] = X[:,0] - np.mean(X[:,0])\n",
    "#     print(X.shape)\n",
    "#     gpr = GaussianProcessRegressor(kernel=RBF(length_scale=20),random_state=0).fit(X, y)\n",
    "#     X_pred = X\n",
    "#     X_pred[:,1] = np.mean(X[:,1])\n",
    "#     y1 = gpr.predict(X_pred,return_std=False)\n",
    "# #     plt.scatter(a1['time'],a1['stress_likelihood_ppg'],c=a['quality_mag'])\n",
    "#     plt.plot(a1['time'],a1['stress_likelihood_ppg_qual'],'o-r')\n",
    "#     plt.plot(a1['time'],y1,'o-k')\n",
    "#     a = a.sort_values('time').reset_index(drop=True)\n",
    "#     plt.bar(a1['time'],a1['quality_mag'],500,color='blue')\n",
    "#     plt.plot(a['time'],a['stress_likelihood_ecg'],'s-k')\n",
    "#     plt.show()\n",
    "    return a\n",
    "\n",
    "clf_ecg = pickle.load(open('../models/stress_ecg_final.p','rb'))\n",
    "def get_ecg_stress(a):\n",
    "    a_ecg = deepcopy(a[['window','ecg_features']].dropna())\n",
    "    feature_matrix = np.array(list(a_ecg['ecg_features']))\n",
    "    if len(feature_matrix)<60:\n",
    "        a['stress_likelihood_ecg'] = np.nan\n",
    "        return a\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],22)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    print(means,stds,a['user'].values[0])\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    probs = clf_ecg.predict_proba(feature_matrix)[:,1]\n",
    "    a_ecg['stress_likelihood_ecg'] = probs\n",
    "    a_ecg = a_ecg.drop(['ecg_features'],axis=1)\n",
    "    print(a_ecg.dropna().shape)\n",
    "    a = pd.merge(a, a_ecg, how='left', left_on=['window'], right_on=['window'])\n",
    "    return a\n",
    "\n",
    "def get_all_data(data,hand='left'):\n",
    "    ema = data[['user','day','window','time','ltime','all_scores','score','label']]\n",
    "    data = data.drop(['all_scores','score','label'],axis=1)\n",
    "    data_all = get_daywise(data)\n",
    "    if len(data_all)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = Parallel(n_jobs=25,verbose=4)(delayed(parse_each_day_ppg_ecg)(a) for a in data_all)\n",
    "#     final_output = [parse_each_day_ppg_ecg(a) for a in data_all]\n",
    "    final_output = [a for a in final_output if a.shape[0]>0]\n",
    "    if len(final_output)==0:\n",
    "        return pd.DataFrame([],columns=['c']), pd.DataFrame([],columns=['c'])\n",
    "    final_output = pd.concat(final_output)\n",
    "    a = final_output\n",
    "    quals1 = np.array(list(a['quality_mag'].values))\n",
    "    feature_matrix = np.array(list(a['rr_weighted_features']))\n",
    "    ss = np.repeat(feature_matrix[:,2],np.int64(np.round(100*quals1)))\n",
    "    rr_70th = np.percentile(ss,20)\n",
    "    rr_95th = np.percentile(ss,99)\n",
    "    a['60_all'] = rr_70th\n",
    "    a['100_all'] = rr_95th\n",
    "#     final_output = get_ecg_stress(final_output)\n",
    "#     final_output = get_ppg_stress(final_output)\n",
    "#     final_output['stress_likelihood_ppg_qual'] = final_output['stress_likelihood_ppg']\n",
    "    final_output['hand'] = hand\n",
    "    return final_output,ema\n",
    "\n",
    "def parse_each_participant(directory_left,directory_right,d):\n",
    "    if d in os.listdir(directory1):\n",
    "        return 0\n",
    "    print(d)\n",
    "#     try:\n",
    "    if d in os.listdir(directory_left):\n",
    "        left_data,ema_left = get_all_data(pickle.load(open(directory_left+d,'rb')).reset_index(drop=True),'left')\n",
    "    else:\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "        left_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_left = pd.DataFrame([],columns=['a','b'])\n",
    "#     try:\n",
    "    if d in os.listdir(directory_right):\n",
    "        right_data,ema_right = get_all_data(pickle.load(open(directory_right+d,'rb')).reset_index(drop=True),'right')\n",
    "    else:\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "        right_data = pd.DataFrame([],columns=['a','b'])\n",
    "        ema_right = pd.DataFrame([],columns=['a','b'])\n",
    "    t = [a for a in [left_data,right_data] if a.shape[0]>0]\n",
    "    if len(t)==0:\n",
    "        return 0\n",
    "    data = pd.concat(t)\n",
    "    print(left_data.shape,right_data.shape,data.shape,left_data.columns)\n",
    "    if data.shape[0]>0:\n",
    "        pickle.dump([data,ema_left,ema_right],open(directory1+d,'wb'))\n",
    "        print('saved','-'*30)\n",
    "    return 0\n",
    "\n",
    "directory_left = '../../cc3/rice_data/ecg_ppg_5_left_final/'\n",
    "directory_right = '../../cc3/rice_data/ecg_ppg_5_right_final/'\n",
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_day_5_v13/'\n",
    "import os\n",
    "if not os.path.isdir(directory1):\n",
    "    os.makedirs(directory1)\n",
    "    \n",
    "users = ['00c08d2f-3b9c-48e9-9633-5a341892cc4b',       '02543bbf-84c2-4076-8547-c8a5f451ea02',       '05846fcf-1dd9-4f98-b17b-1ce6e624c0a7',       '0b4bce96-dccb-434b-b4ca-63c7fcd3a7fa',       '0c726695-f016-4019-9aab-c292298ee10c',       '0dbd4034-a57f-4056-a8a1-e0d6ba8fdd93',       '19223eac-7f2e-429c-8fa7-0977eee8ae7c',       '2197d640-d109-4f21-9691-7b009d4bfe4d',\n",
    "       '22c85326-97bf-4e4b-90c4-4255c144ae1b',       '2333036a-2f50-49ca-a119-3c5d66399fe4',       '2ac53735-dac3-4eb6-be36-f84cf9c02e57',       '2d8b5a8c-e990-4442-abf6-578e96d2f5eb',       '2e392ddb-12d6-42d0-a09a-cdcb22343223',       '2ea666ab-a0d0-4d1d-851d-917c3bc54eb1',       '3133920d-164a-48e6-8d9e-79e919c45d43',       '34e42cf6-7c34-417c-a003-874e3b6151e7',\n",
    "       '35109a64-411d-4768-9602-c0a3d519a088',       '3febca74-f12b-4a1b-a469-22d6cad30e74',       '4bf6078d-afcd-432a-a8a5-8b5e8a4eda9e',       '4ddc3405-d256-4f66-95df-2b13bf69a616',       '522a66a7-1502-46fe-bce4-9e022c52219f',       '59818d39-28e4-421f-9292-09258abd626f',       '59d70c4d-c87c-4a5e-b9c9-b9fad710fffb',       '5a47080d-a1ec-48f2-a174-a6a017fcb100',\n",
    "       '5f3f7553-6d2f-4c08-adb9-dbc3e88ba0aa',       '6ef875b3-2f7e-48b8-bf00-de3ee1316830',       '70c54196-b8d1-4e42-b067-9e9d648e0e58',       '780f84d0-eb06-4696-a47d-9320bc17d117',       '7846909f-c4a5-4d49-b11d-d1b1ca2c7309',       '7b63e06f-4931-41e2-aa62-1eb51c5e250b',       '8008f00d-2549-46e4-ab1f-01542c1076e2',       '808b555a-6573-457e-b1c6-e008594b0f9a',\n",
    "       '81d3b376-3549-4639-aad5-35670e772492',       '87877961-b2f8-4256-806b-973fbd798920',       '87933c28-09dc-41da-a3b2-a1fe522a8865',       '87a2bf88-ef4e-4bd5-96b6-eda8faac6a8e',       '892e71e0-a5a4-4315-89a4-fa5518d78591',       '896d9cb5-2e54-4900-9b8a-58c087549d19',       '897fdfcf-9004-4ef6-bf9a-8d3fe339c8ce',       '8d96c9a4-a13b-4729-adf3-969e84b9a6d2',\n",
    "       '9197be51-f220-4c63-a6a8-3ec1bbd50810',       '96f6e25f-4dd0-4070-a9ac-b04957969382',       '9744e4ae-63d8-49df-be6a-37cbb24532a1',       '9998aedf-5144-4402-9806-fd0965ca85c0',       '9a3bd464-f273-4f97-a48c-1f3c6a705a69',       '9b3e5f2e-99e8-4c4c-8580-6f4e2b107e37',       'ad1e878e-7bea-48ca-b890-92770fe02a4c',       'b0954814-ad5a-4a8f-ac5a-8436a70889d0',\n",
    "       'b0f051e0-69ef-4153-9a1e-22ed921d76c3',       'b53e7168-0a87-4646-b389-fb0fe60cc36a',       'b71b2071-6330-434d-a2ab-8e929e9b96a9',       'bb0fe5f5-798b-45ea-be50-8e56e3116369',       'bde40f50-8e35-4707-8260-b69f07773c4d',       'c64ca471-369e-43fa-a07b-8260fd1c745c',       'cd7235b7-56ac-49a9-a0a7-b309e0f3507c',       'ce5dd2cb-b2d5-49de-aa35-3bbcae7bcaff',\n",
    "       'cfe02b15-0332-4590-9ac6-9ef2eb8b3edd',       'd3cf5812-85fd-4328-9b2a-1b3b6b2cd0b0',       'd768791c-7479-4aaf-a6c8-61e82f1517e8',       'e099e913-4796-4408-af63-1d35c84f29fd',       'e2f73e61-5d54-4bfd-ba14-9b596203e425',       'ea2fa266-3e43-4552-8c74-cba474ae0038',       'f0286b14-18d2-46bd-845e-f83b43a2ef7b',       'f2bdd151-3fb8-497a-8380-a2b65dc01c2c',\n",
    "       'f35c1279-806e-4546-839b-037ee01b0116',       'f8d33ca1-e0fa-4b59-a7c2-b1aee8afcaea',       'fdddb3bd-bb88-458f-bcc8-e50bb3f87742']\n",
    "users = [a+'.p' for a in users]\n",
    "# users = ['87a2bf88-ef4e-4bd5-96b6-eda8faac6a8e.p']\n",
    "# all_data = Parallel(n_jobs=40,verbose=2)(delayed(parse_each_participant)(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right)) if d in users)\n",
    "all_data = [parse_each_participant(directory_left,directory_right,d) for d in np.unique(os.listdir(directory_left)+os.listdir(directory_right)) if d in users]\n",
    "# os.listdir(directory_right),users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "def weighted_mean(self, weights, **kwargs):\n",
    "    weights = self._shallow_copy(weights)\n",
    "    window = self._get_window(weights)\n",
    "\n",
    "    def _get_weighted_mean(X, Y):\n",
    "        X = X.astype('float64')\n",
    "        Y = Y.astype('float64')\n",
    "        sum_f = lambda x: x.rolling(window, self.min_periods, center=self.center).sum(**kwargs)\n",
    "        return sum_f(X * Y) / sum_f(Y)\n",
    "\n",
    "    return _flex_binary_moment(self._selected_obj, weights._selected_obj,\n",
    "                               _get_weighted_mean, pairwise=True)\n",
    "\n",
    "_Rolling_and_Expanding.weighted_mean = weighted_mean\n",
    "def dump_data(directory_ema,directory1,directory2,f):\n",
    "    if f not in os.listdir(directory_ema):\n",
    "        return 0\n",
    "    data = pickle.load(open(directory1+f,'rb'))\n",
    "    a = data[0]\n",
    "    this_participant = []\n",
    "    stress_all = a[['time','ltime','user','final_feature_matrix','stress_likelihood_ppg_qual','stress_likelihood_ppg_no_norm_qual',\n",
    "                    'stress_likelihood_ppg_no_norm_no_qual','stress_likelihood_ppg_no_norm_qual_all_features',\n",
    "                    'stress_likelihood_ppg_no_norm_no_qual_all_features',\n",
    "                    'quality_mag','day','activity','hand','rr_weighted_features','60','100','60_all','100_all']].dropna()\n",
    "    feature_matrix = np.array(list(stress_all['final_feature_matrix']))\n",
    "    stress_all['hr'] = stress_all['rr_weighted_features'].apply(lambda a:a[-5])\n",
    "    stress_days = [a for i,a in stress_all.groupby(['hand','day'],as_index=False) if a.shape[0]>180]\n",
    "    _Rolling_and_Expanding.weighted_mean = weighted_mean\n",
    "    for a in stress_days:\n",
    "        a = a.sort_values('time').reset_index(drop=True)\n",
    "        a['stress_likelihood_ppg_qual_weighted'] = a['stress_likelihood_ppg_qual'].rolling(window = 5).weighted_mean(a['quality_mag'])\n",
    "        a['stress_likelihood_ppg_no_norm_qual_weighted'] = a['stress_likelihood_ppg_no_norm_qual'].rolling(window = 5).weighted_mean(a['quality_mag'])\n",
    "        a['stress_likelihood_ppg_no_norm_qual_all_features_weighted'] = a['stress_likelihood_ppg_no_norm_qual_all_features'].rolling(window = 5).weighted_mean(a['quality_mag'])\n",
    "        a['stress_likelihood_ppg_no_norm_no_qual_weighted'] = a['stress_likelihood_ppg_no_norm_no_qual'].rolling(window = 5).weighted_mean(a['quality_mag'])\n",
    "        a['stress_likelihood_ppg_no_norm_no_qual_all_features_weighted'] = a['stress_likelihood_ppg_no_norm_no_qual'].rolling(window = 5).weighted_mean(a['quality_mag'])\n",
    "        a['qual'] = a['quality_mag'].rolling(window = 7).weighted_mean(a['quality_mag'])\n",
    "        a['hr'] = a['hr'].rolling(window = 7).weighted_mean(a['quality_mag'])\n",
    "        this_participant.append(a)\n",
    "    if len(this_participant)==0:\n",
    "        return 0\n",
    "    data = pd.concat(this_participant)\n",
    "    ema = pickle.load(open(directory_ema+f,'rb'))\n",
    "    ema = ema.sort_values('score').reset_index(drop=True)\n",
    "    def get_sign(s,reverse=False):\n",
    "        if not reverse:\n",
    "            negative = 0\n",
    "            positive = 1\n",
    "        else:\n",
    "            negative = 1\n",
    "            positive = 0\n",
    "        if s<=0:\n",
    "            return negative\n",
    "        else:\n",
    "            return positive\n",
    "    ema['happy'] = ema['all_scores'].apply(lambda a:a[0])\n",
    "    ema['joyful'] = ema['all_scores'].apply(lambda a:a[1])\n",
    "    ema['nervous'] = ema['all_scores'].apply(lambda a:a[2])\n",
    "    ema['sad'] = ema['all_scores'].apply(lambda a:a[3])\n",
    "    ema['angry'] = ema['all_scores'].apply(lambda a:a[4])\n",
    "#     for c in ['happy','angry','joyful','nervous','sad']:\n",
    "#         ema[c] = (ema[c] - ema[c].mean())/ema[c].std()\n",
    "#     happy,joy,nervous,sad,angry = ema['happy'].mean(),ema['joyful'].mean(),ema['nervous'].mean(),ema['sad'].mean(),ema['angry'].mean()\n",
    "#     ema['happy'] = ema['happy'].apply(lambda a:get_sign(a-happy))\n",
    "#     ema['joyful'] = ema['joyful'].apply(lambda a:get_sign(a-joy))\n",
    "#     ema['nervous'] = ema['nervous'].apply(lambda a:get_sign(a-nervous))\n",
    "#     ema['sad'] = ema['sad'].apply(lambda a:get_sign(a-sad))\n",
    "#     ema['angry'] = ema['angry'].apply(lambda a:get_sign(a-angry))    \n",
    "    pickle.dump([data,ema],open(directory2+f,'wb'))\n",
    "    return 0\n",
    "\n",
    "directory1 = '../../cc3/rice_data/after_computation/ecg_ppg_final_day_5_v11/'\n",
    "directory2 = '../../cc3/rice_data/after_ema_parsing/ecg_ppg_final_weighted_day_5_v11/'\n",
    "if not os.path.isdir(directory2):\n",
    "    os.makedirs(directory2)\n",
    "directory_ema = '../../cc3/rice_data/ecg_ppg_ema_final/'\n",
    "from joblib import Parallel,delayed\n",
    "output = Parallel(n_jobs=30,verbose=3)(delayed(dump_data)(directory_ema,directory1,directory2,f) for f in os.listdir(directory1) if f[-1]=='p')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.window import _flex_binary_moment, _Rolling_and_Expanding\n",
    "import matplotlib.pyplot as plt\n",
    "directory2 = '../../cc3/rice_data/after_ema_parsing/ecg_ppg_final_weighted_day_5_v11/'\n",
    "count = 0\n",
    "df_col = []\n",
    "ema_day = []\n",
    "for f in os.listdir(directory2):\n",
    "    if f[-1]!='p':\n",
    "        continue\n",
    "    data,ema = pickle.load(open(directory2+f,'rb'))\n",
    "    if ema.shape[0]<10:\n",
    "        continue\n",
    "    for day in ema['day'].unique():\n",
    "        ema_day.append(ema[ema.day==day])\n",
    "        if ema_day[-1].shape[0]<1:\n",
    "            ema_day = ema_day[:-1]\n",
    "            continue\n",
    "        if data[data.day==day]['stress_likelihood_ppg_qual'].dropna().shape[0]<60:\n",
    "            ema_day = ema_day[:-1]\n",
    "            continue\n",
    "        df_col.append(data[data.day==day])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel,delayed\n",
    "import numpy as np\n",
    "def get_data_data(duration,df_col,ema_day):\n",
    "    all_data = []\n",
    "    all_emas = []\n",
    "    all_users = []\n",
    "    hands = []\n",
    "    for i in range(len(df_col)):\n",
    "        data = df_col[i].sort_values('time').reset_index(drop=True)\n",
    "        ema = ema_day[i].sort_values('time').reset_index(drop=True)\n",
    "        for hand in ['left','right']:\n",
    "            for j,row in ema.iterrows():\n",
    "                temp_data = data[(data.time>=row['time']-duration*60) & (data.time<row['time']) & (data.hand==hand)].sort_values('time').reset_index(drop=True)\n",
    "                print(temp_data.columns)\n",
    "#                 if duration<=35:\n",
    "#                     min_length = 4\n",
    "#                 else:\n",
    "#                     min_length = 10\n",
    "#                 if duration<=35:\n",
    "#                     min_duration = 4\n",
    "#                 else:\n",
    "#                     min_duration = 10\n",
    "                min_length =  np.floor(duration*.2)\n",
    "                min_duration = 4\n",
    "                \n",
    "                if temp_data.shape[0]>min_length and temp_data['time'].values[-1]-temp_data['time'].values[0]>min_duration:\n",
    "                    all_data.append(temp_data[['stress_likelihood_ppg_qual',\n",
    "                                               'stress_likelihood_ppg_no_norm_qual',\n",
    "                                               'stress_likelihood_ppg_no_norm_no_qual',\n",
    "                                               'stress_likelihood_ppg_no_norm_qual_all_features',\n",
    "                                               'stress_likelihood_ppg_no_norm_no_qual_all_features',\n",
    "                                               'stress_likelihood_ppg_no_norm_qual_weighted',\n",
    "                                               'stress_likelihood_ppg_qual_weighted',\n",
    "                                               'stress_likelihood_ppg_no_norm_no_qual_weighted',\n",
    "                                               'stress_likelihood_ppg_no_norm_qual_all_features_weighted',\n",
    "                                               'stress_likelihood_ppg_qual_weighted',\n",
    "                                               'stress_likelihood_ppg_no_norm_no_qual_all_features_weighted',\n",
    "                                               'quality_mag']])\n",
    "                    all_emas.append(row)\n",
    "                    all_users.append(row['user'])\n",
    "                    hands.append(hand)\n",
    "    return [duration,all_data,all_emas,all_users,hands]\n",
    "data_data = Parallel(n_jobs=30,verbose=3)(delayed(get_data_data)(duration,df_col,ema_day) for duration in np.arange(10,125,5))\n",
    "# [get_data_data(duration,df_col,ema_day) for duration in np.arange(5,225,10)]\n",
    "pickle.dump(data_data,open('../data/data_emas_all_duration_day_v11.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pylab as pb\n",
    "# import GPy \n",
    "# %pylab inline\n",
    "# def get_predictions(X,Y,error):\n",
    "#     X = (X - np.mean(X))/np.std(X)\n",
    "#     mm = np.mean(Y)\n",
    "#     ss = np.std(Y)\n",
    "#     Y = (Y-np.mean(Y))/ss\n",
    "#     kern =  GPy.kern.RBF(input_dim=1) + GPy.kern.MLP(1) \n",
    "#     Y_meta = {'output_index':np.arange(len(Y))[:,None]}\n",
    "#     m = GPy.models.GPHeteroscedasticRegression(X[:,None],Y[:,None],kern,Y_metadata=Y_meta)\n",
    "#     m['.*het_Gauss.variance'] = np.abs(error)[:,None] #Set the noise parameters to the error in Y\n",
    "#     m.het_Gauss.variance.fix() #We can fix the noise term, since we already know it\n",
    "#     m.optimize()\n",
    "#     preds,varss  = m.predict(m.X,full_cov=False,Y_metadata=None,kern=None,likelihood=None,include_likelihood=False)\n",
    "#     return preds*ss+mm,varss\n",
    "len(df_col),len(data_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def save_pdf(data,ema,i):\n",
    "    for hand in ['left','right']:\n",
    "        g= deepcopy(data[data.hand==hand]).dropna()\n",
    "#         g['hour'] = g['ltime'].apply(lambda a:int(a.hour))\n",
    "#         print(g.hour.values)\n",
    "#         g = g[(g.hour>=6) & (g.hour<20)]\n",
    "        if g.shape[0]<100:\n",
    "            continue\n",
    "        g['ltime_str'] = g['ltime'].apply(lambda a:a.strftime(\"%H:%M\"))\n",
    "        preds,varss = get_predictions(g['time'].values,g['stress_likelihood_ppg_qual'].values,(1.1-g['quality_mag'].values)/2)\n",
    "        plt.figure(figsize=(19,10))\n",
    "        plt.title(g['user'].values[0] +'--'+ g['day'].values[0]+'--'+g['hand'].values[0]+' wrist')\n",
    "        plt.plot((g['time']-np.min(g['time']))/60,g['stress_likelihood_ppg_qual'],'*-r',linewidth=1,label='Stress Likelihood')\n",
    "        plt.plot((g['time']-np.min(g['time']))/60,preds,'*-k',linewidth=3,label='Stress Likelihood Post Processed by GP')\n",
    "#         plt.plot((g['time']-np.min(g['time']))/60,g['hr']/60,'s-g',linewidth=3,label='Normalized Heart Rate w.r.t. 60 BPM')\n",
    "        #         plt.plot((g['time']-np.min(g['time']))/60,g['stress_likelihood_ppg'],'y',linestyle='--')\n",
    "        plt.bar((g['time']-np.min(g['time']))/60,g['quality_mag'],4,alpha=1,label='Minute Level Quality Metric')\n",
    "        plt.bar((g['time']-np.min(g['time']))/60,g['probs']*-1,4,alpha=1,label='LAB(0) vs Field(-1)')\n",
    "        #         for j,row in ema.iterrows():\n",
    "        #             print(row['label'])\n",
    "        #             if np.int64(row['label'])==1:\n",
    "        row = ema[ema.label==1]\n",
    "        if row.shape[0]>0:\n",
    "            plt.bar((row['time']-np.min(g['time']))/60,[1]*row.shape[0],10,alpha=1,label='Stress EMA',color='darkred')\n",
    "        #             else:\n",
    "        row = ema[ema.label==0]\n",
    "        if row.shape[0]>0:\n",
    "            plt.bar((row['time']-np.min(g['time']))/60,[1]*row.shape[0],10,alpha=1,label='Not Stress EMA',color='darkgreen')\n",
    "        plt.legend(ncol=5)\n",
    "        plt.xticks(np.array((g['time']-np.min(g['time']))/60)[np.arange(g.shape[0])%10==0],g['ltime_str'][np.arange(g.shape[0])%10==0],rotation=60)\n",
    "        plt.ylim([-1.3,1.3])\n",
    "        plt.xlabel('Time of Day')\n",
    "    #     plt.ylabel('Likelihood Values')\n",
    "        plt.savefig('../pics_day/'+g['user'].values[0] +'--'+ g['day'].values[0]+'--'+g['hand'].values[0]+' wrist'+'.pdf',dps=1e6)\n",
    "        plt.close('all')\n",
    "    return 0\n",
    "from joblib import Parallel,delayed\n",
    "ouput = Parallel(n_jobs=-1,verbose=2)(delayed(save_pdf)(df_col[i].sort_values('time').reset_index(drop=True),\n",
    "                                                        ema_day[i].sort_values('time').reset_index(drop=True),\n",
    "                                                       i) for i in range(len(df_col)))\n",
    "# ouput = [save_pdf(df_col[i].sort_values('time').reset_index(drop=True),\n",
    "#                                                         ema_day[i].sort_values('time').reset_index(drop=True),\n",
    "#                                                        i) for i in range(len(df_col))]\n",
    "    #         plt.bar((g['time']-np.min(g['time']))/60,g['activity_f'],.2,alpha=.5,color='r')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "import os\n",
    "filenames = os.listdir('../pics_day/')\n",
    "filenames = sorted([a for a in filenames if a[-1]=='f'])\n",
    "\n",
    "merger = PdfFileMerger()\n",
    "for filename in filenames:\n",
    "    merger.append(PdfFileReader(open('../pics_day/'+filename, 'rb')))\n",
    "\n",
    "merger.write(\"result-output_daywise_normalization.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3 High Performance",
   "language": "python",
   "name": "cc3_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
