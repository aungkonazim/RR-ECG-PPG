{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from ecg import ecg_feature_computation\n",
    "import matplotlib.pyplot as plt\n",
    "# from hrvanalysis import get_time_domain_features,get_geometrical_features,get_csi_cvi_features,get_poincare_plot_features\n",
    "# from hrvanalysis import get_frequency_domain_features\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from enum import Enum\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,f1_score,roc_curve,auc,precision_score,recall_score,accuracy_score,classification_report,make_scorer,precision_recall_curve\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def my_score_auc(y_true,y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr,tpr)\n",
    "\n",
    "\n",
    "def fit_model(X,y,groups,k,paramGrid):\n",
    "    X = np.delete(X,k,axis=1)\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    return np.array([a,k]).reshape(-1)\n",
    "\n",
    "def get_results_backward_elimination(X,y,groups):\n",
    "    \n",
    "    \n",
    "    delta = 0.1\n",
    "    paramGrid = {'rf__kernel': ['rbf'],\n",
    "                 'rf__C': [100,10,200],\n",
    "                 'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .25)],\n",
    "                 'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3]],\n",
    "                 'rf__probability':[True]\n",
    "    }\n",
    "    feature_names = ['var','iqr','mean','median','80th','20th','heartrate','vlf','lf','hf','lf-hf']\n",
    "#     gg = fit_model(deepcopy(X),y,groups,k,paramGrid)\n",
    "    data = []\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring='f1',verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    data.append(['all',a])\n",
    "    print(data)\n",
    "    while len(feature_names)>1:\n",
    "        results = Parallel(n_jobs=30,verbose=4)(delayed(fit_model)(deepcopy(X),y,groups,k,paramGrid) for k,name in enumerate(feature_names))\n",
    "        results = np.array(results)\n",
    "        print(results,results.shape)\n",
    "        ind_min = np.argmax(results[:,0])\n",
    "        min_f1 = results[ind_min,0]\n",
    "        min_index = np.int64(results[ind_min,1])\n",
    "        name_feature = feature_names[min_index]\n",
    "        data.append([name_feature,min_f1])\n",
    "        X = np.delete(X,min_index,axis=1)\n",
    "        feature_names = feature_names[:min_index] + feature_names[(min_index+1):]\n",
    "        print(data)\n",
    "    return data\n",
    "    \n",
    "\n",
    "\n",
    "def get_results(X,y,groups):\n",
    "    my_score = make_scorer(my_score_auc,needs_threshold=True)\n",
    "    delta = 0.1\n",
    "    paramGrid = {'rf__kernel': ['rbf'],\n",
    "                 'rf__C': [10,1,100,200],\n",
    "                 'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .25)],\n",
    "                 'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.25,.35]],\n",
    "                 'rf__probability':[True]\n",
    "    }\n",
    "    pca = PCA(n_components=4)\n",
    "    clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "    gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                               scoring=my_score,verbose=5)\n",
    "    grid_search.fit(X,y)\n",
    "    clf = grid_search.best_estimator_\n",
    "    probs = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20,method='predict_proba')[:,1]\n",
    "    pp = deepcopy(probs)\n",
    "    a,b = f1Bias_scorer_CV(probs, y, ret_bias=True)\n",
    "    print(roc_auc_score(y,probs),a,b)\n",
    "    probs[probs>=b] = 1\n",
    "    probs[probs<b] = 0\n",
    "    y_pred = np.int64(probs)\n",
    "    print(classification_report(y,y_pred))\n",
    "    clf.fit(X,y)\n",
    "    return clf,np.array([f1_score(y,y_pred),precision_score(y,y_pred),recall_score(y,y_pred)])\n",
    "\n",
    "def get_label(user_data,st,et):\n",
    "    label = 2\n",
    "    for k in range(user_data.shape[0]):\n",
    "        if st>=user_data[k,0] and et<=user_data[k,1]:\n",
    "            label = user_data[k,2]\n",
    "\n",
    "    return label\n",
    "\n",
    "def get_ecg_windowss(rr_interval,window=0):\n",
    "    window_col,ts_col = [],[]\n",
    "    if window==0:\n",
    "        n = 30\n",
    "    else:\n",
    "        n = (.5*60)/(window/2)\n",
    "    m = np.mean(rr_interval[:,1])\n",
    "    s = np.std(rr_interval[:,1])\n",
    "    ts_array = np.arange(rr_interval[0,0],rr_interval[-1,0],30000)\n",
    "    for t in ts_array:\n",
    "        index = np.where((rr_interval[:,0]>=t)&(rr_interval[:,0]<=t+60000))[0]\n",
    "        if len(index)<n:\n",
    "            continue\n",
    "        rr_temp = rr_interval[index,:]\n",
    "        rr_temp = rr_temp[np.where((rr_temp[:,1]>m-3*s)&(rr_temp[:,1]<m+3*s))[0],:]\n",
    "        if len(rr_temp)<n:\n",
    "            continue\n",
    "        window_col.append(rr_temp)\n",
    "        ts_col.append(t)\n",
    "    return window_col,ts_col\n",
    "\n",
    "def get_rr_features(a):\n",
    "    return np.array([np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),np.percentile(a,20),60000/np.median(a)])\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(RRints,tmStamps, band_type = None, lf_bw = 0.11, hf_bw = 0.1, plot = 0):\n",
    "    \"\"\" Computes frequency domain features on RR interval data\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    RRints : list, shape = [n_samples,]\n",
    "           RR interval data\n",
    "    \n",
    "    band_type : string, optional\n",
    "             If band_type = None, the traditional frequency bands are used to compute \n",
    "             spectral power:\n",
    "           \n",
    "                 LF: 0.003 - 0.04 Hz\n",
    "                 HF: 0.04 - 0.15 Hz\n",
    "                 VLF: 0.15 - 0.4 Hz           \n",
    "           \n",
    "             If band_type is set to 'adapted', the bands are adjusted according to \n",
    "             the protocol laid out in:\n",
    "           \n",
    "             Long, Xi, et al. \"Spectral boundary adaptation on heart rate \n",
    "             variability for sleep and wake classification.\" International \n",
    "             Journal on Artificial Intelligence Tools 23.03 (2014): 1460002. \n",
    "                        \n",
    "    lf_bw : float, optional\n",
    "          Low frequency bandwidth centered around LF band peak frequency\n",
    "          when band_type is set to 'adapted'. Defaults to 0.11\n",
    "             \n",
    "    hf_bw : float, optional\n",
    "          High frequency bandwidth centered around HF band peak frequency\n",
    "          when band_type is set to 'adapted'. Defaults to 0.1\n",
    "          \n",
    "    plot : int, 1|0\n",
    "          Setting plot to 1 creates a matplotlib figure showing frequency\n",
    "          versus spectral power with color shading to indicate the VLF, LF,\n",
    "          and HF band bounds.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    freqDomainFeats : dict\n",
    "                   VLF_Power, LF_Power, HF_Power, LF/HF Ratio              \n",
    "    \"\"\"\n",
    "\n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = []\n",
    "    tss = []\n",
    "    for c, rr in enumerate(RRints):        \n",
    "        if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "            NNs.append(rr)\n",
    "            tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    if band_type == 'adapted':     \n",
    "            \n",
    "        vlf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])]))[0][0]] \n",
    "        lf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])]))[0][0]]\n",
    "        hf_peak = fxx[np.where(pxx == np.max(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])]))[0][0]]\n",
    "    \n",
    "        peak_freqs =  (vlf_peak, lf_peak, hf_peak) \n",
    "            \n",
    "        hf = (peak_freqs[2] - hf_bw/2, peak_freqs[2] + hf_bw/2)\n",
    "        lf = (peak_freqs[1] - lf_bw/2, peak_freqs[1] + lf_bw/2)   \n",
    "        vlf = (0.003, lf[0])\n",
    "        \n",
    "        if lf[0] < 0:\n",
    "            print('***Warning***: Adapted LF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of LF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            lf = (0, lf[1])        \n",
    "            vlf = (0, 0)\n",
    "        elif hf[0] < 0:\n",
    "            print('***Warning***: Adapted HF band lower bound spills into negative frequency range')\n",
    "            print('Lower thresold of HF band has been set to zero')\n",
    "            print('Adjust LF and HF bandwidths accordingly')\n",
    "            hf = (0, hf[1])        \n",
    "            lf = (0, 0)        \n",
    "            vlf = (0, 0)\n",
    "            \n",
    "        plot_labels = ['Adapted_VLF', 'Adapted_LF', 'Adapted_HF']\n",
    "\n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    if plot == 1:\n",
    "        #Plot option\n",
    "        freq_bands = {'vlf': vlf, 'lf': lf, 'hf': hf}\n",
    "        freq_bands = OrderedDict(sorted(freq_bands.items(), key=lambda t: t[0]))\n",
    "        colors = ['lightsalmon', 'lightsteelblue', 'darkseagreen']\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(fxx, pxx, c = 'grey')\n",
    "        plt.xlim([0, 0.40])\n",
    "        plt.xlabel(r'Frequency $(Hz)$')\n",
    "        plt.ylabel(r'PSD $(s^2/Hz$)')\n",
    "        \n",
    "        for c, key in enumerate(freq_bands):\n",
    "            ax.fill_between(fxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            pxx[min(np.where(fxx >= freq_bands[key][0])[0]): max(np.where(fxx <= freq_bands[key][1])[0])],\n",
    "                            0, facecolor = colors[c])\n",
    "            \n",
    "        patch1 = mpatches.Patch(color = colors[0], label = plot_labels[2])\n",
    "        patch2 = mpatches.Patch(color = colors[1], label = plot_labels[1])\n",
    "        patch3 = mpatches.Patch(color = colors[2], label = plot_labels[0])\n",
    "        plt.legend(handles = [patch1, patch2, patch3])\n",
    "        plt.show()\n",
    "\n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "def combine_data_sobc(window_col,ts_col,label_data,participant):\n",
    "    feature_matrix = []\n",
    "    user_col = []\n",
    "    label_col = []\n",
    "    for i,item in enumerate(window_col):\n",
    "#         try:\n",
    "        r = frequencyDomain(item[:,1]/1000,item[:,0]/1000)\n",
    "#         except Exception as:\n",
    "#             continue\n",
    "        feature = np.array(list(get_rr_features(item[:,1])) + list(r.values()))\n",
    "        feature_matrix.append(feature.reshape(-1,len(feature)))\n",
    "        user_col.append(participant)\n",
    "        label_col.append(get_label(label_data,ts_col[i],ts_col[i]+50000))\n",
    "    return np.array(feature_matrix).reshape(-1,11),user_col,label_col\n",
    "\n",
    "def get_2_sec_ts(rr_ppg_int,window=2):\n",
    "    m = np.mean(rr_ppg_int[:,1])\n",
    "    s = np.std(rr_ppg_int[:,1])\n",
    "    ts_array = np.arange(rr_ppg_int[0,0],rr_ppg_int[-1,0],1000*window/2)\n",
    "    rr_interval = np.zeros((0,2))\n",
    "    for t in ts_array:\n",
    "        index = np.where((rr_ppg_int[:,0]>=t-1000*window//2)&(rr_ppg_int[:,0]<=t+1000*window//2))[0]\n",
    "        if len(index) < 1:\n",
    "            continue\n",
    "        rr_interval = np.concatenate((rr_interval,np.array([t,np.mean(rr_ppg_int[index,1])]).reshape(-1,2)))\n",
    "    return rr_interval\n",
    "\n",
    "def get_dd(ecg_rr,label_data,window,participant):\n",
    "    window_col,ts_col = get_ecg_windowss(ecg_rr,window)\n",
    "    feature_matrix,user_col,label_col = combine_data_sobc(window_col,ts_col,label_data,participant)\n",
    "    rr_70th = np.percentile(feature_matrix[:,2],60)\n",
    "    rr_95th = np.percentile(feature_matrix[:,2],99)\n",
    "    index = np.where((feature_matrix[:,2]>rr_70th)&(feature_matrix[:,2]<rr_95th))[0]\n",
    "    means = np.mean(feature_matrix[index],axis=0)\n",
    "    stds = np.std(feature_matrix[index],axis=0)\n",
    "    feature_matrix = (feature_matrix - means)/stds\n",
    "    temp = np.array(label_col)\n",
    "    labels = np.array(label_col)\n",
    "    return feature_matrix,user_col,label_col\n",
    "\n",
    "def get_precision_recall_f1(path,window):\n",
    "    participants = os.listdir(path)\n",
    "    X,y,groups = np.zeros((0,11)),[],[]\n",
    "    data_col = []\n",
    "    for participant in participants:\n",
    "        file_list = os.listdir(path+'/'+participant)\n",
    "        for file in file_list:\n",
    "            final_path = path+'/'+participant+'/'+file+'/'\n",
    "            if 'ecg_rr_final.csv' not in os.listdir(final_path):\n",
    "                continue\n",
    "            else:\n",
    "                ecg_rr = pd.read_csv(final_path+'ecg_rr_final.csv',header=None,sep=',').values\n",
    "            if ecg_rr.shape[0]<500:\n",
    "                continue\n",
    "            if window>0:\n",
    "                ecg_rr = get_2_sec_ts(ecg_rr,window)\n",
    "            label_data = pd.read_csv(final_path+'label_data.csv',header=None,sep=',').values\n",
    "            data_col.append([ecg_rr,label_data,window,participant])\n",
    "    final = Parallel(n_jobs=30,verbose=3)(delayed(get_dd)(*a) for a in data_col)\n",
    "    for feature_matrix,user_col,label_col in final:\n",
    "        X = np.concatenate((X,feature_matrix))\n",
    "        y.extend(label_col)\n",
    "        groups.extend(user_col)\n",
    "    print(X.shape)\n",
    "    y = np.array(y)\n",
    "    groups = np.array(groups)\n",
    "    index = np.where(y<2)[0]\n",
    "    X,y,groups = X[index],y[index],groups[index]\n",
    "    print(X.shape,len(y),sum(y),len(np.unique(groups)))\n",
    "    return get_results(X,y,groups)\n",
    "#     return get_results_backward_elimination(X,y,groups)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:   10.1s remaining:  1.1min\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:   15.1s remaining:   15.1s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:   20.3s remaining:    3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 11)\n",
      "(3031, 11) 3031 780.0 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 29 folds for each of 256 candidates, totalling 7424 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4512 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5106 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5736 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6402 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7104 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7424 out of 7424 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9137756438733783 0.772117962466488 0.39297667463055375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.93      2251\n",
      "         1.0       0.81      0.74      0.77       780\n",
      "\n",
      "    accuracy                           0.89      3031\n",
      "   macro avg       0.86      0.84      0.85      3031\n",
      "weighted avg       0.89      0.89      0.89      3031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats_basic import winsorize\n",
    "import pickle\n",
    "path = '/home/jupyter/mullah/Test/data_yield/data/sobc_2nd_chance//'\n",
    "all_results = []\n",
    "for duration in [0]:\n",
    "    clf,temp = get_precision_recall_f1(path,duration)\n",
    "    temp = [duration]+list(temp)\n",
    "    all_results.append(np.array(temp))\n",
    "# #     pickle.dump(all_results,open('../data/rice/stress_results.p','wb'))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf,open('../models/stress_ecg_final.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(f,open('../models/stress_ecg_results.p','wb'))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pickle.load(open('../data/rice/stress_results.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.77211796, 0.80898876, 0.73846154])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(np.array(range(0,df.shape[0]*4,4)),df[:,1],1,label='F1 score')\n",
    "plt.bar(np.array(range(1,df.shape[0]*4,4)),df[:,2],1,label='Precision score')\n",
    "plt.bar(np.array(range(2,df.shape[0]*4,4)),df[:,3],1,label='Recall score')\n",
    "plt.xticks(np.array(range(1,df.shape[0]*4,4)),['No \\n smoothing']+list(np.arange(2,16,1)),rotation=60)\n",
    "plt.xlabel('Moving Average Duration')\n",
    "plt.legend(ncol=3)\n",
    "plt.title('Stress Detection from ECG')\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y = np.array(y)\n",
    "groups = np.array(groups)\n",
    "y = y[~np.isnan(X).any(axis=1)]\n",
    "groups = groups[~np.isnan(X).any(axis=1)]\n",
    "X = X[~np.isnan(X).any(axis=1)]\n",
    "print(X.shape,len(np.unique(groups)))\n",
    "for user in np.unique(groups):\n",
    "    index = np.where(groups==user)[0]\n",
    "    X[index,:] = StandardScaler().fit_transform(X[index,:])\n",
    "# X[X>4] = 4\n",
    "# X[X<-4] = -4\n",
    "index = np.where(y<2)[0]\n",
    "X,y,groups = X[index],y[index],groups[index]\n",
    "print(X.shape,len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {'rf__kernel': ['rbf'],\n",
    "             'rf__C': [11,1000],\n",
    "             'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-6, -2, .25)],\n",
    "             'rf__class_weight': [{0: w, 1: 1 - w} for w in [.2,.3,.25,.35]],\n",
    "             'rf__probability':[True]\n",
    "}\n",
    "pca = PCA(n_components=4)\n",
    "clf = Pipeline([('sts',StandardScaler()),('rf', SVC())])\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='accuracy',verbose=5)\n",
    "grid_search.fit(X[:,:],y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=20)\n",
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('../models/stress_model_ecg_2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mats = confusion_matrix(y,y_pred)\n",
    "mats\n",
    "# sns.set(font_scale=1.5)\n",
    "# df_cm = pd.DataFrame(mats, index = [i for i in ['Not Stress','Stress']],\n",
    "#                   columns = [i for i in ['Not Stress','Stress']])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sns.heatmap(df_cm, annot=True,fmt='g',annot_kws={\"fontsize\":28})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba')[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator, param_grid, scoring, fit_params, n_jobs, iid,\n",
    "                refit, cv, verbose, pre_dispatch, error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "X1 = preprocessing.StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': np.logspace(0,2,2),\n",
    "              'gamma': np.logspace(-9,9,10),\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "parameters = {\n",
    "    'min_samples_leaf': [4],\n",
    "    'max_features': [.7,1],\n",
    "    'n_estimators': [100,200,300],\n",
    "    'n_jobs': [-1],\n",
    "    'criterion':['gini','entropy'],\n",
    "    'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "    'random_state': [42]\n",
    "       }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X1,y,groups=groups)),\n",
    "                                   n_jobs=20, scoring=f1Bias_scorer_CV, verbose=1, iid=False)\n",
    "grid_search.fit(X1,y)\n",
    "clf = grid_search.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X1, y, gkf.split(X1,y,groups=groups))\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "def plot_confusion_matrix(cm, classes=['Not Stress','Stress'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('result.png')\n",
    "    plt.show()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "predicted = cross_val_predict(clf, X, y, cv=gkf.split(X,y,groups=groups),n_jobs=24)\n",
    "plot_confusion_matrix(confusion_matrix(y,predicted))\n",
    "print(f1_score(y,predicted),precision_score(y,predicted),recall_score(y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "svc = predictor\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=gkf.split(X,y,groups=groups),\n",
    "              scoring='f1',n_jobs=24)\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1/2,1/3,1/6],[0,1/3,2/3],[1/2,0,1/2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.matmul(a,a),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float.as_integer_ratio(0.36111111)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3",
   "language": "python",
   "name": "cc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
